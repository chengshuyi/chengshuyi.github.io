[{"categories":["杂谈"],"content":"最近遇到一个bug，内存的数据被篡改，但是由于数据频繁变化、时间短（几毫秒更新一次）以及代码极多，无法利用单步调试确定是哪行代码修改了其内存数据。采用内存地址条件断点调试很快就解决了问题。\n场景描述 有64位整数数组为uint64_t result[0x10000000]，外界通过http协议获取该数据的二进制，但是该数据有时会出现异常值。可以知道，result数据被任意的篡改了。难点在于代码多，无法很好的定位哪里修改了result。\n解决方案 通过观察发现，result[0]每次都会被修改成整数0x14d00000002，因此可以插入内存条件断点：\n 假设result的首地址为0x12108920，则插入内存条件断点为*((uint64_t *)(0x12108920)) == 0x14d00000002； 运行程序，当条件成立时，程序自动停止，发现是全局变量重名的问题。在另外一个文件里面同样定义了一个uint64_t result[]的全局数组；  虽然问题很简单，但是也有不少启发：\n 全局变量最好能用统一的文件进行维护； 内存数据篡改可以利用内存条件断点找到修改数据的指令地址； 可以采用反汇编搜索被篡改的内存地址，可以找到哪些函数或者指令修改了其数据； 能用static就用static；  内存断点原理 内存断点是通过修改该物理地址对应的页面属性，比如设置成只读、只写或者不可读写。当有指令试图修改该地址对应的值就会触发异常，然后由异常处理函数接管会进行如下判断：\n 判断是否有在该地址插入内存断点，有的话需要恢复该物理地址的属性，如变成可读可写的； 如果是条件断点还需要判断可以判断是否满足条件，如果满足条件则需要设置一个单步调试断点。  ","description":"","tags":["杂谈"],"title":"记一次利用条件断点调试内存踩踏问题","uri":"/posts/%E7%BB%8F%E9%AA%8C/condition_breakpoints/"},{"categories":["redis源码分析"],"content":" 由于内容较多，暂不分析:-)，感觉写了也没他写的好。\n Redis内部数据结构详解(4)——ziplist\n","description":"","tags":["redis源码分析"],"title":"redis源码分析七：Ziplist","uri":"/posts/redis/7-ziplist/"},{"categories":["redis源码分析"],"content":"基本结构 1 2 3 4 5  typedef struct intset { uint32_t encoding; // 编码方式:16 32 64三种  uint32_t length; // 集合包含的元素数量  int8_t contents[]; // 保存元素的数组 } intset;   基本方法  intsetNew：创建一个空的整数集合 intsetAdd：向整数集合添加元素； intsetRemove：从整数集合删除元素； intsetFind intsetRandom intsetGet intsetLen intsetBlobLen  创建整数集合 1 2 3 4 5 6  intset *intsetNew(void) { intset *is = zmalloc(sizeof(intset)); // 1. 为整数集合结构分配空间  is-\u003eencoding = intrev32ifbe(INTSET_ENC_INT16); // 2. 设置初始编码  is-\u003elength = 0; // 3. 初始化元素数量  return is; }   添加元素 整数集合一个比较大的特点就是能够根据插入的值的大小动态的选择编码方式。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  intset *intsetAdd(intset *is, int64_t value, uint8_t *success) { uint8_t valenc = _intsetValueEncoding(value); // 1. 根据值的大小选择不同的编码方式  uint32_t pos; if (success) *success = 1; if (valenc \u003e intrev32ifbe(is-\u003eencoding)) { // 2. 编码长度升级  return intsetUpgradeAndAdd(is,value); } else { if (intsetSearch(is,value,\u0026pos)) { // 3. 找到其应该插入的位置  if (success) *success = 0; return is; } is = intsetResize(is,intrev32ifbe(is-\u003elength)+1); // 4. 为 value 在集合中分配空间  if (pos \u003c intrev32ifbe(is-\u003elength)) intsetMoveTail(is,pos,pos+1); // 5. 如果新元素不是被添加到底层数组的末尾那么需要对现有元素的数据进行移动，空出 pos 上的位置，用于设置新值  } _intsetSet(is,pos,value); // 6. 将新值设置到底层数组的指定位置中  is-\u003elength = intrev32ifbe(intrev32ifbe(is-\u003elength)+1); return is; }   删除元素 1 2 3 4 5 6 7 8 9 10 11 12 13  intset *intsetRemove(intset *is, int64_t value, int *success) { uint8_t valenc = _intsetValueEncoding(value); // 1. 计算 value 的编码方式  uint32_t pos; if (success) *success = 0; if (valenc \u003c= intrev32ifbe(is-\u003eencoding) \u0026\u0026 intsetSearch(is,value,\u0026pos)) { uint32_t len = intrev32ifbe(is-\u003elength); if (success) *success = 1; if (pos \u003c (len-1)) intsetMoveTail(is,pos+1,pos); // 2. 如果 value 不是位于数组的末尾，那么需要对原本位于 value 之后的元素进行移动  is = intsetResize(is,len-1); // 3. 缩小数组的大小，移除被删除元素占用的空间  is-\u003elength = intrev32ifbe(len-1); // 4. 更新长度  } return is; }   源码文件  src/intset.h src/intset.c  ","description":"","tags":["redis源码分析"],"title":"redis源码分析六：intset整数集合","uri":"/posts/redis/6-intset/"},{"categories":["redis源码分析"],"content":"Redis HyperLogLog 是用来做基数统计的算法，HyperLogLog 的优点是，在输入元素的数量或者体积非常非常大时，计算基数所需的空间总是固定的、并且是很小的。在 Redis 里面，每个 HyperLogLog 键只需要花费 12 KB 内存，就可以计算接近 $2^64$ 个不同元素的基 数。这和计算基数时，元素越多耗费内存就越多的集合形成鲜明对比。但是，因为 HyperLogLog 只会根据输入元素来计算基数，而不会储存输入元素本身，所以 HyperLogLog 不能像集合那样，返回输入的各个元素。\n接下来我们将从两个方面来分析hyperloglog，一个是hyperloglog的添加数据，另外一个是hyperloglog的合并过程。\nhll添加数据 具体见注释：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44  int hllAdd(robj *o, unsigned char *ele, size_t elesize) { struct hllhdr *hdr = o-\u003eptr;\t// 1. hll头部信息  switch(hdr-\u003eencoding) { case HLL_DENSE: return hllDenseAdd(hdr-\u003eregisters,ele,elesize); // 2. 密集模式添加元素  case HLL_SPARSE: return hllSparseAdd(o,ele,elesize); // 3. 稀疏模式添加元素  default: return -1; /* Invalid representation. */ } } int hllDenseAdd(uint8_t *registers, unsigned char *ele, size_t elesize) { long index; uint8_t count = hllPatLen(ele,elesize,\u0026index);\t// 4. 计算该元素第一个1出现的位置  return hllDenseSet(registers,index,count); } int hllPatLen(unsigned char *ele, size_t elesize, long *regp) { uint64_t hash, bit, index; int count; hash = MurmurHash64A(ele,elesize,0xadc83b19ULL);\t// 5. 利用MurmurHash64A哈希函数来计算该元素的hash值  index = hash \u0026 HLL_P_MASK;\t// 6. 取低14位作为桶的编号  hash \u003e\u003e= HLL_P; hash |= ((uint64_t)1\u003c\u003cHLL_Q); // 7. 为了保证循环能够终止，将第50（从0计数）位置1  bit = 1; count = 1;\t// 8. 存储第一个1出现的位置  while((hash \u0026 bit) == 0) { count++; bit \u003c\u003c= 1; } *regp = (int) index; return count; } int hllDenseSet(uint8_t *registers, long index, uint8_t count) { uint8_t oldcount; HLL_DENSE_GET_REGISTER(oldcount,registers,index);\t// 9. 得到第index个桶内的count值，因为hll每个桶的大小是6bit，所以需要采用特别的方法获取；  if (count \u003e oldcount) { HLL_DENSE_SET_REGISTER(registers,index,count);\t// 10. 如果比现有的值还大，则替换；也就是每个桶内统计的是最大值  return 1; } else { return 0; } }   hll统计结果 下图是hll统计的公式，其中：\n m表示分成多少个桶，在redis里低14位作为桶的编号，也就是$2^14$； const表示修正因子，一个常量； $R_j$表示桶j统计的数值（也就是高50位第一次出现1的位置）；  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77  uint64_t hllCount(struct hllhdr *hdr, int *invalid) { double m = HLL_REGISTERS; double E; int j; int reghisto[64] = {0}; // 1. 实际使用到的只有[0,51]  /* Compute register histogram */ if (hdr-\u003eencoding == HLL_DENSE) { hllDenseRegHisto(hdr-\u003eregisters,reghisto); // 2. 密集存储的hll统计，统计频次，也就是桶内值为x的桶的个数；  } else if (hdr-\u003eencoding == HLL_SPARSE) { hllSparseRegHisto(hdr-\u003eregisters, sdslen((sds)hdr)-HLL_HDR_SIZE,invalid,reghisto); } else if (hdr-\u003eencoding == HLL_RAW) { hllRawRegHisto(hdr-\u003eregisters,reghisto); } else { serverPanic(\"Unknown HyperLogLog encoding in hllCount()\"); } double z = m * hllTau((m-reghisto[HLL_Q+1])/(double)m); // 5. 特殊处理hash值高50位全为0的情况  for (j = HLL_Q; j \u003e= 1; --j) { // 6. 计算调和级数分母部分  z += reghisto[j]; z *= 0.5; } z += m * hllSigma(reghisto[0]/(double)m); // 7. 特殊处理空挡，也就是桶内数值为0  E = llroundl(HLL_ALPHA_INF*m*m/z); // 8. 上图的公式  return (uint64_t) E; } void hllDenseRegHisto(uint8_t *registers, int* reghisto) { int j; if (HLL_REGISTERS == 16384 \u0026\u0026 HLL_BITS == 6) { uint8_t *r = registers; unsigned long r0, r1, r2, r3, r4, r5, r6, r7, r8, r9, r10, r11, r12, r13, r14, r15; for (j = 0; j \u003c 1024; j++) { // 3. 循环1024次，每次计算16个桶，16*6=96也就是12字节  r0 = r[0] \u0026 63; r1 = (r[0] \u003e\u003e 6 | r[1] \u003c\u003c 2) \u0026 63; r2 = (r[1] \u003e\u003e 4 | r[2] \u003c\u003c 4) \u0026 63; r3 = (r[2] \u003e\u003e 2) \u0026 63; r4 = r[3] \u0026 63; r5 = (r[3] \u003e\u003e 6 | r[4] \u003c\u003c 2) \u0026 63; r6 = (r[4] \u003e\u003e 4 | r[5] \u003c\u003c 4) \u0026 63; r7 = (r[5] \u003e\u003e 2) \u0026 63; r8 = r[6] \u0026 63; r9 = (r[6] \u003e\u003e 6 | r[7] \u003c\u003c 2) \u0026 63; r10 = (r[7] \u003e\u003e 4 | r[8] \u003c\u003c 4) \u0026 63; r11 = (r[8] \u003e\u003e 2) \u0026 63; r12 = r[9] \u0026 63; r13 = (r[9] \u003e\u003e 6 | r[10] \u003c\u003c 2) \u0026 63; r14 = (r[10] \u003e\u003e 4 | r[11] \u003c\u003c 4) \u0026 63; r15 = (r[11] \u003e\u003e 2) \u0026 63; reghisto[r0]++;\t// 4. 频次增加  reghisto[r1]++; reghisto[r2]++; reghisto[r3]++; reghisto[r4]++; reghisto[r5]++; reghisto[r6]++; reghisto[r7]++; reghisto[r8]++; reghisto[r9]++; reghisto[r10]++; reghisto[r11]++; reghisto[r12]++; reghisto[r13]++; reghisto[r14]++; reghisto[r15]++; r += 12; } } else { for(j = 0; j \u003c HLL_REGISTERS; j++) { unsigned long reg; HLL_DENSE_GET_REGISTER(reg,registers,j); reghisto[reg]++; } } }   源码文件  src/hyperloglog.c  ","description":"","tags":["redis源码分析"],"title":"redis源码分析五：Hyperloglog","uri":"/posts/redis/5-hyperlog/"},{"categories":["redis源码分析"],"content":"创建 zslCreate 插入 zslInsert 删除 zslDelete\n跳跃表基础知识  摘自：https://blog.csdn.net/universe_ant/article/details/51134020\n 跳跃表(skiplist)是一种有序数据结构，它通过在每个节点中维持多个指向其他节点的指针（也就是level），从而达到快速访问节点的目的。跳跃表支持平均O(logN)、最坏O(N)复杂度的节点查找，还可以通过顺序性操作来批量处理节点。在大部分情况下，跳跃表的效率可以和平衡树相媲美，并且因为跳跃表的实现比平衡树要来得更为简单，所以有不少程序都使用跳跃表来代替平衡树。Redis使用跳跃表作为有序集合键的底层实现之一，如果一个有序集合包含的元素数量比较多，又或者有序集合中元素的成员(member)是比较长的字符串时，Redis就会使用跳跃表来作为有序集合键的底层实现。和链表、字典等数据结构被广泛地应用在Redis内部不同，Redis只在两个地方用到了跳跃表，一个是实现有序集合键，另一个是在集群节点中用作内部数据结构，除此之外，跳跃表在Redis里面没有其他用途。\n跳跃表数据结构 跳跃表核心数据结构有两个：\n zskiplistNode：同一个节点在每层的score和sds是一致的；backward指针用于从尾到头遍历，跨度恒为1；每一层会变化的则放在zskiplistLevel结构中； zskiplist：保存跳跃表节点的相关信息；  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  typedef struct zskiplistNode { sds ele; // sds  double score; // 得分，按分数从小到大排序  struct zskiplistNode *backward; // 从尾到头遍历  struct zskiplistLevel { struct zskiplistNode *forward; // 从头到尾遍历  unsigned long span; // 跨度  } level[]; // 最大32层 } zskiplistNode; typedef struct zskiplist { struct zskiplistNode *header, *tail; // 头尾指针，不存实际的数据  unsigned long length; // 节点数量  int level; // level最大值（不包括头节点） } zskiplist;   跳跃表的创建 见注释：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  zskiplistNode *zslCreateNode(int level, double score, sds ele) { zskiplistNode *zn = zmalloc(sizeof(*zn)+level*sizeof(struct zskiplistLevel)); zn-\u003escore = score; zn-\u003eele = ele; return zn; } zskiplist *zslCreate(void) { int j; zskiplist *zsl; zsl = zmalloc(sizeof(*zsl)); zsl-\u003elevel = 1; // 1. zsl的初始level是1  zsl-\u003elength = 0; zsl-\u003eheader = zslCreateNode(ZSKIPLIST_MAXLEVEL,0,NULL); // 2. 头节点的level = 32，但是不计入zsl.level中  for (j = 0; j \u003c ZSKIPLIST_MAXLEVEL; j++) { zsl-\u003eheader-\u003elevel[j].forward = NULL; // 3. 头节点每一层的前进指针置空、跨度为0  zsl-\u003eheader-\u003elevel[j].span = 0; } zsl-\u003eheader-\u003ebackward = NULL; zsl-\u003etail = NULL; // 4.  return zsl; }   跳跃表的插入 需要关注的几个点是：\n rank数组：表示每一层遍历到截止点的跨度（经历的节点数量）； update数组：插入新节点后会受到影响的节点（每一层都会有一个）； 随机生成level： 有了rank和update后，后面的操作可以看成每一个层级对双向链表的操作；  具体见注释：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49  zskiplistNode *zslInsert(zskiplist *zsl, double score, sds ele) { zskiplistNode *update[ZSKIPLIST_MAXLEVEL], *x; unsigned int rank[ZSKIPLIST_MAXLEVEL]; int i, level; serverAssert(!isnan(score)); x = zsl-\u003eheader; for (i = zsl-\u003elevel-1; i \u003e= 0; i--) { // 1. 从最高level往下遍历  rank[i] = i == (zsl-\u003elevel-1) ? 0 : rank[i+1]; // 2. rank表示当前新插入的节点在每一层的跨度  while (x-\u003elevel[i].forward \u0026\u0026 (x-\u003elevel[i].forward-\u003escore \u003c score || // 3. 比对分值，forward节点的score小于待插入的score，则继续遍历；  (x-\u003elevel[i].forward-\u003escore == score \u0026\u0026 sdscmp(x-\u003elevel[i].forward-\u003eele,ele) \u003c 0))) // 4. score一致时，比对成员，如果forward节点的sds小于待插入的sds，则继续遍历；  { rank[i] += x-\u003elevel[i].span; // 5. 记录沿途跨越了多少个节点  x = x-\u003elevel[i].forward; // 6. 移动至下一指针  } update[i] = x; // 7. 记录当前层i遍历到的截止点（新节点会被插入在该截止点后面）；  } // zslInsert() 的调用者会确保同分值且同成员的元素不会出现，所以这里不需要进一步进行检查，可以直接创建新元素。  level = zslRandomLevel(); // 8. 获取一个随机值作为新节点的层数  if (level \u003e zsl-\u003elevel) { // 9. 如果新节点的层数比表中其他节点的层数都要大  for (i = zsl-\u003elevel; i \u003c level; i++) { rank[i] = 0; // 10.  update[i] = zsl-\u003eheader; // 11.  update[i]-\u003elevel[i].span = zsl-\u003elength; // 12. 将来也指向新节点  } zsl-\u003elevel = level; // 13. 更新zsl的最大层数  } x = zslCreateNode(level,score,ele); // 14. 创建新节点  for (i = 0; i \u003c level; i++) { x-\u003elevel[i].forward = update[i]-\u003elevel[i].forward; // 15. 设置新节点的forward指针  update[i]-\u003elevel[i].forward = x; // 16. 将各层的截止点的forward指针指向新节点  x-\u003elevel[i].span = update[i]-\u003elevel[i].span - (rank[0] - rank[i]); //17. 计算新节点在各层跨越的节点数量  update[i]-\u003elevel[i].span = (rank[0] - rank[i]) + 1; // 18. 更新新节点插入之后，截止点的span值  } for (i = level; i \u003c zsl-\u003elevel; i++) { update[i]-\u003elevel[i].span++; // 19. 新层的span值也需要增一，编号11：update[i]对应着头部，也就是头部节点到新节点的跨度  } x-\u003ebackward = (update[0] == zsl-\u003eheader) ? NULL : update[0]; // 20. 设置新节点的后退指针  if (x-\u003elevel[0].forward) x-\u003elevel[0].forward-\u003ebackward = x; else zsl-\u003etail = x; zsl-\u003elength++; // 21. 跳跃表的节点计数增一  return x; }   跳跃表的删除 类似于插入，暂不分析。\n源码文件  src/server.h src/t_zset.c  ","description":"","tags":["redis源码分析"],"title":"redis源码分析四：跳跃表","uri":"/posts/redis/4-zsl/"},{"categories":["redis源码分析"],"content":"redis提供了如下几个dict操作：\n  创建一个dict：dictCreate\n  dict扩张，设置rehash相关参数：dictExpand\n  向dict添加一个{key,vale}，不支持覆盖：dictAdd\n  向dict添加一个{key,vale}，支持覆盖：dictReplace\n  从dict删除一个key：dictDelete\n  dict rehash操作：dictRehash\n   dictDelete\ndictFind\n基本结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36  typedef struct dictEntry { void *key; // 哈希表的键  union { // 联合体，键值可以是指针、uint64、int64_t或者double  void *val; uint64_t u64; int64_t s64; double d; } v; struct dictEntry *next; // 链地址法解决哈希冲突 } dictEntry; typedef struct dictType { uint64_t (*hashFunction)(const void *key); // 自定义哈希算法  void *(*keyDup)(void *privdata, const void *key); // key的自定义拷贝函数  void *(*valDup)(void *privdata, const void *obj); // val的自定义拷贝函数  int (*keyCompare)(void *privdata, const void *key1, const void *key2); // 比较算法  void (*keyDestructor)(void *privdata, void *key); // key的释放  void (*valDestructor)(void *privdata, void *obj); // val的释放 } dictType; /* This is our hash table structure. Every dictionary has two of this as we * implement incremental rehashing, for the old to the new table. */ typedef struct dictht { dictEntry **table; // 哈希表  unsigned long size; // 大小  unsigned long sizemask; // 哈希表大小的掩码  unsigned long used; // 当前哈希表存在dictEntry的个数 } dictht; typedef struct dict { dictType *type; // 提供dict的自定义函数  void *privdata; // 私有数据  dictht ht[2]; // ht[0]作为原始哈希表,ht[1]作为rehash  long rehashidx; // rehash标志位，-1没有rehash  unsigned long iterators; // 安全迭代器的个数，防止rehash和修改操作冲突 } dict;   哈希算法 redis采用的哈希算法默认是siphash算法。\n创建dict 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  dict *dictCreate(dictType *type, void *privDataPtr) { dict *d = zmalloc(sizeof(*d)); // 1. 分配内存：dict  _dictInit(d,type,privDataPtr); return d; } int _dictInit(dict *d, dictType *type, void *privDataPtr) { _dictReset(\u0026d-\u003eht[0]); // 2. 初始化ht[0]  _dictReset(\u0026d-\u003eht[1]); // 3. 初始化ht[1]  d-\u003etype = type; // 4. dictType  d-\u003eprivdata = privDataPtr; // 5. 私有数据  d-\u003erehashidx = -1; // 6. 未进行rehash  d-\u003eiterators = 0; // 7. 迭代器的个数  return DICT_OK; } static void _dictReset(dictht *ht) { ht-\u003etable = NULL; ht-\u003esize = 0; ht-\u003esizemask = 0; ht-\u003eused = 0; }   添加键 添加键需要重点关注以下几个关键点：\n 执行添加键并且安全迭代器个数为0的话可以触发渐进式rehash； _dictKeyIndex根据key获取index，其中必须遍历ht[0]和ht[1]。因为在rehash过程中，对键的修改操作都是直接操作到ht[1]上； 如果在rehash过程中，则直接将键插入到ht[1]上；  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  /* Add an element to the target hash table */ int dictAdd(dict *d, void *key, void *val) { dictEntry *entry = dictAddRaw(d,key,NULL); if (!entry) return DICT_ERR; dictSetVal(d, entry, val);\t// 8. 拷贝复制（如果自定义拷贝函数存在的话）或者直接复制  return DICT_OK; } dictEntry *dictAddRaw(dict *d, void *key, dictEntry **existing) { long index; dictEntry *entry; dictht *ht; if (dictIsRehashing(d)) _dictRehashStep(d);\t// 1. 渐进式rehash，执行单步rehash（但是得确保当前安全迭代器的个数为0）  // 2. 获取index = HashFunction(key)  if ((index = _dictKeyIndex(d, key, dictHashKey(d,key), existing)) == -1) return NULL; // 3. 根据是否在进行rehash，选择哈希表  ht = dictIsRehashing(d) ? \u0026d-\u003eht[1] : \u0026d-\u003eht[0]; entry = zmalloc(sizeof(*entry)); // 4. 分配表项  entry-\u003enext = ht-\u003etable[index]; // 5. 头插法  ht-\u003etable[index] = entry; ht-\u003eused++; // 6. 表项个数  dictSetKey(d, entry, key);\t// 7. 拷贝复制（如果自定义拷贝函数存在的话）或者直接复制  return entry; }   删除键 类似于插入，暂不分析。\n更新键 类似于插入，暂不分析。\nrehash redis rehash分为两种：\n 渐进式rehash：将rehash操作分布在每个增加、删除、修改操作上； 集中式rehash：一次性rehash完成；  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  int dictRehash(dict *d, int n) { int empty_visits = n*10; // 1. 访问到empty_visits个空桶的话直接返回  if (!dictIsRehashing(d)) return 0; while(n-- \u0026\u0026 d-\u003eht[0].used != 0) { // 2. 最多访问n个有效桶  dictEntry *de, *nextde; while(d-\u003eht[0].table[d-\u003erehashidx] == NULL) { // 3. 找到第一个不为空的桶  d-\u003erehashidx++; if (--empty_visits == 0) return 1; } de = d-\u003eht[0].table[d-\u003erehashidx]; while(de) { // 4. 遍历桶内节点  uint64_t h; nextde = de-\u003enext; h = dictHashKey(d, de-\u003ekey) \u0026 d-\u003eht[1].sizemask; // 5. 获取该key在ht[1]上的索引，由于ht[1]比ht[0]的尺寸大或小，因此key对应的index可能会有所不同  de-\u003enext = d-\u003eht[1].table[h]; // 6. 执行头插法，将数据插入到ht[1]  d-\u003eht[1].table[h] = de; d-\u003eht[0].used--; d-\u003eht[1].used++; de = nextde; } d-\u003eht[0].table[d-\u003erehashidx] = NULL; // 7. 清空ht[0]中对应的桶  d-\u003erehashidx++; } if (d-\u003eht[0].used == 0) { // 8. 检查是否已经完成了rehash，恢复相应的标志位，把ht[1]给ht[0]  zfree(d-\u003eht[0].table); d-\u003eht[0] = d-\u003eht[1]; _dictReset(\u0026d-\u003eht[1]); d-\u003erehashidx = -1; return 0; } return 1; }   源码文件  src/dict.c src/dict.h  ","description":"","tags":["redis源码分析"],"title":"redis源码分析三：dict数据结构","uri":"/posts/redis/3-dict/"},{"categories":["redis源码分析"],"content":"redis实现了双向链表，其名称是adlist（A generic Doubly linked list implementation）。adlist提供了以下几种操作方法：\n 创建双向链表 释放双向链表：释放节点、释放节点值、释放双向链表 头插法 尾插法 删除节点 迭代器：头部迭代或者尾部迭代  本文主要讲解了adlist数据结构、创建双向链表和头插法。其余方法不作介绍。\nadlist数据结构 见注释：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  typedef struct listNode { struct listNode *prev; // 前驱节点  struct listNode *next; // 后驱节点  void *value; // 节点值 } listNode; typedef struct listIter { listNode *next; // 迭代器：方便遍历  int direction; // 指明方向，从头到尾还是从尾到头 } listIter; typedef struct list { // 双向链表结构体，注意同节点结构体区分开来  listNode *head; // 头部节点指针  listNode *tail; // 尾部节点指针  void *(*dup)(void *ptr); // 自定义复制函数  void (*free)(void *ptr); // 自定义释放函数  int (*match)(void *ptr, void *key); // 自定义匹配函数，用于查找  unsigned long len; // 链表长度 } list;   创建双向链表 双向链表创建只需要分配双向链表结构体即可，具体见注释：\nlist *listCreate(void) { struct list *list;\nif ((list = zmalloc(sizeof(*list))) == NULL) // 1. 分配内存：struct list return NULL; list-\u003ehead = list-\u003etail = NULL; // 2. 双向链表置空 list-\u003elen = 0; list-\u003edup = NULL; list-\u003efree = NULL; list-\u003ematch = NULL; return list;  }\n双向链表头插 见注释：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  list *listAddNodeHead(list *list, void *value) { listNode *node; if ((node = zmalloc(sizeof(*node))) == NULL) // 1. 分配一个新的节点listNode  return NULL; node-\u003evalue = value; // 2. 保存数据到该节点  if (list-\u003elen == 0) { // 3. list为空，则头尾都指向新节点  list-\u003ehead = list-\u003etail = node; node-\u003eprev = node-\u003enext = NULL; } else { // 4. list不为空，执行头插  node-\u003eprev = NULL; node-\u003enext = list-\u003ehead; list-\u003ehead-\u003eprev = node; list-\u003ehead = node; } list-\u003elen++; // 5. 长度+1  return list; }   源码文件：  src/adlist.h src/adlist.c  ","description":"","tags":["redis源码分析"],"title":"redis源码分析二：adlist数据结构","uri":"/posts/redis/2-adlist/"},{"categories":["redis源码分析"],"content":"本文主要介绍了redis的字符串。\nsds和sds头部结构体 sds（simple dynamic string）是redis管理字符串的一种数据结构。sds结构体是char *类型，指向sds头部结构体的buf。sds头部有五种不同的结构：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  struct sdshdr5 { unsigned char flags; // 低三位表示SDS类型，高五位存储buf的长度  char buf[]; }; struct sdshdr8 { uint8_t len; // buffer的长度  uint8_t alloc; // 字符串的实际长度，不包含头部和终止符  unsigned char flags; // 低三位表示SDS类型  char buf[]; }; struct sdshdr16 { uint16_t len; // buffer的长度  uint16_t alloc; // 字符串的实际长度，不包含头部和终止符  unsigned char flags; // 低三位表示SDS类型  char buf[]; }; struct sdshdr32 { uint32_t len; // buffer的长度  uint32_t alloc; // 字符串的实际长度，不包含头部和终止符  unsigned char flags; // 低三位表示SDS类型  char buf[]; }; struct sdshdr64 { uint64_t len; // buffer的长度  uint64_t alloc; // 字符串的实际长度，不包含头部和终止符  unsigned char flags; // 低三位表示SDS类型  char buf[]; };   创建sds sdsnewlen可以创建一个sds字符串，传入参数是：\n init: 待转换成sds结构体的字符串首地址指针； init_len：字符串长度；  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  sds sdsnewlen(const void *init, size_t initlen) { void *sh; sds s; char type = sdsReqType(initlen); // 1. 根据初始长度选择合适的sds类型  if (type == SDS_TYPE_5 \u0026\u0026 initlen == 0) // 2. 尽量不使用SDS_TYPE_5  type = SDS_TYPE_8; int hdrlen = sdsHdrSize(type); // 3. 根据sds类型获取sds头部长度  unsigned char *fp; sh = s_malloc(hdrlen+initlen+1); // 4. 分配内存，后面的+1是'\\0'所占空间  if (init==SDS_NOINIT) init = NULL; else if (!init) memset(sh, 0, hdrlen+initlen+1); // 5. sds结构体清0  s = (char*)sh+hdrlen; // 6. 指向buf的指针  fp = ((unsigned char*)s)-1; // 7. 指向flags的指针  switch(type) { ...... case SDS_TYPE_8: { SDS_HDR_VAR(8,s); // 8. 根据buf的指针计算出sds头部的首地址，宏定义展开：struct sdshdr8 *sh = (void *)((s)-sizeof(struct sdshdr8))  sh-\u003elen = initlen; sh-\u003ealloc = initlen; *fp = type; break; } ....... } if (initlen \u0026\u0026 init) memcpy(s, init, initlen); // 9. 将数据拷贝到sds的buf里  s[initlen] = '\\0'; // 10. 插入终止符，方便调用printf函数  return s; // 11. 返回的sds是指向buf的指针，可以通过SDS_HDR_VAR得到sds头部的首地址 }   基本语法：  char buf[]不占用内存空间，也就是sizeof(struct sdshdr64)等于24，由于对齐的原因flags被扩充到64bit； char_arry[-1]取的是当前首地址-1的位置数据；  源码文件：  src/sds.h src/sds.c  ","description":"","tags":["redis源码分析"],"title":"redis源码分析一：sds数据结构","uri":"/posts/redis/1-sds/"},{"categories":["linux内核"],"content":"cfs相关结构体 同cfs相关的结构体一共有三个：\n struct sched_class：cfs调度类的实现； cfs_rq：当前cpu使用cfs调度算法的进程队列； sched_entity：调度实体；  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50  /* * All the scheduling class methods: */ const struct sched_class fair_sched_class = { .next\t= \u0026idle_sched_class, .enqueue_task\t= enqueue_task_fair,\t// 将进程加入到cfs rq \t.dequeue_task\t= dequeue_task_fair,\t// 从cfs rq中删除 \t.yield_task\t= yield_task_fair, .yield_to_task\t= yield_to_task_fair, .check_preempt_curr\t= check_preempt_wakeup, .pick_next_task\t= __pick_next_task_fair,\t// 选择下一个进程 \t.put_prev_task\t= put_prev_task_fair, .set_next_task = set_next_task_fair, .task_tick\t= task_tick_fair, .task_fork\t= task_fork_fair,\t// 新进程创建，参数预处理，确定其vruntime \t.prio_changed\t= prio_changed_fair, .switched_from\t= switched_from_fair, .switched_to\t= switched_to_fair, .get_rr_interval\t= get_rr_interval_fair, .update_curr\t= update_curr_fair, // 更新进程的vruntime和更新cfsrq min_vruntime }; struct cfs_rq { struct load_weight\tload;\t// 运行队列总的进程权重 \tunsigned long\trunnable_weight; unsigned int\tnr_running;\t// 进程的个数 \tunsigned int\th_nr_running; /* SCHED_{NORMAL,BATCH,IDLE} */ unsigned int\tidle_h_nr_running; /* SCHED_IDLE */ u64\texec_clock;\t// 运行的时钟 \tu64\tmin_vruntime;\t// 该cpu运行队列的vruntime推进值, 一般是红黑树中最小的vruntime值 \tstruct rb_root_cached\ttasks_timeline;\t// 红黑树的根结点，也能获取到left_rbmost \tstruct sched_entity\t*curr;\t// 当前运行进程, 下一个将要调度的进程, 马上要抢占的进程, \tstruct sched_entity\t*next; struct sched_entity\t*last; struct sched_entity\t*skip; }; struct sched_entity { struct load_weight\tload;\t// 进程的权重 \tunsigned long\trunnable_weight;\tstruct rb_node\trun_node;\t// 运行队列中的红黑树结点 \tstruct list_head\tgroup_node;\t// 与组调度有关 \tunsigned int\ton_rq;\t// 进程现在是否处于TASK_RUNNING状态  u64\texec_start;\t// 一个调度tick的开始时间 \tu64\tsum_exec_runtime;\t// 进程从出生开始, 已经运行的实际时间 \tu64\tvruntime;\t// 虚拟运行时间 \tu64\tprev_sum_exec_runtime;\t// 本次调度之前, 进程已经运行的实际时间 };   cfs调度关键参数  每个调度实体的vruntime：每次选择vruntime最小的调度实体去运行； cfs_rq的min_vruntime：该值是单调递增的，主要是用于计算新进程、从另外一个cpu迁移过来的进程和睡眠进程的vruntime的计算；  cfs相关算法：物理时间和虚拟时间对应关系 $$ delta = delta_exec * \\frac{NICE_0_LOAD}{curr-\u003eload.weight} $$\n delta_exec：curr运行的实际物理时间； NICE_0_LOAD：nice值为0的权重，也就是1024； curr-\u003eload.weight：进程的权重； delta：进程虚拟时间的增加值  cfs调度基础 下面脑图描述了创建新进程后，经历的流程。特别需要注意update_curr和place_entity这两个函数，update_curr负责更新当前运行进程的vruntime和更新cfs rq的min_vruntime。当有新的进程加入该cfs rq时，place_entity计算该新进程合理的vruntime。结合脑图有以下几个问题：\n 如何保证min_vruntime的单调递增？ min_vruntime在update_curr函数里被更新，可以知道min_vruntime = max(cfs_rq当前的min_vunrtime,cfs_rq curr的vruntime,cfs leftmost的vruntime)； 在task_for_fair函数里，新进程的vruntime为什么要减去cfs_rq-\u003emin_vruntime呢？ 因为父进程和新进程可能是在不同的cpu上运行的，不同cpu的cfs rq的min_vruntime可能是不同的，所以可以等到将该进程部署到具体的cpu时，再加上对应的cfs_rq-\u003emin_vruntime； 在place_entity函数中，如果是新进程，则其vruntime添加惩罚值（延迟跟踪机制），如果是睡眠进程唤醒的话，则减去一定的thresh（以便其尽快得到调度）； 在place_entity函数中，为什么进程最终的vruntime取一个最大值？主要是考虑到睡眠进程唤醒vruntime的更新。假设一个进程睡眠1ms，如果thresh为3ms，则会导致该调度实体的vruntime回退。  cfs调度比较器 vruntime是unsigned，首先将二者vruntime相减，然后结果转换成signed，最后同0比较，便可以得到正确的大小关系。\n1 2 3 4 5  static inline int entity_before(struct sched_entity *a, struct sched_entity *b) { return (s64)(a-\u003evruntime - b-\u003evruntime) \u003c 0; }   举个例子，当其中一个溢出的时候，判断是否能够得到正确的大小关系：\n1 2 3 4 5 6 7 8 9 10 11 12 13  #include \u003ciostream\u003e#include \u003cmemory\u003eusing namespace std; int main() { unsigned long long int vruntime1 = ULLONG_MAX; unsigned long long int vruntime2 = ULLONG_MAX; long long int res = ((vruntime1 + 5) - vruntime2); cout \u003c\u003c res \u003c\u003c endl;\t// 5 正确  res = (vruntime1 - (vruntime2 + 5)); cout \u003c\u003c res \u003c\u003c endl;\t// -5 正确  return 0; }     ","description":"","tags":["linux内核"],"title":"linux完全公平调度源码解析","uri":"/posts/linux-kernel/cfs/"},{"categories":["面试"],"content":" 基于linux内核有做过哪些方面的工作?\n  linux内核有哪些子系统? 对哪个子系统比较熟悉?\n  进程管理：进程创建、进程调度 系统调用： 中断管理：注册中断、上半部和下半部、中断上下文、中断控制 内存管理：页、区、kmalloc、vmalloc、伙伴系统、slab、虚拟内存 虚拟文件系统： 块IO：块设备、bio、请求队列、I/O调度 设备与模块： 网络：   linux内核内存分配策略? 假如申请128MB,系统会分配多少内存\n  malloc分配小于128KB时，采用sbrk函数移动堆顶，内存分配在堆区； malloc分配大于128KB时，采用mmap系统调用，内存分配映射区； 无论malloc通过sbrk还是mmap实现，分配到的内存只是虚拟内存，而且只是虚拟内存的页号，代表这块空间进程可以用，实际上还没有分配到实际的物理页面。   整个系统由1G的内存,可以用malloc分配1G以上的内存吗?\n 主要是看进程的虚拟地址空间，实际的物理地址空间不够的话可以采用请求分页，将内存暂存到外存，需要时调入；\n malloc申请之后会不会不分配物理内存,只是分配虚拟地址空间?\n 会的；\n libc的库,还知道其他的内存库吗? tcmalloc tpmalloc有听过吗?\n  你对cache怎么理解的\n  cpu cache page cache   uma机器的l1 l2 l3的区别? l1和l2有差异吗?\n uma：cpu共享物理内存 numa：每个cpu都有自己的物理内存，访问自己的物理内存较快，访问其他cpu的物理内存较慢\nl1 l2 l3的差异：\n 速度、价格和容量 l1分为指令和数据缓存，l2和l3则部分 l1和l2在每个cpu核中，l3则是所有cpu共享的 linux针对l2会有个cache color算法   理解超线程概念吗?l2是超线程之间可以共享的\n 超线程的基础是在物理cpu核心之上 虚拟出来的两个逻辑cpu核心，两颗逻辑cpu核心共享执行资源，包括：执行引擎，高速缓存，以及系统总线。最重要的是两个逻辑核心各自有一套自己的线程状态存储设施：控制寄存器，通用寄存器。从而调度器可以同时调度两个线程，这个是关键。最终这么做的目的是充分利用执行引擎。比如：一个线程在执行定点数运算时，另外一个线程可以同时执行浮点数运算。\n 磁盘的page cache回收时怎么回收(lru,回收的时机?)\n  页高速缓存（cache）是Linux 内核实现磁盘缓存。它主要用来减少对磁盘的I/O操作。具体地讲，是通过把磁盘中的数据缓存到物理内存中，把对磁盘的访问变为对物理内存的访问。 写缓存策略：写直通法、回写法（linux目前采用的）； 缓存回收：LRU、双链策略（LRU/n） 时机（回收的是干净页，所以首先对脏页进行回写）[这些工作由内核flusher线程完成]： a. 空闲内存低于一个特定的阈值时 b. 当脏页在内存中驻留的时间超过一个特定的阈值 c. 用户进程调用sync()和fsync   linux内存去向哪里?(应该通过某个命令行查看系统统计)\n 查看整个系统的：cat /proc/meminfo 查看某个进程的：cat /proc/\u003cpid\u003e/maps\n slab和伙伴系统是什么关系\n slab： 在linux内核中伙伴系统用来管理物理内存，其分配的基本单位是页。由于伙伴系统分配的粒度又太大，因此linux采用slab分配器提供动态内存的管理功能，而且可以作为经常分配并释放的对象的高速缓存。slab分配器的优点：\n 可以提供小块内存的分配支持，通用高速缓存可分配的大小从到，专用高速缓存没有大小限制； 不必每次申请释放都和伙伴系统打交道，提供了分配释放效率； 如果在slab缓存的话，其在CPU高速缓存的概率也会较高； 伙伴系统对系统的数据和指令高速缓存有影响，slab分配器采用着色降低了这种副作用； cat /proc/slabinfo可以查看slab的分配信息。  伙伴系统\n cat /proc/buddyinfo可以查看伙伴系统的分配信息。   怎么使用slab\n  通用缓存：kmalloc 高速缓存：kmem_cache结构表示一个高速缓存，该结构包含三个链表：slabs_full、slabs_partial和slabs_empty；   kmalloc和kvmalloc和vmalloc\n  kmalloc保证分配的内存在物理上是连续的保证分配的内存在物理上是连续的； vmalloc保证的是在虚拟地址空间上的连续，开销更大； kvmalloc先调用kmalloc，如果失败在调用vmalloc。  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  //下面是kvmalloc代替kmalloc和vmalloc的样例  static void *seq_buf_alloc(unsigned long size) { -\tvoid *buf; -\tgfp_t gfp = GFP_KERNEL; - -\t/* -\t* For high order allocations, use __GFP_NORETRY to avoid oom-killing - -\t* it's better to fall back to vmalloc() than to kill things. For small -\t* allocations, just use GFP_KERNEL which will oom kill, thus no need -\t* for vmalloc fallback. -\t*/ -\tif (size \u003e PAGE_SIZE) -\tgfp |= __GFP_NORETRY | __GFP_NOWARN; -\tbuf = kmalloc(size, gfp); -\tif (!buf \u0026\u0026 size \u003e PAGE_SIZE) -\tbuf = vmalloc(size); -\treturn buf; +\treturn kvmalloc(size, GFP_KERNEL); }    做过哪些驱动,什么类型的\n  自己写一个字符驱动的需要哪些步骤\n  上下文切换什么时候需要进行切换\n  上下文切换大概需要多少时间\n  io uring做了什么改进\n  io_uring 为了避免在提交和完成事件中的内存拷贝，设计了一对共享的 ring buffer 用于应用和内核之间的通信。其中，针对提交队列（SQ），应用是 IO 提交的生产者（producer），内核是消费者（consumer）；反过来，针对完成队列（CQ），内核是完成事件的生产者，应用是消费者； 异步化就是不阻塞当前进程的上下文，所以只要能够使用其他线程来操作 buffer io 的内存拷贝等核心操作，我认为也是可以支持的。Linux aio 没有这么做，我理解是这么做效率不高，同时还是有可能因为等待线程池资源而阻塞。io_uring 最新版本已经优化了buffer；   io uring对上下文切换有什么影响吗?怎么减少切换次数\n  io uring在哪个版本支持\n  更换内核吗\n  协程有了解过吗\n  协程一般用在什么场合\n  协程和线程有什么优势\n  虚拟机迁移\n  kvm qemu k8s docker\n  bio起什么作用\n  bio结构体表示内核中I/O操作；   文件系统有了解吗\n  性能优化有了解吗\n  ebpf怎么看\n  写过bcc工具吗\n  libbpf是干什么的\n eBPF 的易用性方面涉及很多问题：\n 兼容性问题，涉及到对内核数据结构的依赖，导致编写的BPF程序无法复用，容易不工作。 BPF是一种字节码，类似汇编语言，较难使用，需要高层次的语言来方便编写。 BPF要执行，需要一系列的动作，包括编写代码，加载进内核，从内核读取结果数据，卸载退出等步骤。  BCC：\n 针对这些问题，BCC（bpf compiler collection）这个工具集专门出现来弥补这些问题 BCC编写程序采用c的子集作为前端 编写完毕后自动调用clang进行编译 llvm生成ebpf字节码 加载程序到内核 程序退出时自动从内核卸载bpf程序  BCC还有各种前端语言辅助进行变成，前端语言主要是用户态用来处理加载到内核态bpf程序的输出和交互。python支持较好，也支持go。 但是兼容性问题并不好，首先，bcc是类似一种动态语言的方式，每次执行都会进行编译，编译需要工具链和依赖内核头文件（需要安装kernel-header包），而编译依赖是最脆弱，容易失败的。\nlibbpf：\n libbpf的出现，目标是为了使得bpf程序像其它程序一样，编译好后，可以放在任何一台机器，任何一个kernel版本上运行（当然要对内核版本有一些要求）。 头文件的问题，依赖内核态特性支持BTF，将内核的数据结构类型构建在内核中。用户态的程序可以导出BTF成一个单独的.h 头文件，bpf程序只要依赖这个头文件就行，不需要安装内核头文件的包了。 兼容性问题，使用clang-11的针对ebpf后端专门的特性：preserve_access_index，支持记录数据结构field相对位置信息，可实现relocation，从而解决了数据结构在不同内核版本间的变化带来的无法对应问题。 性能提升：内核中bpf模块做了一些增强，bpf verifier支持直接字段访问，不需要call bpf函数的方式来访问结构体字段，这样提升了性能。  可以看到，这里其实libbpf是在运行时，对之前编译好的bpf程序做了进一步针对运行平台的处理，即绑定到运行时的操作。而这个操作是依赖内核的BTF支持，是比较轻量的，对外部依赖较小，因而可移植性较好。\n system tab也可以加一些hook\n  system tab和bpf有什么相比\n   bcc用python有什么缺陷吗\n bcc：bpf compiler collection\n 调度这一块\n  tcp网络协议栈和网络基础知识\n 通过socket发送数据：\n应用层：将数据传输给tcp/udp层\n write(socket,buffer,length) 或者 writev(socket,iovector,vectorlen) 系统调用sock_sendmsg __sock_sendmsg根据proto_ops调用tcp_sendmsg或者udp_sendmsg  tcp/udp层：拷贝数据、添加头部信息（序列号）、流量控制和拥塞控制\n 创建struct sk_buff，拷贝用户数据到内核空间 tcp_transmit_skb添加tcp或者udp头部、序列号等信息 queue_xmit：将sk_buff加入到发送队列  ip层：添加ip头、维护路由表、TTL、分段\n ip层首先添加ip头，然后检查路由表和维护TTL，如果发送到网络则调用ip_queue_xmit，否则将该数据转发到上层协议  数据链路层：添加以太网头部、arp 2. dev_queue_xmit\n物理层：网卡驱动\n cgroup有了解吗\n cgroups 是Linux内核提供的一种可以限制单个进程或者多个进程所使用资源的机制，可以对 cpu，内存等资源实现精细化的控制，目前越来越火的轻量级容器 Docker 就使用了 cgroups 提供的资源限制能力来完成cpu，内存等部分的资源控制。\n另外，开发者也可以使用 cgroups 提供的精细化控制能力，限制某一个或者某一组进程的资源使用。比如在一个既部署了前端 web 服务，也部署了后端计算模块的八核服务器上，可以使用 cgroups 限制 web server 仅可以使用其中的六个核，把剩下的两个核留给后端计算模块。\n 中断 系统调用 可延迟函数 做系统资源互斥\n 采用自旋锁。\n 在单cpu，不可抢占内核中，自旋锁为空操作。 在单cpu，可抢占内核中，自旋锁实现为“禁止内核抢占”，并不实现“自旋”。 在多cpu，可抢占内核中，自旋锁实现为“禁止内核抢占” + “自旋”。  NOTE：禁止内核抢占只是关闭“可抢占标志”，而不是禁止进程切换。显式使用schedule或进程阻塞（此也会导致调用schedule）时，还是会发生进程调度的。\n 中断上下部分\n 中断会打断内核中进程的正常调度和运行，调用中断处理函数。在大多数的系统中，当中断到来时可能耗费大量的时间对它进行处理。如果在中断处理函数中没有禁止中断，该中断处理函数执行过程中仍有可能被其他中断打断。所以我们当然希望中断处理函数执行得越快越好。为了在中断执行时间尽可能短的和中断需要处理完大量的工作之间找一个平衡点，将中断分为上下两部分。\n  顶半部完成尽可能少比较紧急的任务，它往往是简单的读取寄存器中的中断状态并清除中断标志后就进行“登记中断”的工作。登记中断：也就是将底半部处理程序挂在底半部执行队列中去。这样上半部执行的速度就会很快，可以服务更多的中断请求。\n  现在，中断处理工作的重心就落在了底半部的头上，它来完成中断事件的绝大多数任务。底半部几乎做了中断处理程序所有的事情，而且可以被新的中断打断，这也是底半部和顶半部的最大不同，因为顶半部往往被设计成不可中断。底半部则相对来说并不是非常紧急的，而且相对比较耗时，不在硬件中断服务程序中执行。\n   中断下部分一般怎么做? 软中断 tasklet(可延迟函数) workqueue\n 可延迟函数：软中断和tasklet(tasklet是基于软中断之上实现的)；\n软中断：\n 产生后并不是马上可以执行，必须要等待内核的调度才能执行。软中断不能被自己打断，只能被硬件中断打断（上半部）（中断优先级的关系）; 可以并发运行在多个CPU上（即使同一类型的也可以）。所以软中断必须设计为可重入的函数（允许多个CPU同时操作），因此也需要使用自旋锁来保护其数据结构。  tasklet：\n由于软中断必须使用可重入函数，这就导致设计上的复杂度变高，作为设备驱动程序的开发者来说，增加了负担。而如果某种应用并不需要在多个CPU上并行执行，那么软中断其实是没有必要的。因此诞生了弥补以上两个要求的tasklet。它具有以下特性：\n 一种特定类型的tasklet只能运行在一个CPU上，不能并行，只能串行执行; 多个不同类型的tasklet可以并行在多个CPU上; 软中断是静态分配的，在内核编译好之后，就不能改变。但tasklet就灵活许多，可以在运行时改变（比如添加模块时）。  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  // linux内核目前使用的软中断 // /kernel/softirq.c const char * const softirq_to_name[NR_SOFTIRQS] = { \"HI\", \"TIMER\", \"NET_TX\", \"NET_RX\", \"BLOCK\", \"IRQ_POLL\", \"TASKLET\", \"SCHED\", \"HRTIMER\", \"RCU\" }; // HI：处理高优先级的tasklet // TIMER：和时钟中断相关的tasklet // NET_TX：把数据包传送到网卡 // BLOCK： // TASKLET：处理常规的tasklet // SCHED // HRTIMER：高精度定时器中断 // RCU  // tasklet动态使用 void tasklet_init(struct tasklet_struct *t, void (*func)(unsigned long), unsigned long data)   workqueue：\n 可延迟函数运行在中断上下文中（软中断的一个检查点就是do_IRQ退出的时候），于是导致了一些问题：软中断不能睡眠、不能阻塞。由于中断上下文出于内核态，没有进程切换，所以如果软中断一旦睡眠或者阻塞，将无法退出这种状态，导致内核会整个僵死。但可阻塞函数不能用在中断上下文中实现，必须要运行在进程上下文中，例如访问磁盘数据块的函数。因此，可阻塞函数不能用软中断来实现。但是它们往往又具有可延迟的特性。 在2.6版的内核中出现了在内核态运行的工作队列（运行在进程上下文）（替代了2.4内核中的任务队列）。它也具有一些可延迟函数的特点（需要被激活和延后执行），但是能够能够在不同的进程间切换，以完成不同的工作。可延迟函数：软中断和tasklet(tasklet是基于软中断之上实现的)；  ","description":"","tags":["面试"],"title":"快手一面复盘 - 内核组","uri":"/posts/%E9%9D%A2%E8%AF%95/%E5%BF%AB%E6%89%8B%E4%B8%80%E9%9D%A2%E5%A4%8D%E7%9B%98/"},{"categories":["文件系统"],"content":"本文主要介绍了fat32文件系统，包括硬盘分区的格式、文件分配表和目录项等重要内容。\n下面简要的描述fat32文件系统比较关键的两个函数：\n  f_mount函数：在指定的硬盘上挂载文件系统（主要是读取引导区和Volume ID）；\n  f_open函数：\na. 首先去根目录区（0x02H簇号），找到该文件的根目录（主要是匹配目录项，一个目录项占32B，所以一个簇大概$2^n512/32=2^n16$个目录项）；\nb. 查看当前目录项属性，如果是子目录，则查找下一个目录项（类似于步骤a）。如果是当前要查找的文件，则查找完毕；\n  fat32磁盘空间的划分 首先第一个扇区（512字节）作为引导扇区，其中引导扇区记录了当前硬盘的分区情况，以及每个分区所包含的信息：文件系统类型、分区开始的扇区和占用扇区的个数；\n然后根据对应的分区可以找到该分区的第一个扇区，该扇区称为volume ID。volume ID包含了一些关于当前文件系统的重要信息，以fat32为例：\n 每个扇区占多少字节（总是512字节）； 每个簇有多少个扇区（2的n次方） 保留扇区的个数； 文件分配表的个数（总是2，有一个是备份）； 文件分配表占用的扇区数（根据分区大小计算即可）； 根目录所在的簇（一般是2）；  接着是保留扇区空间；\n再接着是文件分配表区（显示链接法）：\n fat32说明其文件分配表的表项是32位的； 每一个表项的位置代表着一个簇号，该表项的值表示下一个簇号；  最后是数据区，不仅包含了文件数据还包含了文件系统数据。\n引导区 硬盘的第一个扇区称为引导区，也叫主引导扇区（MBR）。Boot Code也就是我们常说的bootloader代码。partition n表示分区，可以看到最多支持四个分区。\n这里我们主要看下分区描述符所占的16字节包含哪些信息？格式如下图所示：\n重要的信息有三个：\n Type code：表明当前文件系统的类型； LBA Begin：当前分区起始扇区号； Number of Sectors：当前分区占有多少扇区；  volume ID 分区的第一个扇区称为volume ID。volume ID包含了一些关于当前文件系统的重要信息：\n 每个扇区占多少字节（总是512字节）； 每个簇有多少个扇区（2的n次方） 保留扇区的个数； 文件分配表的个数（总是2，有一个是备份）； 文件分配表占用的扇区数（根据分区大小计算即可）； 根目录所在的簇（一般是2）；  保留扇区 文件分配表区 文件分配表区是FAT文件系统管理磁盘空间和文件的最重要区域，它保存逻辑盘数据区各簇使用情况信息，采用位示图法来表示，文件所占用的存储空间及空闲空间的管理都是通过FAT实现的。FAT区共保存了两个相同的文件分配表, 便于第一个损坏时, 还有第二个可用。\n一个扇区的大小是512B，那么一个扇区可以存放128个簇号。因此，32bit的值中高7-31位表示扇区号（当前FAT表该簇号所在的扇区），低7位表示偏移（便宜单位是32bit）。\n未被分配使用和已回收的簇相应位置写零，坏簇相应位置填入特定值 0FFFFFF7H 标识，已分配的簇相应位置填入非零值，具体为：如果该簇是文件的最后一簇, 填入的值为 0FFFFFFFH, 如果该簇不是文件的最后一簇，填入的值为该文件占用的下一个簇的簇号，这样，正好将文件占用的各簇构成一个簇链，保存在FAT32 表中。\n数据区 数据区是存放文件数据的地方，这里需要特别考虑根目录区或者目录区。FAT32 文件系统中，根目录作为数据区的一部分，采用与子目录相似的管理方式，这 一点 与FAT12、FAT16明显不同，如FAT16文件系统的根目录区( ROOT区) 是固定区域、固定大小的，占用从FAT区之后紧接着的32个扇区，最多保存 512 个目录项( 其根目录保存的文件数受限的原因在此) ，作为系统区的一部分。FAT32的根目录一般情况下从02H簇开始使用，大小视需要增加，因此根目录下的文件数目不再受最多 512 的限制。\n下图是目录项的格式：\n其中比较重要的有：\n 短文件名：8+3格式； 文件属性：只读、隐藏、是否为子目录等； 簇号（HIgh）：存储文件起始簇的簇号的高16位； 簇号（Low）：存储文件起始簇的簇号的低16位； 大小：文件的长度；  参考文献 [1]. Understanding FAT32 Filesystems. https://www.pjrc.com/tech/8051/ide/fat32.html.\n[2]. 张明亮, 张宗杰. 浅析FAT32文件系统[J]. 计算机与数字工程, 2005(01):56-59.\n","description":"","tags":["文件系统"],"title":"Fat32文件系统详解","uri":"/posts/filesystem/fat/"},{"categories":["linux内核"],"content":"kprobe是linux内核的一个重要特性，是一个轻量级的内核调试工具，同时它又是其他一些更高级的内核调试工具（比如perf和systemtap）的“基础设施”。利用kprobe可以在内核任何代码位置插入用户代码，因此可以在一些关键点插入kprobe达到收集内核运行信息的目的。kprobe的机制也很简单，就是将被探测的位置的指令替换为断点指令（不考虑jmp优化），断点指令被执行后会通过notifier_call_chain机制来通知kprobes，kprobes会首先调用用户指定的pre_handler接口。执行pre_handler接口后会单步执行原始的指令，如果用户也指定了post_handler接口，会在调用post_handler接口后结束处理。基本的处理过程如下图所示：\n上图是x86平台下的流程，arm平台也大致差不多（除了触发断点的指令不同）。本文只涉及arm64平台的kprobe源码。\nkprobe结构体 kprobe结构体比较简单，只要注意两点即可：\n kprobe_opcode_t *addr、const char *symbol_name和unsigned int offset三者的关系：addr单独使用、symbol_name和offset联合使用。因为symbol_name只能定位到函数的首地址，加上offset就可以定位到函数内部的任意一条指令； fault_handler：  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  typedef int (*kprobe_pre_handler_t) (struct kprobe *, struct pt_regs *); // typedef void (*kprobe_post_handler_t) (struct kprobe *, struct pt_regs *, unsigned long flags); typedef int (*kprobe_fault_handler_t) (struct kprobe *, struct pt_regs *, int trapnr); typedef int (*kretprobe_handler_t) (struct kretprobe_instance *, struct pt_regs *); struct kprobe { struct hlist_node hlist; // 所有注册的kprobe都会添加到kprobe_table哈希表中 \tstruct list_head list; // 如果在同一个位置注册了多个kprobe，这些kprobe会形成一个队列 \tunsigned long nmissed; kprobe_opcode_t *addr; // 探测的地址 \tconst char *symbol_name; // 探测点的符号名称。名称和地址不能同时指定，否则注册时会返回EINVAL错误 \tunsigned int offset; // 探测点相对于符号地址的偏移，同symblo_name联合使用 \tkprobe_pre_handler_t pre_handler; // \tkprobe_post_handler_t post_handler; // \tkprobe_fault_handler_t fault_handler; // \tkprobe_opcode_t opcode; // 被修改的指令保存下来以便返回时恢复 \tstruct arch_specific_insn ainsn; // 保存了探测点原始指令的拷贝。这里拷贝的指令要比opcode中存储的指令多，拷贝的大小为MAX_INSN_SIZE * sizeof(kprobe_opcode_t)。 \tu32 flags; };   register_kprobe 前面讲到kprobe的基本结构，接着我们会看到kprobe的注册流程，下面的脑图已经很清晰了，见脑图：\nbreakpoint handler注册 前面讲到在某个内核地址或者符号注册一个kprobe时，kprobe会将该地址或者符号+offset对应的地址的指令保存下来，然后用触发断点指令替换掉，也就是BRK_OPCODE_KPROBES指令。接下来，我将会介绍到内核如何将断点中断路由给kprobe进行处理。\nkprobe自身也是作为linux内核的一个模块而存在，linux内核启动时会加载并初始化每一个模块，kprobe也不例外。kernel/kprobes.c是初始化kprobe的源码，在init_kprobes会调用arch_init_kprobes，arch_init_kprobes是系统结构相关代码初始化，kprobe的breakpoint handler就是在这个函数内进行注册的。见下面的代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  int __init arch_init_kprobes(void) { register_kernel_break_hook(\u0026kprobes_break_hook);\t// 注册断点kprobe处理函数 \tregister_kernel_step_hook(\u0026kprobes_step_hook);\t// 注册单步kprobe处理函数 \treturn 0; } static struct break_hook kprobes_break_hook = { .imm = KPROBES_BRK_IMM,\t// 这里的立即数就是之前脑图上提到的16位立即数，用于标识kprobe \t.fn = kprobe_breakpoint_handler, }; static struct step_hook kprobes_step_hook = { .fn = kprobe_single_step_handler, };   先简述一下整个流程：\n krpobe模块调用arch_init_kprobes； register_kernel_break_hook：当内核出现断点中断时，内核会检查出现断点的位置的断点指令（同步中断的基础概念），根据断点指令的立即数同break_hook结构体的imm成员比较，匹配成功则调用回调函数，执行进一步的处理； register_kernel_step_hook：同break_hook不同，step_hook没有立即数，但是kprobe会检查单步的指令地址来判断是否是自己发出的单步请求。  1 2 3 4 5 6 7 8 9 10 11  struct break_hook { struct list_head node; int (*fn)(struct pt_regs *regs, unsigned int esr); u16 imm; // 这里的立即数就是之前脑图上提到的16位立即数，用于标识kprobe \tu16 mask; /* These bits are ignored when comparing with imm */ }; struct step_hook { struct list_head node; int (*fn)(struct pt_regs *regs, unsigned int esr); };   下面的脑图详尽的描述了kprobe的kprobe_breakpoint_handler和kprobe_single_step_handler函数流程。需要注意以下两点：\n kprobe对于post handler的触发做了优化，也就是判断探测地址的指令是否支持模拟，如果支持模拟直接进行模拟，省去了单步的开销； 需要检查单步的请求是否是自己发出的，因为单步没有立即数用来区分是否是kprobe发出的单步。  参考阅读  An introduction to KProbes  ","description":"","tags":["linux内核"],"title":"linux Kprobe源码解析","uri":"/posts/linux-kernel/kprobe/"},{"categories":["linux内核"],"content":"2001年，Eric Dumazet往bpf添加了jit（just in time）compiler功能，加速了bpf filter程序运行时间（包含编译+运行）。在上一篇博文，我们知道bpf的原始filter程序是通过翻译一条指令也代表着执行一条指令（没有转换成对应机器的机器码的过程），没有编译的过程。jit编译器可以将bpf的filter程序（伪代码）编译成可以直接在主机运行的机器码。该patch实现了bfp程序到x86_64机器码的转换。\n本文简单介绍了jit编译器的原理，并有实例分析。\njit编译器 jit编译器的出现更多的是为了不同指令集之间的转换，对应着也就是跨平台的实现。以java和c为例，java的运行程序（字节码）可以在任意平台上运行，而c语言运行程序不行。这是因为java在不同的平台都配置有相应的jit编译器。在程序运行的时候，jit编译器会将字节码转换成对应平台的指令，然后执行。\n  为什么不将程序编译好后运行，而是要运行时编译，再执行？\n静态编译能够对程序做以比jit编译器更大的优化，但是静态编译生成的可执行文件并不能跨平台。jit编译器虽然优化能力有限，但是能够对一条指令在不同的平台实现对应的转换，达到跨平台的目的。\n  bpf jit编译器 本节介绍了bpf的jit编译器的具体实现，为了更好的了解jit编译器实现的原理。我们先看一个具体示例，即bpf filter程序和编译后的程序对比；最后我们给出jit编译器的源码，分析整个的工作流程。\nbpf filter程序使用的寄存器都是伪寄存器，jit首先要将其与物理寄存器对应起来：\n  BPF的accumulator寄存器对应着x86_64平台下的eax寄存器；\n  BPF的index寄存器对应着x86_64平台下的ebx寄存器；\n  使用rdi寄存器存放skb的首地址；\n  使用rbp存放frame的首地址；\n  r9存放头部长度；\n  r8存放数据的首地址；\n  # jit filter程序 (000) ldh [12] (001) jeq #0x800 jt 2\tjf 6 (002) ld [26] (003) jeq #0xc0a80001 jt 12\tjf 4 (004) ld [30] (005) jeq #0xc0a80001 jt 12\tjf 13 (006) jeq #0x806 jt 8\tjf 7 (007) jeq #0x8035 jt 8\tjf 13 (008) ld [28] (009) jeq #0xc0a80001 jt 12\tjf 10 (010) ld [38] (011) jeq #0xc0a80001 jt 12\tjf 13 (012) ret #65535 (013) ret #0 # jit编译后的机器码 JIT code: ffffffffa00d5000: 55 48 89 e5 48 83 ec 60 48 89 5d f8 44 8b 4f 60 JIT code: ffffffffa00d5010: 44 2b 4f 64 4c 8b 87 b8 00 00 00 be 0c 00 00 00 JIT code: ffffffffa00d5020: e8 24 1b 2f e1 3d 00 08 00 00 75 24 be 1a 00 00 JIT code: ffffffffa00d5030: 00 e8 fe 1a 2f e1 3d 01 00 a8 c0 74 43 be 1e 00 JIT code: ffffffffa00d5040: 00 00 e8 ed 1a 2f e1 3d 01 00 a8 c0 74 32 eb 37 JIT code: ffffffffa00d5050: 3d 06 08 00 00 74 07 3d 35 80 00 00 75 29 be 1c JIT code: ffffffffa00d5060: 00 00 00 e8 cc 1a 2f e1 3d 01 00 a8 c0 74 11 be JIT code: ffffffffa00d5070: 26 00 00 00 e8 bb 1a 2f e1 3d 01 00 a8 c0 75 07 JIT code: ffffffffa00d5080: b8 ff ff 00 00 eb 02 31 c0 c9 c3 # 机器码转换成便于分析的汇编代码 0000: push %rbp mov %rsp,%rbp 0004: subq $96,%rsp 0008: mov %rbx, -8(%rbp) 000c: mov off8(%rdi),%r9d 0010: sub off8(%rdi),%r9d 0014: mov off32(%rdi),%r8 b8 00 00 00 001c: mov 0x0c,%esi 0020: call function(addr is 0xe12f1b24) 0025: cmp #0x800,%eax 002a: jne 0x24 002c: mov $26,%esi 0031: call function(addr is 0xe12f1afe) 0036: cmp $0xc0a80001,%eax 003b: je 0x43 003d: mov $30,%esi 0042: call function(addr is 0xe12f1aed) 0047: cmp $0xc0a80001,%eax 004c: je 0x32 004e: jmp 0x37 0050: cmp $0x806,%eax 0055: je 0x07 0057: cmp $0x8035,%eax 005d: jne 0x29 005e: jmp 0x1c 0063: call function(addr is 0xe12f1acc) 0068: cmp $0xc0a80001,%eax 006d: je 0x11 006f: mov 0x26,%esi 0074: call function(addr is 0xe12f1abb) 0079: cmp $0xc0a80001,%eax 007e: jne 0x07 mov $65535,%eax jmp 02 xor %eax,%eax leaveq ret 下面是jit编译的源码，有删除一部分。同时，给了一部分的注释：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445  #include \u003clinux/moduleloader.h\u003e#include \u003casm/cacheflush.h\u003e#include \u003clinux/netdevice.h\u003e#include \u003clinux/filter.h\u003e/* * Conventions : * EAX : BPF A accumulator * EBX : BPF X accumulator * RDI : pointer to skb (first argument given to JIT function) * RBP : frame pointer (even if CONFIG_FRAME_POINTER=n) * r9d : skb-\u003elen - skb-\u003edata_len (headlen) * r8 : skb-\u003edata */ int bpf_jit_enable __read_mostly; /* * assembly code in net/core/bpf_jit.S */ extern u8 sk_load_word[], sk_load_half[], sk_load_byte[], sk_load_byte_msh[]; extern u8 sk_load_word_ind[], sk_load_half_ind[], sk_load_byte_ind[]; static inline u8 *emit_code(u8 *ptr, u32 bytes, unsigned int len) { if (len == 1) *ptr = bytes; else if (len == 2) *(u16 *)ptr = bytes; else { *(u32 *)ptr = bytes; barrier(); } return ptr + len; } #define EMIT(bytes, len)\tdo { prog = emit_code(prog, bytes, len); } while (0)  #define EMIT1(b1)\tEMIT(b1, 1) #define EMIT2(b1, b2)\tEMIT((b1) + ((b2) \u003c\u003c 8), 2) #define EMIT3(b1, b2, b3)\tEMIT((b1) + ((b2) \u003c\u003c 8) + ((b3) \u003c\u003c 16), 3) #define EMIT4(b1, b2, b3, b4) EMIT((b1) + ((b2) \u003c\u003c 8) + ((b3) \u003c\u003c 16) + ((b4) \u003c\u003c 24), 4) #define EMIT1_off32(b1, off)\tdo { EMIT1(b1); EMIT(off, 4);} while (0)  #define CLEAR_A() EMIT2(0x31, 0xc0) /* xor %eax,%eax */#define CLEAR_X() EMIT2(0x31, 0xdb) /* xor %ebx,%ebx */ static inline bool is_imm8(int value) { return value \u003c= 127 \u0026\u0026 value \u003e= -128; } static inline bool is_near(int offset) { return offset \u003c= 127 \u0026\u0026 offset \u003e= -128; } #define EMIT_JMP(offset)\t\\ do {\t\\ if (offset) {\t\\ if (is_near(offset))\t\\ EMIT2(0xeb, offset); /* jmp .+off8 */\\ else\t\\ EMIT1_off32(0xe9, offset); /* jmp .+off32 */\\ }\t\\ } while (0)  /* list of x86 cond jumps (. + s8) * Add 0x10 (and an extra 0x0f) to generate far jumps (. + s32) */ #define X86_JB 0x72 #define X86_JAE 0x73 #define X86_JE 0x74 #define X86_JNE 0x75 #define X86_JBE 0x76 #define X86_JA 0x77  #define EMIT_COND_JMP(op, offset)\t\\ do {\t\\ if (is_near(offset))\t\\ EMIT2(op, offset); /* jxx .+off8 */\\ else {\t\\ EMIT2(0x0f, op + 0x10);\t\\ EMIT(offset, 4); /* jxx .+off32 */\\ }\t\\ } while (0)  #define COND_SEL(CODE, TOP, FOP)\t\\ case CODE:\t\\ t_op = TOP;\t\\ f_op = FOP;\t\\ goto cond_branch  void bpf_jit_compile(struct sk_filter *fp) { u8 temp[64]; // 编译后生成的机器码 \tu8 *prog; unsigned int proglen, oldproglen = 0; int ilen, i; int t_offset, f_offset; u8 t_op, f_op, seen = 0, pass; u8 *image = NULL; u8 *func; int pc_ret0 = -1; /* bpf index of first RET #0 instruction (if any) */ unsigned int cleanup_addr; unsigned int *addrs; const struct sock_filter *filter = fp-\u003einsns; // filter程序的指令个数 \tint flen = fp-\u003elen; if (!bpf_jit_enable) return; addrs = kmalloc(flen * sizeof(*addrs), GFP_KERNEL); if (addrs == NULL) return; /* Before first pass, make a rough estimation of addrs[] * each bpf instruction is translated to less than 64 bytes */ for (proglen = 0, i = 0; i \u003c flen; i++) { proglen += 64; addrs[i] = proglen; } cleanup_addr = proglen; /* epilogue address */ for (pass = 0; pass \u003c 10; pass++) { /* no prologue/epilogue for trivial filters (RET something) */ proglen = 0; prog = temp; // 第一遍不做翻译，只是浏览整个程序是否包含某些指令，主要是为了处理prologue和epilogue \tif (seen) { // prologue \tEMIT4(0x55, 0x48, 0x89, 0xe5); /* push %rbp; mov %rsp,%rbp */ EMIT4(0x48, 0x83, 0xec, 96);\t/* subq $96,%rsp\t*/ /* note : must save %rbx in case bpf_error is hit */ // 保存callee register \tif (seen \u0026 (SEEN_XREG | SEEN_DATAREF)) EMIT4(0x48, 0x89, 0x5d, 0xf8); /* mov %rbx, -8(%rbp) */ if (seen \u0026 SEEN_XREG) CLEAR_X(); /* make sure we dont leek kernel memory */ // 传递参数 \tif (seen \u0026 SEEN_DATAREF) { if (offsetof(struct sk_buff, len) \u003c= 127) /* mov off8(%rdi),%r9d */ EMIT4(0x44, 0x8b, 0x4f, offsetof(struct sk_buff, len)); else { /* mov off32(%rdi),%r9d */ EMIT3(0x44, 0x8b, 0x8f); EMIT(offsetof(struct sk_buff, len), 4); } if (is_imm8(offsetof(struct sk_buff, data_len))) /* sub off8(%rdi),%r9d */ EMIT4(0x44, 0x2b, 0x4f, offsetof(struct sk_buff, data_len)); else { EMIT3(0x44, 0x2b, 0x8f); EMIT(offsetof(struct sk_buff, data_len), 4); } if (is_imm8(offsetof(struct sk_buff, data))) /* mov off8(%rdi),%r8 */ EMIT4(0x4c, 0x8b, 0x47, offsetof(struct sk_buff, data)); else { /* mov off32(%rdi),%r8 */ EMIT3(0x4c, 0x8b, 0x87); EMIT(offsetof(struct sk_buff, data), 4); } } } // 解析bpf的第一条指令 \tswitch (filter[0].code) { case BPF_S_RET_K: case BPF_S_LD_W_LEN: case BPF_S_ANC_PROTOCOL: case BPF_S_ANC_IFINDEX: case BPF_S_ANC_MARK: case BPF_S_ANC_RXHASH: case BPF_S_ANC_CPU: case BPF_S_LD_W_ABS: case BPF_S_LD_H_ABS: case BPF_S_LD_B_ABS: /* first instruction sets A register (or is RET 'constant') */ break; default: CLEAR_A(); /* A = 0 */ } // 编译每一条bpf filter程序指令 \tfor (i = 0; i \u003c flen; i++) { unsigned int K = filter[i].k; // 解析bpf指令的操作码 \tswitch (filter[i].code) { case BPF_S_ALU_ADD_X: /* A += X; */ seen |= SEEN_XREG; EMIT2(0x01, 0xd8);\t/* add %ebx,%eax */ break; case BPF_S_ALU_ADD_K: /* A += K; */ if (!K) break; if (is_imm8(K)) EMIT3(0x83, 0xc0, K);\t/* add imm8,%eax */ else EMIT1_off32(0x05, K);\t/* add imm32,%eax */ break; case BPF_S_ALU_MUL_X: /* A *= X; */ seen |= SEEN_XREG; EMIT3(0x0f, 0xaf, 0xc3);\t/* imul %ebx,%eax */ break; case BPF_S_RET_K: if (!K) { if (pc_ret0 == -1) pc_ret0 = i; CLEAR_A(); } else { EMIT1_off32(0xb8, K);\t/* mov $imm32,%eax */ } /* fallinto */ case BPF_S_RET_A: if (seen) { if (i != flen - 1) { EMIT_JMP(cleanup_addr - addrs[i]); break; } if (seen \u0026 SEEN_XREG) EMIT4(0x48, 0x8b, 0x5d, 0xf8); /* mov -8(%rbp),%rbx */ EMIT1(0xc9);\t/* leaveq */ } EMIT1(0xc3);\t/* ret */ break; case BPF_S_LD_MEM: /* A = mem[K] : mov off8(%rbp),%eax */ seen |= SEEN_MEM; EMIT3(0x8b, 0x45, 0xf0 - K*4); break; case BPF_S_LDX_MEM: /* X = mem[K] : mov off8(%rbp),%ebx */ seen |= SEEN_XREG | SEEN_MEM; EMIT3(0x8b, 0x5d, 0xf0 - K*4); break; case BPF_S_LD_W_LEN: /*\tA = skb-\u003elen; */ BUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, len) != 4); if (is_imm8(offsetof(struct sk_buff, len))) /* mov off8(%rdi),%eax */ EMIT3(0x8b, 0x47, offsetof(struct sk_buff, len)); else { EMIT2(0x8b, 0x87); EMIT(offsetof(struct sk_buff, len), 4); } break; case BPF_S_LDX_W_LEN: /* X = skb-\u003elen; */ seen |= SEEN_XREG; if (is_imm8(offsetof(struct sk_buff, len))) /* mov off8(%rdi),%ebx */ EMIT3(0x8b, 0x5f, offsetof(struct sk_buff, len)); else { EMIT2(0x8b, 0x9f); EMIT(offsetof(struct sk_buff, len), 4); } break; case BPF_S_ANC_CPU: #ifdef CONFIG_SMP \tEMIT4(0x65, 0x8b, 0x04, 0x25); /* mov %gs:off32,%eax */ EMIT((u32)\u0026cpu_number, 4); /* A = smp_processor_id(); */ #else \tCLEAR_A(); #endif \tbreak; case BPF_S_LD_W_ABS: func = sk_load_word; common_load:\tseen |= SEEN_DATAREF; if ((int)K \u003c 0) goto out; t_offset = func - (image + addrs[i]); EMIT1_off32(0xbe, K); /* mov imm32,%esi */ EMIT1_off32(0xe8, t_offset); /* call */ break; case BPF_S_LD_H_ABS: func = sk_load_half; goto common_load; case BPF_S_LD_B_ABS: func = sk_load_byte; goto common_load; case BPF_S_LDX_B_MSH: if ((int)K \u003c 0) { if (pc_ret0 != -1) { EMIT_JMP(addrs[pc_ret0] - addrs[i]); break; } CLEAR_A(); EMIT_JMP(cleanup_addr - addrs[i]); break; } seen |= SEEN_DATAREF | SEEN_XREG; t_offset = sk_load_byte_msh - (image + addrs[i]); EMIT1_off32(0xbe, K);\t/* mov imm32,%esi */ EMIT1_off32(0xe8, t_offset); /* call sk_load_byte_msh */ break; case BPF_S_LD_W_IND: func = sk_load_word_ind; common_load_ind:\tseen |= SEEN_DATAREF | SEEN_XREG; t_offset = func - (image + addrs[i]); EMIT1_off32(0xbe, K);\t/* mov imm32,%esi */ EMIT1_off32(0xe8, t_offset);\t/* call sk_load_xxx_ind */ break; case BPF_S_LD_H_IND: func = sk_load_half_ind; goto common_load_ind; case BPF_S_LD_B_IND: func = sk_load_byte_ind; goto common_load_ind; case BPF_S_JMP_JA: t_offset = addrs[i + K] - addrs[i]; EMIT_JMP(t_offset); break; COND_SEL(BPF_S_JMP_JGT_K, X86_JA, X86_JBE); COND_SEL(BPF_S_JMP_JGE_K, X86_JAE, X86_JB); COND_SEL(BPF_S_JMP_JEQ_K, X86_JE, X86_JNE); COND_SEL(BPF_S_JMP_JSET_K,X86_JNE, X86_JE); COND_SEL(BPF_S_JMP_JGT_X, X86_JA, X86_JBE); COND_SEL(BPF_S_JMP_JGE_X, X86_JAE, X86_JB); COND_SEL(BPF_S_JMP_JEQ_X, X86_JE, X86_JNE); COND_SEL(BPF_S_JMP_JSET_X,X86_JNE, X86_JE); cond_branch: f_offset = addrs[i + filter[i].jf] - addrs[i]; t_offset = addrs[i + filter[i].jt] - addrs[i]; /* same targets, can avoid doing the test :) */ if (filter[i].jt == filter[i].jf) { EMIT_JMP(t_offset); break; } switch (filter[i].code) { case BPF_S_JMP_JGT_X: case BPF_S_JMP_JGE_X: case BPF_S_JMP_JEQ_X: seen |= SEEN_XREG; EMIT2(0x39, 0xd8); /* cmp %ebx,%eax */ break; case BPF_S_JMP_JSET_X: seen |= SEEN_XREG; EMIT2(0x85, 0xd8); /* test %ebx,%eax */ break; case BPF_S_JMP_JEQ_K: if (K == 0) { EMIT2(0x85, 0xc0); /* test %eax,%eax */ break; } case BPF_S_JMP_JGT_K: case BPF_S_JMP_JGE_K: if (K \u003c= 127) EMIT3(0x83, 0xf8, K); /* cmp imm8,%eax */ else EMIT1_off32(0x3d, K); /* cmp imm32,%eax */ break; case BPF_S_JMP_JSET_K: if (K \u003c= 0xFF) EMIT2(0xa8, K); /* test imm8,%al */ else if (!(K \u0026 0xFFFF00FF)) EMIT3(0xf6, 0xc4, K \u003e\u003e 8); /* test imm8,%ah */ else if (K \u003c= 0xFFFF) { EMIT2(0x66, 0xa9); /* test imm16,%ax */ EMIT(K, 2); } else { EMIT1_off32(0xa9, K); /* test imm32,%eax */ } break; } if (filter[i].jt != 0) { if (filter[i].jf) t_offset += is_near(f_offset) ? 2 : 6; EMIT_COND_JMP(t_op, t_offset); if (filter[i].jf) EMIT_JMP(f_offset); break; } EMIT_COND_JMP(f_op, f_offset); break; default: /* hmm, too complex filter, give up with jit compiler */ goto out; } // bpf一条指令编译后的指令长度 \tilen = prog - temp; if (image) { if (unlikely(proglen + ilen \u003e oldproglen)) { pr_err(\"bpb_jit_compile fatal error\\n\"); kfree(addrs); module_free(NULL, image); return; } memcpy(image + proglen, temp, ilen); } proglen += ilen; addrs[i] = proglen; prog = temp; } /* last bpf instruction is always a RET : * use it to give the cleanup instruction(s) addr */ // 处理epilogue \tcleanup_addr = proglen - 1; /* ret */ // 恢复上一个函数的rbp和rsp \t// movq %rbp, %rsp  // popq %rbp \tif (seen) cleanup_addr -= 1; /* leaveq */ // 恢复rbx寄存器的值 \tif (seen \u0026 SEEN_XREG) cleanup_addr -= 4; /* mov -8(%rbp),%rbx */ if (image) { // 编译完成，退出 \t// pass = 2 \tWARN_ON(proglen != oldproglen); break; } // pass = 1 \tif (proglen == oldproglen) { // 为代码分配内存（主要是设置page的属性为可执行，也就是指令而非数据） \t// 这也是动态模块加载可以运行的基本条件 \timage = module_alloc(max_t(unsigned int,proglen,sizeof(struct work_struct))); if (!image) goto out; } // pass = 0 \toldproglen = proglen; } if (bpf_jit_enable \u003e 1) pr_err(\"flen=%d proglen=%u pass=%d image=%p\\n\", flen, proglen, pass, image); if (image) { if (bpf_jit_enable \u003e 1) print_hex_dump(KERN_ERR, \"JIT code: \", DUMP_PREFIX_ADDRESS, 16, 1, image, proglen, false); // 刷新代码到内存 \tbpf_flush_icache(image, image + proglen); fp-\u003ebpf_func = (void *)image; } out: kfree(addrs); return; }   参考文章 [1]. Mccanne S , Jacobson V . The BSD Packet Filter: A New Architecture for User-level Packet Capture[C]// Proceedings of the USENIX Winter 1993 Conference Proceedings on USENIX Winter 1993 Conference Proceedings. USENIX Association, 1993. [2]. A JIT for packet filters. https://lwn.net/Articles/437981/. [3]. net: filter: Just In Time compiler. https://lwn.net/Articles/437884/. [4]. Understanding JIT compiler (just-in-time compiler) for java. https://aboullaite.me/understanding-jit-compiler-just-in-time-compiler/.\n","description":"","tags":["linux内核","linux工具"],"title":"bpf的jit编译器","uri":"/posts/linux-kernel/bpf%E7%9A%84jit%E7%BC%96%E8%AF%91%E5%99%A8/"},{"categories":["linux内核"],"content":"本文主要介绍了bpf论文和bpf源码。\n简介 从下图可以看出，当网卡驱动接收到packet的时，一般情况下会直接传给协议栈。但是当bpf处于工作状态，驱动首先调用bpf程序，bpf程序会执行与其绑定的filter程序。如果该packet通过过滤条件，将该packet拷贝到对应进程的buffer中。由此，bpf程序主要由两部分构成：\n network tap：将网卡驱动收到的packet传给监听的应用程序； filter：过滤掉不需要的packet。  network tap 网卡驱动一旦收到/发出packet，会先调用bpf_tap函数，下面是bpf_tap函数的源码。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  void bpf_tap(arg, pkt, pktlen) caddr_t arg; register u_char *pkt; register u_int pktlen; { struct bpf_if *bp; register struct bpf_d *d; register u_int slen; // arg是和特定网卡绑定的bpf_if，类似于net_if \tbp = (struct bpf_if *)arg; // 遍历所有的监听描述符，该描述符对应着一个进程 \tfor (d = bp-\u003ebif_dlist; d != 0; d = d-\u003ebd_next) { ++d-\u003ebd_rcount; // 执行每个描述符的过滤程序 \tslen = bpf_filter(d-\u003ebd_filter, pkt, pktlen, pktlen); if (slen != 0) // 返回非0，将包传给该描述符，也就是拷贝到该描述符指定的buffer中；  // 1. bpf可以设置immediate参数，每次都唤醒该描述符对应的进程  // 2. 或者等缓冲区满的时候再唤醒该描述符对应的进程，该进程从缓冲区读取数据 \tcatchpacket(d, pkt, pktlen, slen, bcopy); } }   filter filter是bpf实现比较关键的部分，作者首先提出了布尔表达式树和控制流图的方式来计算是否需要该packet，并指出在大部分情况下，控制流图的方式所需要的操作要少一些。\n其次，作者自定义了一些伪机器码，该伪机器码同一般的机器码类似，但是不能够直接运行在真实的机器上，需要进行动态的翻译。\nfilter程序对应的机器模型由两个寄存器，分别是accumulator（记作A）和index register（记作x），下面列举了filter程序的指令格式、指令集及取址模式：\n 指令格式：16bit的操作码，jt是jump true，jf是jump false。   指令集：  ld类型指令：ld,ldh,ldb拷贝数值到寄存器A，ldx拷贝数值到寄存器X；下面是执行ld类型指令的流程； alu类型指令（add,sub...）：使用寄存器A、立即数k、和寄存器x做运算，并将运算结果存储到寄存器A； jump类型指令：比较寄存器A和操作数的值，根据是否相同做跳转； 存储类型指令：st拷贝数值到寄存器A，stx拷贝数值到寄存器X； ret指令：表示结束，以及需要接收字节； tax和txa指令：将寄存器值传递给另外一个寄存器；     取址模式  带#的表示数值在指令格式的k字段上； M[k]：表示分配的临时内存中第k个字节； [k]：packet的偏移地址为k上的数据； L：当前指令所在的偏移地址；    实例分析：\nldh [12]\t# 加载packet偏移地址为12的半字到寄存器A，根据以太网包头部格式知道该半字表示以太网包中的数据类型 jeq #ETHERPROTO_IP, L1, L5\t# 比较A和ETHERPROTO_IP的值，相等去L1，不等去L5 L1: ldb [23]\t# 加载packet偏移地址为23的字节到寄存器A jeq #IPPROTO_TCP, L2, L5\t# 比较A和IPPROTO_TCP的值，相等去L2，不等去L5 L2: ldh [20]\t# 加载packet偏移地址为20的半字到寄存器A jset #0x1fff, L5, L3\t# A\u00260x1fff是否为真，真去L5，假去L3 L3: ldx 4*([14]\u00260xf)\t# (packet[14]\u00260xf)*4加载到寄存器X ldh [x+16]\t# 加载packet偏移地址为x+16的半字到寄存器A jeq #N, L4, L5\t# 比较port和N是否相等，相等去L4，不等去L5 L4: ret #TRUE\t# 返回长度k L5: ret #0\t# 返回长度0 下面是动态翻译的源码，只给了几个示例注释：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329  struct bpf_insn { u_short\tcode;\t// 操作码 \tu_char jt;\t// jump ture 时对应的offset \tu_char jf;\t// jump false时对应的offset \tbpf_int32 k;\t// 立即数 }; u_int bpf_filter(pc, p, wirelen, buflen) register struct bpf_insn *pc; register u_char *p; u_int wirelen; register u_int buflen; { // 寄存器A,X \tregister u_int32 A, X; register int k; // scratch memory：临时内存空间 \tint32 mem[BPF_MEMWORDS]; if (pc == 0) /* * No filter means accept all. */ return (u_int)-1; A = 0; X = 0; --pc; // 遍历所有的伪指令 \twhile (1) { ++pc; // 判断操作码类别 \tswitch (pc-\u003ecode) { default: #ifdef KERNEL \treturn 0; #else \tabort(); #endif  // ret指令且指令k字段有效 \tcase BPF_RET|BPF_K: return (u_int)pc-\u003ek; // ret指令且寄存器A有效 \tcase BPF_RET|BPF_A: return (u_int)A; case BPF_LD|BPF_W|BPF_ABS: k = pc-\u003ek; if (k + sizeof(int32) \u003e buflen) { #ifdef KERNEL \tint merr; if (buflen != 0) return 0; A = m_xword((struct mbuf *)p, k, \u0026merr); if (merr != 0) return 0; continue; #else \treturn 0; #endif \t} A = EXTRACT_LONG(\u0026p[k]); continue; case BPF_LD|BPF_H|BPF_ABS: k = pc-\u003ek; if (k + sizeof(short) \u003e buflen) { #ifdef KERNEL \tint merr; if (buflen != 0) return 0; A = m_xhalf((struct mbuf *)p, k, \u0026merr); continue; #else \treturn 0; #endif \t} A = EXTRACT_SHORT(\u0026p[k]); continue; case BPF_LD|BPF_B|BPF_ABS: k = pc-\u003ek; if (k \u003e= buflen) { #ifdef KERNEL \tregister struct mbuf *m; register int len; if (buflen != 0) return 0; m = (struct mbuf *)p; MINDEX(len, m, k); A = mtod(m, u_char *)[k]; continue; #else \treturn 0; #endif \t} A = p[k]; continue; case BPF_LD|BPF_W|BPF_LEN: A = wirelen; continue; case BPF_LDX|BPF_W|BPF_LEN: X = wirelen; continue; case BPF_LD|BPF_W|BPF_IND: k = X + pc-\u003ek; if (k + sizeof(int32) \u003e buflen) { #ifdef KERNEL \tint merr; if (buflen != 0) return 0; A = m_xword((struct mbuf *)p, k, \u0026merr); if (merr != 0) return 0; continue; #else \treturn 0; #endif \t} A = EXTRACT_LONG(\u0026p[k]); continue; case BPF_LD|BPF_H|BPF_IND: k = X + pc-\u003ek; if (k + sizeof(short) \u003e buflen) { #ifdef KERNEL \tint merr; if (buflen != 0) return 0; A = m_xhalf((struct mbuf *)p, k, \u0026merr); if (merr != 0) return 0; continue; #else \treturn 0; #endif \t} A = EXTRACT_SHORT(\u0026p[k]); continue; case BPF_LD|BPF_B|BPF_IND: k = X + pc-\u003ek; if (k \u003e= buflen) { #ifdef KERNEL \tregister struct mbuf *m; register int len; if (buflen != 0) return 0; m = (struct mbuf *)p; MINDEX(len, m, k); A = mtod(m, u_char *)[k]; continue; #else \treturn 0; #endif \t} A = p[k]; continue; case BPF_LDX|BPF_MSH|BPF_B: k = pc-\u003ek; if (k \u003e= buflen) { #ifdef KERNEL \tregister struct mbuf *m; register int len; if (buflen != 0) return 0; m = (struct mbuf *)p; MINDEX(len, m, k); X = (mtod(m, char *)[k] \u0026 0xf) \u003c\u003c 2; continue; #else \treturn 0; #endif \t} X = (p[pc-\u003ek] \u0026 0xf) \u003c\u003c 2; continue; case BPF_LD|BPF_IMM: A = pc-\u003ek; continue; case BPF_LDX|BPF_IMM: X = pc-\u003ek; continue; case BPF_LD|BPF_MEM: A = mem[pc-\u003ek]; continue; case BPF_LDX|BPF_MEM: X = mem[pc-\u003ek]; continue; case BPF_ST: mem[pc-\u003ek] = A; continue; case BPF_STX: mem[pc-\u003ek] = X; continue; case BPF_JMP|BPF_JA: pc += pc-\u003ek; continue; case BPF_JMP|BPF_JGT|BPF_K: pc += (A \u003e pc-\u003ek) ? pc-\u003ejt : pc-\u003ejf; continue; case BPF_JMP|BPF_JGE|BPF_K: pc += (A \u003e= pc-\u003ek) ? pc-\u003ejt : pc-\u003ejf; continue; case BPF_JMP|BPF_JEQ|BPF_K: pc += (A == pc-\u003ek) ? pc-\u003ejt : pc-\u003ejf; continue; case BPF_JMP|BPF_JSET|BPF_K: pc += (A \u0026 pc-\u003ek) ? pc-\u003ejt : pc-\u003ejf; continue; case BPF_JMP|BPF_JGT|BPF_X: pc += (A \u003e X) ? pc-\u003ejt : pc-\u003ejf; continue; case BPF_JMP|BPF_JGE|BPF_X: pc += (A \u003e= X) ? pc-\u003ejt : pc-\u003ejf; continue; case BPF_JMP|BPF_JEQ|BPF_X: pc += (A == X) ? pc-\u003ejt : pc-\u003ejf; continue; case BPF_JMP|BPF_JSET|BPF_X: pc += (A \u0026 X) ? pc-\u003ejt : pc-\u003ejf; continue; case BPF_ALU|BPF_ADD|BPF_X: A += X; continue; case BPF_ALU|BPF_SUB|BPF_X: A -= X; continue; case BPF_ALU|BPF_MUL|BPF_X: A *= X; continue; case BPF_ALU|BPF_DIV|BPF_X: if (X == 0) return 0; A /= X; continue; case BPF_ALU|BPF_AND|BPF_X: A \u0026= X; continue; case BPF_ALU|BPF_OR|BPF_X: A |= X; continue; case BPF_ALU|BPF_LSH|BPF_X: A \u003c\u003c= X; continue; case BPF_ALU|BPF_RSH|BPF_X: A \u003e\u003e= X; continue; case BPF_ALU|BPF_ADD|BPF_K: A += pc-\u003ek; continue; case BPF_ALU|BPF_SUB|BPF_K: A -= pc-\u003ek; continue; case BPF_ALU|BPF_MUL|BPF_K: A *= pc-\u003ek; continue; case BPF_ALU|BPF_DIV|BPF_K: A /= pc-\u003ek; continue; case BPF_ALU|BPF_AND|BPF_K: A \u0026= pc-\u003ek; continue; case BPF_ALU|BPF_OR|BPF_K: A |= pc-\u003ek; continue; case BPF_ALU|BPF_LSH|BPF_K: A \u003c\u003c= pc-\u003ek; continue; case BPF_ALU|BPF_RSH|BPF_K: A \u003e\u003e= pc-\u003ek; continue; case BPF_ALU|BPF_NEG: A = -A; continue; case BPF_MISC|BPF_TAX: X = A; continue; case BPF_MISC|BPF_TXA: A = X; continue; } } }   参考阅读 [1]. Mccanne S , Jacobson V . The BSD Packet Filter: A New Architecture for User-level Packet Capture[C]// Proceedings of the USENIX Winter 1993 Conference Proceedings on USENIX Winter 1993 Conference Proceedings. USENIX Association, 1993.\n","description":"","tags":["linux内核","linux工具"],"title":"The BSD Packet Filter论文笔记及源码解析","uri":"/posts/linux-kernel/bpf-paper/"},{"categories":["linux内核"],"content":"wait queue思想比较简单，但是涉及到的多核和锁的问题比较多，这些问题也很复杂，因此本文并不会涉及这些内容。本文主要梳理wait queue的基本思想，以及一些驱动程序如何使用wait queue，包括常见的epoll驱动和uio驱动。\n文章1提出了filtered wakeups方法，该方法需要进程被加入到wait queue时提供一个key，同时wakeup唤醒时也需要传入一个key，通过比较key是否相同来决定唤醒哪一个进程。该方案主要用于解决大量的进程在等待同一事件时，一起唤醒会导致惊群效应（在此之前采用hash的方法，将事件进行hash来决定将进程加入哪个wait queue中，但是实验显示hash方法很容易导致冲突，也会出现惊群效应）。\n文章2提出在rt-kernel中使用simple wait queues，也就是当前的waitqueues机制中删除exclusive wakeup特性和回调函数特性，采用遍历的方法，一一唤醒每一个等待的进程。\n文章3在文章2的基础上谈到了waitqueue中的custom wakeup callbacks是否会被加入到rt-kernel中。其指出custom wakeup callbacks在IO复用中会有很大的好处，特别是poll、epoll和select。但是custom wakeup callbacks会带来的问题就是回调函数的执行时间无法确定，无法满足实时性的要求。而且custom wakeup callbacks需要使用spinlock，spinlock在rt-kernel内可休眠的。所以，在rt-kernel内引入custom wakeup callbacks是比较困难的。\n简介 wait queue采用双向链表的方式管理等待的进程，当事件发生的时候，会调用wake up函数，该函数会从等待队列中唤醒进程。wait queue主要提供了以下几个结构体和方法：\nwait queue主要提供了两个结构体：\n1 2 3 4 5 6 7 8 9 10 11 12 13  // 队列元素 struct wait_queue_entry { unsigned int\tflags; void\t*private;\t//等待进程 \twait_queue_func_t\tfunc;\t//回调函数，用于唤醒进程，一般采用默认的default_wake_function \tstruct list_head\tentry; }; // 队首，一个队首往往代表着一个事件，该队首代表的队列中的队列元素就是等待该事件发生的进程 struct wait_queue_head { spinlock_t\tlock; struct list_head\thead; };   wait queue主要提供了八个基本方法：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  // 创建一个新的wait queue entry，包含声明和初始化 #define DECLARE_WAITQUEUE(name, tsk) // 创建一个新的wait queue，包含声明和初始化 #define DECLARE_WAIT_QUEUE_HEAD(name) // 初始化wait queue #define init_waitqueue_head(wq_head) // 初始化wait queue entry static inline void init_waitqueue_entry(struct wait_queue_entry *wq_entry, struct task_struct *p) // 向wait queue添加wait queue entry void add_wait_queue(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry) // 向wait queue添加wait queue entry，该entry带有WQ_FLAG_EXCLUSIVE特性，防止惊群问题，导致性能下降 void add_wait_queue_exclusive(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry) // 从wait queue移除一个entry，一般是唤醒之后的操作 void remove_wait_queue(struct wait_queue_head *wq_head, struct wait_queue_entry *wq_entry) // 遍历wait queue，唤醒nr_exclusive个进程：回调函数default_wake_function，调用关系比较复杂，主要是将该进程挂载到run_queue以及将其状态置为TASK_RUNNING static int __wake_up_common(struct wait_queue_head *wq_head, unsigned int mode, int nr_exclusive, int wake_flags, void *key, wait_queue_entry_t *bookmark)   历史 wait queue早期的策略比较简单。当事件发生时，调用wake_up函数唤醒所有的在该事件的阻塞进程。但是会有一个严重的问题，就是惊群问题。惊群问题是指多个被唤醒的进程竞争同一个临界资源。wait queue添加exclusive wait特性，使得每次只有一个进程被唤醒（如果不是临界资源可以不使用exclusive wait特性）。随着逐渐发展，wait queue变得越来越复杂，尤其是其回调函数\nWaiting / Blocking in Linux Driver Part – 3 https://sysplay.in/blog/linux-kernel-internals/2015/12/waiting-blocking-in-linux-driver-part-3/\nuio wait queue的使用 uio使用wait queue机制完成向用户驱动通知某个中断号的中断事件，当用户驱动程序读/proc/irq/irq num/irq文件时，如果中断没有发生时会被阻塞。当中断发生时，中断处理函数会唤醒该进程：\nglobal wqh global condition func init register_irq(irq,irq_handler) wqh = init_wait_queue_head condition = false func irq_handler condition = true wake_up(wqh) func read: add_wait_queue(current, wqh) while set_current_state(TASK_INTERRUPTIBLE) if condition is true set_current_state(TASK_RUNNING) break schedule set_current_state(TASK_RUNNING) remove_wait_queue(current) 当调用int __uio_register_device(struct module *owner, struct device *parent, struct uio_info *info)函数注册一个uio驱动时，初始化该wait queue，并注册中断处理函数，下面是代码片段：\n1 2 3 4 5 6 7 8 9 10  int __uio_register_device(struct module *owner, struct device *parent, struct uio_info *info) struct uio_device *idev; idev = kzalloc(sizeof(*idev), GFP_KERNEL); // 初始化一个wait queue head \tinit_waitqueue_head(\u0026idev-\u003ewait); // wait queue head和中断事件绑定 \tret = request_irq(info-\u003eirq, uio_interrupt, info-\u003eirq_flags, info-\u003ename, idev);   用户驱动需要自己开一个单独的线程去接收中断事件，避免整个进程被阻塞。该线程通过读/proc/irq/irq number/irq文件，下面时读时执行的代码。流程如下：\n 首先通过add_wait_queue将自己挂载到wait queue上； set_current_state将自己的状态设置为TASK_INTERRUPTIBLE，避免自己在事件没到来之前就被唤醒，导致无用的上下文切换； 当前仍然处于该进程的上下文，通过event_count是否增加来判断中断是否发生，如果发生了则重新将进程挂载到run queue，否则发起调度，等待通知，即wake_up；  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47  static ssize_t uio_read(struct file *filep, char __user *buf, size_t count, loff_t *ppos) { struct uio_listener *listener = filep-\u003eprivate_data; struct uio_device *idev = listener-\u003edev; DECLARE_WAITQUEUE(wait, current); ssize_t retval = 0; s32 event_count; add_wait_queue(\u0026idev-\u003ewait, \u0026wait); do { // 将当前进程状态设置为TASK_INTERRUPTIBLE，可被信号唤醒 \tset_current_state(TASK_INTERRUPTIBLE); // 读取当前事件次数 \tevent_count = atomic_read(\u0026idev-\u003eevent); if (event_count != listener-\u003eevent_count) { // 说明发生了新的事件，可以通知进程去处理 \t__set_current_state(TASK_RUNNING); if (copy_to_user(buf, \u0026event_count, count)) retval = -EFAULT; else { listener-\u003eevent_count = event_count; retval = count; } break; } // 如果是非阻塞读，则直接返回 \tif (filep-\u003ef_flags \u0026 O_NONBLOCK) { retval = -EAGAIN; break; } // 被信号唤醒 \tif (signal_pending(current)) { retval = -ERESTARTSYS; break; } // 调度，等待irq_handler将自己唤醒 \tschedule(); } while (1); // 发生新的事件，将进程挂载到run queue \t__set_current_state(TASK_RUNNING); // 将该进程从wait queue移除 \tremove_wait_queue(\u0026idev-\u003ewait, \u0026wait); return retval; }   因为是捕捉中断事件，所以需要像该中断号注册中断处理函数，然后在中断处理函数中唤醒wait queue上的等待进程。中断处理函数注册是在__uio_register_device函数完成的，下面我们关注中断发生时中断处理函数的动作：\n1 2 3 4 5 6 7 8  static irqreturn_t uio_interrupt(int irq, void *dev_id) { struct uio_device *idev = (struct uio_device *)dev_id; // 增加中断发生的次数，因为在wake_up后，进程是通过event次数来判断是否发生中断 \tatomic_inc(\u0026idev-\u003eevent); // 唤醒该进程 \twake_up_interruptible(\u0026idev-\u003ewait); }   使用wait queue提供的wait_events 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111  #include \u003clinux/module.h\u003e#include \u003clinux/kernel.h\u003e#include \u003clinux/fs.h\u003e#include \u003clinux/cdev.h\u003e#include \u003clinux/device.h\u003e#include \u003clinux/errno.h\u003e#include \u003casm/uaccess.h\u003e#include \u003clinux/wait.h\u003e#include \u003clinux/sched.h\u003e#include \u003clinux/delay.h\u003e #define FIRST_MINOR 0 #define MINOR_CNT 1  static char flag = 'n'; static dev_t dev; static struct cdev c_dev; static struct class *cl; static DECLARE_WAIT_QUEUE_HEAD(wq); int open(struct inode *inode, struct file *filp) { printk(KERN_INFO \"Inside open\\n\"); return 0; } int release(struct inode *inode, struct file *filp) { printk (KERN_INFO \"Inside close\\n\"); return 0; } ssize_t read(struct file *filp, char *buff, size_t count, loff_t *offp) { printk(KERN_INFO \"Inside read\\n\"); printk(KERN_INFO \"Scheduling Out\\n\"); wait_event_interruptible(wq, flag == 'y'); flag = 'n'; printk(KERN_INFO \"Woken Up\\n\"); return 0; } ssize_t write(struct file *filp, const char *buff, size_t count, loff_t *offp) { printk(KERN_INFO \"Inside write\\n\"); if (copy_from_user(\u0026flag, buff, 1)) { return -EFAULT; } printk(KERN_INFO \"%c\", flag); wake_up_interruptible(\u0026wq); return count; } struct file_operations pra_fops = { read: read, write: write, open: open, release: release }; int wq_init (void) { int ret; struct device *dev_ret; if ((ret = alloc_chrdev_region(\u0026dev, FIRST_MINOR, MINOR_CNT, \"SCD\")) \u003c 0) { return ret; } printk(\"Major Nr: %d\\n\", MAJOR(dev)); cdev_init(\u0026c_dev, \u0026pra_fops); if ((ret = cdev_add(\u0026c_dev, dev, MINOR_CNT)) \u003c 0) { unregister_chrdev_region(dev, MINOR_CNT); return ret; } if (IS_ERR(cl = class_create(THIS_MODULE, \"chardrv\"))) { cdev_del(\u0026c_dev); unregister_chrdev_region(dev, MINOR_CNT); return PTR_ERR(cl); } if (IS_ERR(dev_ret = device_create(cl, NULL, dev, NULL, \"mychar%d\", 0))) { class_destroy(cl); cdev_del(\u0026c_dev); unregister_chrdev_region(dev, MINOR_CNT); return PTR_ERR(dev_ret); } return 0; } void wq_cleanup(void) { printk(KERN_INFO \"Inside cleanup_module\\n\"); device_destroy(cl, dev); class_destroy(cl); cdev_del(\u0026c_dev); unregister_chrdev_region(dev, MINOR_CNT); } module_init(wq_init); module_exit(wq_cleanup); MODULE_LICENSE(\"GPL\"); MODULE_AUTHOR(\"Pradeep\"); MODULE_DESCRIPTION(\"Waiting Process Demo\");   参考文献   Filtered wakeups. https://lwn.net/Articles/83633/. ↩︎\n Simple wait queues. https://lwn.net/Articles/577370/. ↩︎\n The return of simple wait queues. https://lwn.net/Articles/661424/ ↩︎\n   ","description":"","tags":["linux内核"],"title":"Waitqueue简介","uri":"/posts/linux-kernel/waitqueue/"},{"categories":["mit6828 os实验"],"content":"文件系统架构 在init.c通过ENV_CREATE(fs_fs, ENV_TYPE_FS);创建一个专用于文件系统的进程，该进程提供了文件系统的管理，包括读取文件、写入文件等等；\n假设有其他的进程需要进行文件读取，则需要通过ipc（inter process call）的方式将读取文件的元信息传递文件系统进程；\n以打开文件为例，具体代码流程如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  // 库提供的open函数 int open(const char *path, int mode) { int r; struct Fd *fd; // fd_alloc利用之前的自映射快速找到用户进程空间未使用的struct Fd空间。（通过判断虚拟地址对应的表项是否存在） \tif ((r = fd_alloc(\u0026fd)) \u003c 0) return r; // ipc参数 \tstrcpy(fsipcbuf.open.req_path, path); fsipcbuf.open.req_omode = mode; // fsipc的信息传递，具体看下面 \tif ((r = fsipc(FSREQ_OPEN, fd)) \u003c 0) { fd_close(fd, 0); return r; } return fd2num(fd); }   1 2 3 4 5 6 7 8 9 10 11 12 13  // file system对jos提供的ipc机制的封装，主要是传递的参数格式化 // type是操作类型，比如FSREQ_OPEN // dstva是struct Fd的虚拟地址 static int fsipc(unsigned type, void *dstva) { static envid_t fsenv; if (fsenv == 0) // 利用子映射快速遍历所有的env，找到文件系统对应的env（ipc通信需要的参数） \tfsenv = ipc_find_env(ENV_TYPE_FS); // ipc \tipc_send(fsenv, type, \u0026fsipcbuf, PTE_P | PTE_W | PTE_U); return ipc_recv(NULL, dstva, NULL); }   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  Regular env FS env +---------------+ +---------------+ | read | | file_read | | (lib/fd.c) | | (fs/fs.c) | ...|.......|.......|...|.......^.......|............... | v | | | | RPC mechanism | devfile_read | | serve_read | | (lib/file.c) | | (fs/serv.c) | | | | | ^ | | v | | | | | fsipc | | serve | | (lib/file.c) | | (fs/serv.c) | | | | | ^ | | v | | | | | ipc_send | | ipc_recv | | | | | ^ | +-------|-------+ +-------|-------+ | | +-------------------+   spawn spawn是创建进程的另外一种方式，它不同于fork。fork的话是创建和自身一样的运行环境，程序代码也是当前运行的程序代码。而spawn则是从文件系统加载一段新的程序代码去运行。spwan的具体流程如下：\n 加载程序代码到内存中（从文件系统读取程序代码）； 创建子进程；  shell 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50  struct Pipe { off_t p_rpos;\t// read position \toff_t p_wpos;\t// write position \tuint8_t p_buf[PIPEBUFSIZ];\t// data buffer }; struct Dev devpipe = { .dev_id =\t'p', .dev_name =\t\"pipe\", .dev_read =\tdevpipe_read, .dev_write =\tdevpipe_write, .dev_close =\tdevpipe_close, .dev_stat =\tdevpipe_stat, }; // 分开的命令分别进行spawn出新的进程去处理 \tcase '|':\t// 分配出两个fd,绑定的文件是devpipe \tif ((r = pipe(p)) \u003c 0) { cprintf(\"pipe: %e\", r); exit(); } // 创建子进程 \tif ((r = fork()) \u003c 0) { cprintf(\"fork: %e\", r); exit(); } if (r == 0) { // 子进程 \tif (p[0] != 0) { // 0是stdin，将pipe的第一个fd变成子进程的stdin \tdup(p[0], 0); // 释放pipe的fd \tclose(p[0]); } // 释放pipe的fd \tclose(p[1]); goto again; } else { // 当前进程 \tpipe_child = r; if (p[1] != 1) { // 将pipe的第二个fd变成当前进程的stdout \tdup(p[1], 1); close(p[1]); } close(p[0]); goto runit; }    Exercise 1. i386_init identifies the file system environment by passing the type ENV_TYPE_FS to your environment creation function, env_create. Modify env_create in env.c, so that it gives the file system environment I/O privilege, but never gives that privilege to any other environment.\n 标记不同env的类型，文件系统的env类型是ENV_TYPE_FS。修改flags让用户模式的程序可以使用io特权指令。\n1 2 3 4 5 6 7 8 9 10 11 12 13  void env_create(uint8_t *binary, enum EnvType type) { // lab3 \tstruct Env *env; env_alloc(\u0026env,0); env-\u003eenv_type = type; load_icode(env,binary); // lab5 \tif(type == ENV_TYPE_FS){ env-\u003eenv_type = ENV_TYPE_FS; env-\u003eenv_tf.tf_eflags |= FL_IOPL_MASK; } }    Exercise 2. Implement the bc_pgfault and flush_block functions in fs/bc.c. bc_pgfault is a page fault handler, just like the one your wrote in the previous lab for copy-on-write fork, except that its job is to load pages in from the disk in response to a page fault. When writing this, keep in mind that (1) addr may not be aligned to a block boundary and (2) ide_read operates in sectors, not blocks.\n 硬盘的数据在内存中叫block cache，jos分配了固定的虚拟地址空间存放硬盘数据，当我们需要读写的时候，如果该数据在内存中不存在，则会触发异常，该异常由内核传递给用户程序处理，即bc_pgfault函数。\n1 2 3 4 5 6 7 8 9  // static void bc_pgfault(struct UTrapframe *utf) \t// 地址对齐 \taddr = ROUNDDOWN(addr,BLKSIZE); // 分配一个page的物理内存 \tif((r = sys_page_alloc(0,(void *)addr,PTE_SYSCALL))\u003c0) panic(\"in bc_pgfault, sys_page_alloc: %e\",r); // 读取磁盘数据到内存中 \tif((r = ide_read(((uintptr_t)addr-DISKMAP)/BLKSIZE*BLKSECTS,addr,BLKSECTS))\u003c0) panic(\"in bc_pgfault, ide_read: %e\",r);   flush_block主要是维持内存和硬盘中的数据同步。\n1 2 3 4 5 6 7 8 9 10 11  // void flush_block(void *addr)\t\tint r; addr = ROUNDDOWN(addr,BLKSIZE); // 该地址对应的block存在内存中，并且有修改过（没有修改的话就不用flush）。 \tif(va_is_mapped(addr) \u0026\u0026 va_is_dirty(addr)){ if((r = ide_write(blockno*BLKSECTS,addr,BLKSECTS))\u003c0) panic(\"in flush_block, ide_write %e\",r); // 清除dirty位 \tif ((r = sys_page_map(0, addr, 0, addr, uvpt[PGNUM(addr)] \u0026 PTE_SYSCALL)) \u003c 0) panic(\"in flush_block, sys_page_map: %e\", r); }    Exercise 3. Use free_block as a model to implement alloc_block in fs/fs.c, which should find a free disk block in the bitmap, mark it used, and return the number of that block. When you allocate a block, you should immediately flush the changed bitmap block to disk with flush_block, to help file system consistency.\n 从硬盘空闲块中分配一块，返回块号。\n1 2 3 4 5 6 7 8 9 10  //int alloc_block(void) \tuint32_t i; for(i=0;i\u003csuper-\u003es_nblocks;i++){ if(block_is_free(i)){ bitmap[i/32] \u0026= ~(1\u003c\u003c(i%32)); // 将bitmap数据flush到硬盘，防止断电等意外发生，造成数据的丢失 \tflush_block((void *)((2+i/BLKBITSIZE)*BLKSIZE+DISKMAP)); return i; } }    Exercise 4. Implement file_block_walk and file_get_block. file_block_walk maps from a block offset within a file to the pointer for that block in the struct File or the indirect block, very much like what pgdir_walk did for page tables. file_get_block goes one step further and maps to the actual disk block, allocating a new one if necessary.\n jos文件系统采用类似于inode的方式，每个文件数据对应的物理存储块编号采用直接方式和间接方式。直接索引就是将该存储块编号存放在inode的数组中，间接索引是将该存储块编号存放在block中。file_block_walk函数将文件的逻辑地址转换成对应的硬盘上的物理地址。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  //static int file_block_walk(struct File *f, uint32_t filebno, uint32_t **ppdiskbno, bool alloc) \tuintptr_t *addr; // 逻辑块号小于NDIRECT \tif(filebno \u003c NDIRECT){ *ppdiskbno = \u0026f-\u003ef_direct[filebno]; return 0; }else{ // 超过文件最大限制 \tif(filebno \u003e= NDIRECT + NINDIRECT) return -E_INVAL; // 间接存储的block还未分配 \tif(f-\u003ef_indirect== 0 || block_is_free(f-\u003ef_indirect)){ if(alloc){ // 分配一个block用于存储物理存储块编号 \tf-\u003ef_indirect = alloc_block(); if(f-\u003ef_indirect \u003c 0){ f-\u003ef_indirect = 0; return -E_NO_DISK; } }else{ return -E_NOT_FOUND; } } } // 获取间接索引的物理存储块（一般会触发上面的bc_pgfault） \taddr = (uintptr_t *)(f-\u003ef_indirect*BLKSIZE+DISKMAP); // 物理块号 \t*ppdiskbno = \u0026addr[filebno-NDIRECT]; return 0;   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  // int file_get_block(struct File *f, uint32_t filebno, char **blk) \t// LAB 5: Your code here. \tint r; uint32_t *ppdiskbno; // 找到逻辑块号对应的物理块号 \tif((r = file_block_walk(f,filebno,\u0026ppdiskbno,0))\u003c0){ return r; } // 物理块不存在，需要分配一个物理块给当前文件 \tif(*ppdiskbno == 0){ *ppdiskbno = alloc_block(); if(*ppdiskbno \u003c 0){ return -E_NO_DISK; } } // 返回该物理块映射的虚拟地址 \t*blk = (char *)diskaddr(*ppdiskbno); return 0;    Exercise 5. Implement serve_read in fs/serv.c.\nserve_read's heavy lifting will be done by the already-implemented file_read in fs/fs.c (which, in turn, is just a bunch of calls to file_get_block). serve_read just has to provide the RPC interface for file reading. Look at the comments and code in serve_set_size to get a general idea of how the server functions should be structured.\n 前面几个函数是涉及到文件系统到硬盘的管理，下面的则是完成用户的请求。serve_read函数接收用户进程的Fsipc\n1 2 3 4 5 6 7 8 9 10 11 12  //int serve_read(envid_t envid, union Fsipc *ipc) \tint r; struct OpenFile *o; // 检查该进程的打开文件列表是否存在该文件 \tif((r=openfile_lookup(envid,req-\u003ereq_fileid,\u0026o))\u003c0){ return r; } // 读取文件数据（通过bc_pgfault的方式实现） \tr = file_read(o-\u003eo_file,ret-\u003eret_buf,req-\u003ereq_n,o-\u003eo_fd-\u003efd_offset); // 修改seek位置 \tif(r\u003e0) o-\u003eo_fd-\u003efd_offset += r; return r;    Exercise 6. Implement serve_write in fs/serv.c and devfile_write in lib/file.c.\n 1 2 3 4 5 6 7 8 9 10  // int serve_write(envid_t envid, struct Fsreq_write *req) \tint r; struct OpenFile *o; // 检查该进程的打开文件列表是否存在该文件 \tif((r=openfile_lookup(envid,req-\u003ereq_fileid,\u0026o))\u003c0){ return r; } r = file_write(o-\u003eo_file,req-\u003ereq_buf,req-\u003ereq_n \u003e PGSIZE? PGSIZE:req-\u003ereq_n,o-\u003eo_fd-\u003efd_offset); if(r \u003e 0) o-\u003eo_fd-\u003efd_offset += r; return r;   1 2 3 4 5 6 7 8  // static ssize_t devfile_read(struct Fd *fd, void *buf, size_t n)\t\tint r; if ( n \u003e sizeof (fsipcbuf.write.req_buf)) n = sizeof (fsipcbuf.write.req_buf); memmove(fsipcbuf.write.req_buf, buf, n); fsipcbuf.write.req_fileid = fd-\u003efd_file.id; fsipcbuf.write.req_n = n; return fsipc(FSREQ_WRITE, NULL);    Exercise 7. spawn relies on the new syscall sys_env_set_trapframe to initialize the state of the newly created environment. Implement sys_env_set_trapframe in kern/syscall.c (don't forget to dispatch the new system call in syscall()).\n 1 2 3 4 5 6 7 8 9 10  // static int sys_env_set_trapframe(envid_t envid, struct Trapframe *tf)\t\tstruct Env *e; if(envid2env(envid,\u0026e,1) \u003c 0){ return -E_BAD_ENV; } user_mem_assert(e, tf, sizeof(struct Trapframe), PTE_U); e-\u003eenv_tf = *tf; e-\u003eenv_tf.tf_eflags |= FL_IF; e-\u003eenv_tf.tf_eflags \u0026= (~FL_IOPL_MASK); return 0;    Exercise 8. Change duppage in lib/fork.c to follow the new convention. If the page table entry has the PTE_SHARE bit set, just copy the mapping directly. (You should use PTE_SYSCALL, not 0xfff, to mask out the relevant bits from the page table entry. 0xfff picks up the accessed and dirty bits as well.)\nLikewise, implement copy_shared_pages in lib/spawn.c. It should loop through all page table entries in the current process (just like fork did), copying any page mappings that have the PTE_SHARE bit set into the child process.\n spawn的话可以从用户态加载应用程序去运行。\n Exercise 9. In your kern/trap.c, call kbd_intr to handle trap IRQ_OFFSET+IRQ_KBD and serial_intr to handle trap IRQ_OFFSET+IRQ_SERIAL.\n kbd_intr函数读取键盘的输入；serial_intr函数读取输入的数据到buffer里面。\n1 2 3 4 5  case IRQ_OFFSET+IRQ_KBD:{ kbd_intr(); lapic_eoi(); return; }    Exercise 10. The shell doesn't support I/O redirection. It would be nice to run sh \u003cscript instead of having to type in all the commands in the script by hand, as you did above. Add I/O redirection for \u003c to user/sh.c.\n io重定向：以重定向输入为例（比如sh\u003cscript）。因为fd 0代表着stdin，fd 1代表着stdout，所以将待重定向的文件的fd拷贝到子进程的fd 0即可（执行sh的shell可以通过getchar的方式读取该文件内容【根据fd绑定的内容找到对应的devfile，那么getchar的读取转换成devfile-\u003eread】）。\n1 2 3 4 5 6 7 8 9  if ((fd = open(t, O_RDONLY)) \u003c 0) { cprintf(\"open %s for read: %e\", t, fd); exit(); } if(fd != 0){ dup(fd,0); close(fd); } break;   ","description":"","tags":["mit6828 os实验"],"title":"Lab5 - 实验笔记","uri":"/posts/course/mit6828/lab5/"},{"categories":["mit6828 os实验"],"content":"lab4 实验代码：不过最后primes案例没过\nlab4实验代码patch1：通过primes案例\nlab4实验代码patch2：小bug\nx86多核系统 JOS支持的是对称多处理器架构，最先启动的核称为bootstrap处理器（BSP），后续启动的核称为application处理器（APs）。下面介绍一下整个的启动流程：\n  mp_init函数：从BIOS中找到多处理器信息配置表；\n  boot_aps函数：\na. 将APs的启动代码移动到0x7000的位置；\nb. 发送核间中断，参数包括：cpu编号和cpu起始运行地址；\nc. 等待该cpu启动完毕；\n  APs执行的代码和BSP差不太多，主要涉及的文件有：kern/mpentry.S和kern/init.c。\n  per-cpu per-cpu表示的是为每个cpu维护自己的数据结构，其次访问cpu共享的数据结构时需要加锁。\ncopy on write 利用系统调用实现在用户态创建一个新的进程，也就是fork的方式，jos有两种实现方式，一种是dump_fork实现的，即在fork时为新进程分配内存；另外一种是基于copy-on-write的方式，即将父进程和子进程的页表项置为COW，在修改该页表项对应的物理页时会触发page fault异常，有该异常处理函数分配具体内存；\n Exercise 1. Implement mmio_map_region in kern/pmap.c. To see how this is used, look at the beginning of lapic_init in kern/lapic.c.\n 1 2 3 4 5 6 7 8 9 10 11  void *mmio_map_region(physaddr_t pa, size_t size) { static uintptr_t base = MMIOBASE; // 大小取整 \tsize = ROUNDUP(size,PGSIZE); if(size \u003e MMIOLIM) panic(\"error\"); // 做映射 \tboot_map_region(kern_pgdir,base,size,pa,PTE_PCD|PTE_PWT|PTE_W|PTE_P); base += size; return (void *)(base-size); }    Exercise 2. Read boot_aps() and mp_main() in kern/init.c, and the assembly code in kern/mpentry.S. Make sure you understand the control flow transfer during the bootstrap of APs. Then modify your implementation of page_init() in kern/pmap.c to avoid adding the page at MPENTRY_PADDR to the free list, so that we can safely copy and run AP bootstrap code at that physical address.\n 1 2 3 4 5 6 7 8 9 10 11 12  extern char mpentry_start[]; extern char mpentry_end[]; s = MPENTRY_PADDR; e = mpentry_end-mpentry_start+s; s = ROUNDDOWN(s,PGSIZE); e = ROUNDUP(e,PGSIZE); s = PGNUM(s); e = PGNUM(e); for(i=s;i\u003ce;i++){ pages[i].pp_ref = 1; pages[i].pp_link = NULL; }    Exercise 3. Modify mem_init_mp() (in kern/pmap.c) to map per-CPU stacks starting at KSTACKTOP, as shown in inc/memlayout.h. The size of each stack is KSTKSIZE bytes plus KSTKGAP bytes of unmapped guard pages.\n 1 2 3 4 5 6 7 8 9 10  // 将每个cpu的内核栈映射到指定的虚拟地址上 static void mem_init_mp(void) { int i; uintptr_t s = KSTACKTOP; for(i=0;i\u003cNCPU;i++){ boot_map_region(kern_pgdir,s-KSTKSIZE,KSTKSIZE,PADDR(percpu_kstacks[i]),PTE_W|PTE_P); s -= KSTKSIZE+KSTKGAP; } }    Exercise 4. The code in trap_init_percpu() (kern/trap.c) initializes the TSS and TSS descriptor for the BSP.\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  void trap_init_percpu(void) { // Setup a TSS so that we get the right stack \t// when we trap to the kernel. \tint i; thiscpu-\u003ecpu_ts.ts_esp0 = KSTACKTOP; for(i=0;i\u003cthiscpu-\u003ecpu_id;i++){ thiscpu-\u003ecpu_ts.ts_esp0 -= (KSTKSIZE+KSTKGAP); } thiscpu-\u003ecpu_ts.ts_ss0 = GD_KD; thiscpu-\u003ecpu_ts.ts_iomb = sizeof(struct Taskstate); // Initialize the TSS slot of the gdt. \tgdt[(GD_TSS0 \u003e\u003e 3) + thiscpu-\u003ecpu_id] = SEG16(STS_T32A, (uint32_t) (\u0026thiscpu-\u003ecpu_ts), sizeof(struct Taskstate) - 1, 0); gdt[(GD_TSS0 \u003e\u003e 3) + thiscpu-\u003ecpu_id].sd_s = 0; // Load the TSS selector (like other segment selectors, the \t// bottom three bits are special; we leave them 0) \tltr(GD_TSS0+(i\u003c\u003c3)); // Load the IDT \tlidt(\u0026idt_pd); }    Exercise 5. Apply the big kernel lock as described above, by calling lock_kernel() and unlock_kernel() at the proper locations.\n 实现大内核锁。\n Exercise 6. Implement round-robin scheduling in sched_yield() as described above.\n 实现轮转法调度。\n Exercise 7. Implement the system calls described above in kern/syscall.c and make sure syscall() calls them. You will need to use various functions in kern/pmap.c and kern/env.c, particularly envid2env().\n 实现sys_exofork、sys_env_set_status、sys_page_alloc、sys_page_map和sys_page_unmap函数。\nCopy-on-Write Fork 下面是用户空间出现页异常处理流程：\n 通过系统调用注册进程自己的处理函数（在fork的时候配置的）； 当页异常触发时，首先由内核异常处理函数处理，判断该错误地址是否发生在用户地址空间。如果是的话则将进程的esp指向进程异常栈；将错误信息保存在进程异常栈（在fork的时候分配的内存）里；将返回值指向进程异常处理函数； 返回用户空间运行。  下面是fork的流程：\n 通过系统调用注册进程自己的处理函数； 通过系统调用创建子进程； 利用自映射机制可以快速遍历页表项，查看每个页表项的属性（低12位），如果该页存在的话，则将其映射到子进程的地址空间，并将其页表项型属性置为COW；然后修改自身的页表项的属性；（因为子进程和父进程都不能直接修改该页表项对应的物理页（原物理页），只能新建物理页并元物理页的内容拷贝进去。） 为子进程分配进程异常栈； 为子进程注册进程异常处理函数；（因为子进程运行的时候大部分的页属性是COW，也就是不可能自己通过系统调用注册进程异常处理函数）； 修改子进程运行状态-》可运行状态。  Preemptive Multitasking and Inter-Process communication (IPC) 实现时钟中断，利用时钟中断完成任务的抢占；\n实现ipc，其ipc有如下机制：\n ipc是基于系统调用实现的，可以传递一个整数或者传递一个虚拟地址（对应着一个物理页）； ipc的发送方处于polling状态； ipc的接收方处于进程不可运行态，由ipc的发送方不断的尝试发送数据，发送成功后由发送方修改接收方的进程运行状态；  ","description":"","tags":["mit6828 os实验"],"title":"Lab4 - 实验笔记","uri":"/posts/course/mit6828/lab4/"},{"categories":["mit6828 os实验"],"content":"lab3的实验主要涉及到x86的硬件机制：中断、异常等等。因此，需要自己实现中断向量表、保存和恢复上下文、系统调用和页异常相关的函数。\nlab3也新增了用户进程的概念，需要实现相关函数。\nuser environments jos的user environments是tcb的概念，用于描述一个进程实体。其结构如下所示：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  struct Env { struct Trapframe env_tf;\t// Saved registers \tstruct Env *env_link;\t// Next free Env \tenvid_t env_id;\t// Unique environment identifier \tenvid_t env_parent_id;\t// env_id of this env's parent \tenum EnvType env_type;\t// Indicates special system environments \tunsigned env_status;\t// Status of the environment \tuint32_t env_runs;\t// Number of times environment has run \tint env_cpunum;\t// The CPU that the env is running on  // Address space \tpde_t *env_pgdir;\t// Kernel virtual address of page dir  // Exception handling \tvoid *env_pgfault_upcall;\t// Page fault upcall entry point  // Lab 4 IPC \tbool env_ipc_recving;\t// Env is blocked receiving \tvoid *env_ipc_dstva;\t// VA at which to map received page \tuint32_t env_ipc_value;\t// Data value sent to us \tenvid_t env_ipc_from;\t// envid of the sender \tint env_ipc_perm;\t// Perm of page mapping received };   中断和异常 中断描述符表：x86最多有256个表项；\nTSS：包含了段选择子和栈的基址；\n0-31号中断：同步异常；\n31号以上中断：软中断和硬中断；\n嵌套中断和异常：\n Exercise 1. Modify mem_init() in kern/pmap.c to allocate and map the envs array. This array consists of exactly NENV instances of the Env structure allocated much like how you allocated the pages array. Also like the pages array, the memory backing envs should also be mapped user read-only at UENVS (defined in inc/memlayout.h) so user processes can read from this array.\n 为NENV个struct Env分配内存空间，然后将其映射到位于用户地址空间的虚拟地址UENVS。\n1 2 3 4 5  //分配内存 envs = (struct Env*)boot_alloc(sizeof(struct Env)*NENV); memset(envs,0,sizeof(struct Env)*NENV); //做映射 boot_map_region(kern_pgdir,UENVS,ROUNDUP(sizeof(struct Env)*NENV,PGSIZE),PADDR(envs),PTE_P|PTE_U);    Exercise 2. In the file env.c, finish coding the following functions:\n env_init()  Initialize all of the Env structures in the envs array and add them to the env_free_list. Also calls env_init_percpu, which configures the segmentation hardware with separate segments for privilege level 0 (kernel) and privilege level 3 (user).\n env_setup_vm()  Allocate a page directory for a new environment and initialize the kernel portion of the new environment's address space.\n region_alloc()  Allocates and maps physical memory for an environment\n load_icode()  You will need to parse an ELF binary image, much like the boot loader already does, and load its contents into the user address space of a new environment.\n env_create()  Allocate an environment with env_alloc and call load_icode to load an ELF binary into it.\n env_run()  Start a given environment running in user mode.\n 函数较多，实现简单，这里不再贴代码。\n  env_init函数的实现类似于page_init函数，主要将空闲的struct Env插入到env_free_list链表上；\n  env_setup_vm函数为该进程分配一个页目录表，并将内核页目录表内容拷贝到该进程的页目录表；\n  region_alloc函数为进程分配内存，并将其映射到其地址空间，其参数是va和len；\n  load_icode函数解析elf文件头部信息，将其加载到指定的地址上；修改进程的eip值；为该进程创建用户栈；\n  env_alloc函数工作流程：调用env_setup_vm完成进程的页目录表的分配；分配一个id；设置该进程的寄存器的值，包括：ds、es、ss、esp、cs等；\n  env_create函数先后调用了env_alloc函数和load_icode函数；\n  env_run函数根据进程是否发生切换来决定是否更新cr3寄存器的值；\n   Exercise 4. Edit trapentry.S and trap.c and implement the features described above. The macros TRAPHANDLER and TRAPHANDLER_NOEC in trapentry.S should help you, as well as the T_* defines in inc/trap.h. You will need to add an entry point in trapentry.S (using those macros) for each trap defined in inc/trap.h, and you'll have to provide _alltraps which the TRAPHANDLER macros refer to. You will also need to modify trap_init() to initialize the idt to point to each of these entry points defined in trapentry.S; the SETGATE` macro will be helpful here.\n 具体代码没有粘贴，记录一下具体的中断流程：\n 中断或者异常发生，cpu判断当前特权级来决定是否切换到内核栈； 如下图所示，如果发生特权级切换需要特别保存ss和esp寄存器（中断嵌套的话只能发生在内核态）； 保存剩余的寄存器到内核栈中（具体可以查看int指令执行的具体内容）； 根据中段号跳转到IDT的相应entry； 压入中断号； 保存用户上下文； 中断处理函数； 恢复用户上下文； 执行iret指令，从内核栈中pop出ss和esp寄存器；  页异常 页异常的中断向量号是14，当页异常发生时，处理器会自动保存其虚拟地址到cr2寄存器；\n Exercise 5. Modify trap_dispatch() to dispatch page fault exceptions to page_fault_handler().\n 1 2 3 4  case T_PGFLT:{ page_fault_handler(tf); break; }   断点异常 断点异常的中断向量号是3，调试程序时需要在相应的指令下插入该条指令（int 0x3），触发软件异常，可以调试程序。\n Exercise 6. Modify trap_dispatch() to make breakpoint exceptions invoke the kernel monitor.\n 1 2 3 4  case T_BRKPT:{ monitor(tf); break; }   系统调用  Exercise 7. Add a handler in the kernel for interrupt vector T_SYSCALL. You will have to edit kern/trapentry.S and kern/trap.c's trap_init(). You also need to change trap_dispatch() to handle the system call interrupt by calling syscall() (defined in kern/syscall.c) with the appropriate arguments, and then arranging for the return value to be passed back to the user process in %eax. Finally, you need to implement syscall() in kern/syscall.c. Make sure syscall() returns -E_INVAL if the system call number is invalid. You should read and understand lib/syscall.c (especially the inline assembly routine) in order to confirm your understanding of the system call interface. Handle all the system calls listed in inc/syscall.h by invoking the corresponding kernel function for each call.\n 实现系统调用部分，为用户程序提供内核服务，具体看代码:-）。\n用户程序入口 主要关注几个文件的代码，分别是lib/entry.S、lib/libmain.c和lib/exit.c。\n Exercise 8. Add the required code to the user library, then boot your kernel. You should see user/hello print \"hello, world\" and then print \"i am environment 00001000\". user/hello then attempts to \"exit\" by calling sys_env_destroy() (see lib/libmain.c and lib/exit.c).\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  void libmain(int argc, char **argv) { // set thisenv to point at our Env structure in envs[]. \t// LAB 3: Your code here. \tenvid_t id = sys_getenvid(); thisenv = \u0026envs[ENVX(id)]; // save the name of the program so that panic() can use it \tif (argc \u003e 0) binaryname = argv[0]; // call user main routine \tumain(argc, argv); // exit gracefully \texit(); }   内存保护机制  用户进程传进来的指针一定要检查，首先检查该指针不能在内核地址空间，然后判断是否存在该虚拟地址，最后检查读写权限； 内核不能够发生页错误；   Exercise 9. Change kern/trap.c to panic if a page fault happens in kernel mode.\nRead user_mem_assert in kern/pmap.c and implement user_mem_check in that same file.\nChange kern/syscall.c to sanity check arguments to system calls.\n 内核不能够发生页错误。\n1 2 3  if(tf-\u003etf_cs == GD_KT){ panic(\"kernel page fault\\n\"); }   用户进程传进来的指针检查函数：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  int user_mem_check(struct Env *env, const void *va, size_t len, int perm) { // LAB 3: Your code here. \tuintptr_t s = ROUNDDOWN((uintptr_t)va,PGSIZE); uintptr_t e = ROUNDUP((uintptr_t)va+len,PGSIZE); pte_t *pte_entry; for(;s\u003ce;s+=PGSIZE){ // cprintf(\"va is %x ULIM is %x\\n\",s,ULIM); \tpage_lookup(env-\u003eenv_pgdir,(void *)s,\u0026pte_entry); // 1. 首先检查该指针不能在内核地址空间  // 2. 然后判断是否存在该虚拟地址  // 3. 最后检查读写权限 \tif(s\u003eULIM || pte_entry == NULL || (PGOFF(*pte_entry)\u0026perm) != perm){ user_mem_check_addr = MAX(s,(uintptr_t)va); return -E_FAULT; } } return 0; }   ","description":"","tags":["mit6828 os实验"],"title":"Lab3 - 实验笔记","uri":"/posts/course/mit6828/lab3/"},{"categories":["mit6828 os实验"],"content":"lab2实验代码\nlab2的实验主要涉及到物理内存和虚拟内存的管理，其中JOS内核按页对物理内存进行管理，所以其分配、释放的最小内存大小是一个物理页。相应的需要实现三个物理页相关函数：\n void page_init(void) struct PageInfo *page_alloc(int alloc_flags) void page_free(struct PageInfo *pp)  JOS内核的虚拟地址空间大小是256MB，x86中包含分段机制和分页机制，JOS分段机制比较固定，占用整个地址空间大小。因此，整个实验比较关注于基于mmu的分页管理机制。针对分页机制需要实现四个函数：\n pte_t *pgdir_walk(pde_t *pgdir, const void *va, int create) void page_remove(pde_t *pgdir, void *va) struct PageInfo *page_lookup(pde_t *pgdir, void *va, pte_t **pte_store) int page_insert(pde_t *pgdir, struct PageInfo *pp, void *va, int perm)  本文不会涉及到具体代码，简单记录了比较关键的部分。\n物理页管理 JOS的物理内存粒度是页，所有的空闲页通过链表进行管理，下面是内核物理内存管理的流程，特别需要注意到在没有初始化内存之前，内核如何为数据结构分配内存。\n  通过I/O端口获取物理内存的分布情况，分为两部分，base memory占用640KB，extended memory占用130432KB。可以知道base memory指的是lab1讲的low memory；\n Physical memory: 131072K available, base = 640K, extended = 130432K\n   实现boot_alloc函数，该函数主要完成在没有建立页管理机制之前的内存分配工作。其工作流程是：\na. 根据链接脚本的end参数获取内核代码所占内存的终止地址，并将其按页对齐；\nb. 为调用者分配内存，更新终止位置；\n  调用boot_alloc函数为内核页目录表kern_pgdir分配内存，大小是PGSIZE，二级页表在做映射的时候才进行分配；\n  建立自映射，这个在lab4还会遇到；\n  调用boot_alloc函数为管理页数据结构struct PageInfo分配内存，其内存大小是sizeof(struct PageInfo)*npages；\n  page_init函数初始化struct PageInfo，主要是将空闲页挂在空闲页链表page_free_list上，此时空闲页主要有两部分构成：1. low memory部分； 2. extended memory剩余部分；\n  page_init函数从空闲页链表上取出一页；\n  page_free函数将该页挂在空闲链表上；\n  虚拟内存 在x86架构中，有三种地址术语，分别是：虚拟地址、线性地址和物理地址。虚拟地址通过分段机制转换得到线性地址，线性地址通过分页机制转换可以得到物理地址。\n1 2 3 4 5 6 7 8 9  Selector +--------------+ +-----------+ ----------\u003e| | | | | Segmentation | | Paging | Software | |--------\u003e| |----------\u003e RAM Offset | Mechanism | | Mechanism | ----------\u003e| | | | +--------------+ +-----------+ Virtual Linear Physical   关于分段机制，现在操作系统用的很少，大部分将段基地址设置成0，段的大小是0xffffffff，所以虚拟地址等于线性地址。\n 分段机制目前用的比较多的是它的特权等级，jos中分为内核段和用户段；\n 在lab1中只映射了4MB的物理内存，在qemu中运行info pg可以得到：\n1 2 3 4 5 6 7 8 9  VPN range Entry Flags Physical page [00000-003ff] PDE[000] ----A----P [00000-000b7] PTE[000-0b7] --------WP 00000-000b7 ... [00114-003ff] PTE[114-3ff] --------WP 00114-003ff [f0000-f03ff] PDE[3c0] ----A---WP [f0000-f00b7] PTE[000-0b7] --------WP 00000-000b7 ... [f0114-f03ff] PTE[114-3ff] --------WP 00114-003ff   从上面可以看到，物理内存[0,4MB)分别映射到虚拟内存[0,4MB)和[0xf0000000,0xf03ff000)。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58  /* * Virtual memory map: Permissions * kernel/user * * 4 Gig --------\u003e +------------------------------+ * | | RW/-- * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ * : . : * : . : * : . : * |~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~| RW/-- * | | RW/-- * | Remapped Physical Memory | RW/-- * | | RW/-- * KERNBASE, ----\u003e +------------------------------+ 0xf0000000 --+ * KSTACKTOP | CPU0's Kernel Stack | RW/-- KSTKSIZE | * | - - - - - - - - - - - - - - -| | * | Invalid Memory (*) | --/-- KSTKGAP | * +------------------------------+ | * | CPU1's Kernel Stack | RW/-- KSTKSIZE | * | - - - - - - - - - - - - - - -| PTSIZE * | Invalid Memory (*) | --/-- KSTKGAP | * +------------------------------+ | * : . : | * : . : | * MMIOLIM ------\u003e +------------------------------+ 0xefc00000 --+ * | Memory-mapped I/O | RW/-- PTSIZE * ULIM, MMIOBASE --\u003e +------------------------------+ 0xef800000 * | Cur. Page Table (User R-) | R-/R- PTSIZE * UVPT ----\u003e +------------------------------+ 0xef400000 * | RO PAGES | R-/R- PTSIZE * UPAGES ----\u003e +------------------------------+ 0xef000000 * | RO ENVS | R-/R- PTSIZE * UTOP,UENVS ------\u003e +------------------------------+ 0xeec00000 * UXSTACKTOP -/ | User Exception Stack | RW/RW PGSIZE * +------------------------------+ 0xeebff000 * | Empty Memory (*) | --/-- PGSIZE * USTACKTOP ---\u003e +------------------------------+ 0xeebfe000 * | Normal User Stack | RW/RW PGSIZE * +------------------------------+ 0xeebfd000 * | | * | | * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ * . . * . . * . . * |~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~| * | Program Data \u0026 Heap | * UTEXT --------\u003e +------------------------------+ 0x00800000 * PFTEMP -------\u003e | Empty Memory (*) | PTSIZE * | | * UTEMP --------\u003e +------------------------------+ 0x00400000 --+ * | Empty Memory (*) | | * | - - - - - - - - - - - - - - -| | * | User STAB Data (optional) | PTSIZE * USTABDATA ----\u003e +------------------------------+ 0x00200000 | * | Empty Memory (*) | | * 0 ------------\u003e +------------------------------+ --+   上面是虚拟内存地址空间分布的情况，下面是建立映射的大致过程：\n 前面已经为页目录表kern_pgdir分配了内存，所以接下来的建立地址映射的过程就是在kern_pgdir插入页表项，以及分配页表； 将前面分配的sizeof(struct PageInfo) * npage映射到UPAGES； 将内核栈映射到KSTACKTOP； 将整个物理内存映射内核虚拟地址空间； 将kern_pgdir加载到cr3寄存器；  下面是几个函数的实现功能，描述的比较简单，但实际上有很多的细节需要去考虑：\n  pgdir_walk函数查找虚拟地址所对应的页表项，返回该页表项的指针，也就是\u0026kern_pgdir[PDX(va)][PTX(va)]；\n  page_lookup函数查找虚拟地址对应的物理页，也就是pa2page(kern_pgdir[PDX(va)][PTX(va)])；\n  page_remove函数完成两个功能：\na. 取消虚拟地址映射；\nb. 回收物理页。\nc. 取消映射原理是kern_pgdir[PDX(va)][PTX(va)] = 0，回收物理页原理将该页插入到空闲页链表即可。\n  page_insert函数对该页建立映射，也就是kern_pgdir[PDX(va)][PTX(va)]=PGNUM(va)；\n  ","description":"","tags":["mit6828 os实验"],"title":"Lab2 - 实验笔记","uri":"/posts/course/mit6828/lab2/"},{"categories":["mit6828 os实验"],"content":"lab1的实验代码\nlab1的实验主要讲述了x86的物理地址空间的分布，特别需要注意的是BIOS ROM，pc启动后运行的第一条指令就在BIOS ROM中。接着便是BIOS如何从硬盘中加载bootloader，最后是bootloader如何从硬盘中加载内核。\n在bootloader代码中有几个比较难懂的点，一个是使能A20，另外一个是保护模式和实模式。使能A20是为了兼容传统的16位PC的解决方案。保护模式和实模式涉及到CS:IP的计算方式。\n本文不会涉及到具体代码，简单记录了比较关键的部分。特别是较清晰的梳理了BIOS、bootloader和内核之间的关系，以及其各自对应的启动流程。\nThe PC's Physical Address Space 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  +------------------+ \u003c- 0xFFFFFFFF (4GB) | 32-bit | | memory mapped | | devices | | | /\\/\\/\\/\\/\\/\\/\\/\\/\\/\\ /\\/\\/\\/\\/\\/\\/\\/\\/\\/\\ | | | Unused | | | +------------------+ \u003c- depends on amount of RAM | | | | | Extended Memory | | | | | +------------------+ \u003c- 0x00100000 (1MB) | BIOS ROM | +------------------+ \u003c- 0x000F0000 (960KB) | 16-bit devices, | | expansion ROMs | +------------------+ \u003c- 0x000C0000 (768KB) | VGA Display | +------------------+ \u003c- 0x000A0000 (640KB) | | | Low Memory | | | +------------------+ \u003c- 0x00000000    low Memory对应的是早期的处理器，只能够访问640KB内存空间； 然而16位的8088处理器能够访问1MB的地址空间，可以知道对应地址总线是20位，但是由于数据总线是16位，对应的寄存器也就是16位的，所以intel采用segment:offset来计算物理地址1； VGA(video display buffers)：视频显示用的，每隔一定的时间，对应的硬件就会从该区域读取要显示的图像。 ROMs：存放固件，非易失存储器。 BIOS ROM：一般是nor flash，nor flash支持XIP(eXecute In Place)，所以可以在这里存放bios代码。 Extended Memory：高于1MB的剩余物理空间，最多到4GB。   JOS只能使用前256MB的物理内存？\nJOS内核的虚拟地址空间是[0xf0000000,0xffffffff]，大小是256MB。所以内核在没有高端内存的概念下，只能管理256MB物理内存。\n bios和bootloader bios的主要工作内容是：系统初始化和自检、加载bootloader到内存中以及将控制权交给bootloader。下面是bios-\u003ebootloader-\u003ekernel entry的大致流程：\n  处理器启动时进入real模式，并设置cs=0xf000和ip=0xfff0。物理地址是：physical address = 16 * segment + offset，所以第一条指令的地址是0xffff0，位于BIOS ROM空间；\n  ROM BIOS开机自检，其第一个指令是[f000:fff0] 0xffff0:\tljmp $0xf000,$0xe05b；\n  ROM BIOS完成初始化后，会将我们的启动代码（bootloader）从硬盘第一个扇区（512字节）加载到物理地址0000:7c00，最后jmp指令跳转到我们的代码处执行，也就是物理地址0000:7c00；\n   Enable A201：80286（24根地址线）为了兼容8088（20根地址线）；\n 8086所能访问的物理地址空间：0(0x0000:0x0000)到0x10FFEF(FFFF:FFFF)，多出来了第20位，因为8086地址线只有20根，所以没有影响；\n但是在80286中，Intel把地址线扩展成24根了，FFFF:FFFF真的就是0x10FFEF了，你让那些legacy software怎么活？！本来人家想读0xFFEF的，怎么成了0x10FFEF？不是人家不想好好工作，是你硬件设计的不让人家好好工作嘛。\n乃们看出来了没？A20 是 80286 时代照顾8088软件的产物。通常所说的32位保护模式是 80386 才出现的，**所以，A20跟保护模式毛关系都没有！**开不开都一样进，影响的只是第20位而已\n   切换模式：从实模式切换到保护模式，保护模式使用gdt和ldt（段寄存器和偏移从16位到32位），可以访问到比实模式更多的物理地址空间；\n  根据elf的信息将内核代码从硬盘加载到物理内存中；\nIO端口地址2：\n1 2 3  I/O Address\tUSE 0010-001F\tSystem 01F0-01F7\tIDE interface - Primary channel     习题：\n  At what point does the processor start executing 32-bit code? What exactly causes the switch from 16- to 32-bit mode?\n1 2 3 4  # 前面使用lgdt指令将gdt加载到对应的寄存器 orl $CR0_PE_ON, %eax movl %eax, %cr0 ljmp $PROT_MODE_CSEG, $protcseg   上面的代码使能保护模式，将cr0寄存器的第一位置1；第三行代码实在32bit模式下运行的。\n  What is the last instruction of the boot loader executed, and what is the first instruction of the kernel it just loaded?\n((void (*)(void)) (ELFHDR-\u003ee_entry))();\n运行readelf -h obj/kern/kernel可以知道kernel的entry是0x1000c，所以第一条指令是：f010000c:\t66 c7 05 72 04 00 00 movw $0x1234,0x472。\n 这里的0xf010000c是虚拟地址（线性地址），0x1000c是物理地址；bootloader将内核加载到线性地址，这时还没有开启mmu，所以在开启mmu之前的代码必须是位置无关代码；如果要访问位置相关的代码，需要使用RELOC计算出线性地址对应的物理地址；\nkernel.ld /* Link the kernel at this address: \".\" means the current address */ . = 0xF0100000; /* AT(...) gives the load address of this section, which tells the boot loader where to load the kernel in physical memory */ .text : AT(0x100000) { *(.text .stub .text.* .gnu.linkonce.t.*) }     Where is the first instruction of the kernel?\n0x1000c\n  How does the boot loader decide how many sectors it must read in order to fetch the entire kernel from disk? Where does it find this information?\nkernel最终被编译成elf文件，elf头部信息包含代码的详细信息。\n  内核启动流程 下面是内核汇编代码的大致运行流程：\n  此时分页系统还未开启，内核访问位置相关的代码（比如数据），需要使用RELOC，手动将虚拟地址转换成物理地址（取数据时需要转换）；\n  将页目录表基地址加载到cr3寄存器，再修改cr0寄存器，开启分页系统；\n1 2 3 4 5 6 7 8 9  pde_t entry_pgdir[NPDENTRIES] = { // Map VA's [0, 4MB) to PA's [0, 4MB)  [0] = ((uintptr_t)entry_pgtable - KERNBASE) + PTE_P, // Map VA's [KERNBASE, KERNBASE+4MB) to PA's [0, 4MB)  [KERNBASE\u003e\u003ePDXSHIFT] = ((uintptr_t)entry_pgtable - KERNBASE) + PTE_P + PTE_W }; 可以看到这里映射了两段虚拟地址，分别是[0, 4MB) 和 [KERNBASE, KERNBASE+4MB)。讲一下为什么要映射VA[0,4MB)？因为当前的ip处于[0,4MB)，所以开启分页系统之后ip仍然处于[0,4MB)，如果没有VA[0,4MB)的映射会出现page fault。     修改ip的值到内核空间；\n1 2 3 4 5  mov\t$relocated, %eax jmp\t*%eax relocated: ... 编译的时候都是按虚拟地址来进行编译的，所以relocated保存的是虚拟地址。     进入c语言代码；\n  习题：\n  Explain the interface between printf.c and console.c. Specifically, what function does console.c export? How is this function used by printf.c?\nprintf库解析传进来的参数转换成字符串，并一个个的传给console；\nconsole将字符串打印出来，包括串口和显示屏显示；\n  For the following questions you might wish to consult the notes for Lecture 2. These notes cover GCC's calling convention on the x86.\nTrace the execution of the following code step-by-step:\n1 2  int x = 1, y = 3, z = 4; cprintf(\"x %d, y %x, z %d\\n\", x, y, z);    In the call to cprintf(), to what does fmt point? To what does ap point? List (in order of execution) each call to cons_putc, va_arg, and vcprintf. For cons_putc, list its argument as well. For va_arg, list what ap points to before and after the call. For vcprintf list the values of its two arguments.   我们知道再x86中参数都通过栈传递（现在一部分是通过通用寄存器传递的），而参数压栈的顺序是从右到左，所以fmt是指向上一个函数的栈顶，也就是0x8(%ebp)。ap指向fmt的前一个参数，也就是lea 0xc(%ebp),%eax。具体看下面的代码：\n int cprintf(const char *fmt, ...) { f0100ad1:\t55 push %ebp f0100ad2:\t89 e5 mov %esp,%ebp f0100ad4:\t83 ec 10 sub $0x10,%esp va_list ap; int cnt; va_start(ap, fmt); f0100ad7:\t8d 45 0c lea 0xc(%ebp),%eax // 取参数（除了fmt参数）的首地址 cnt = vcprintf(fmt, ap); f0100ada:\t50 push %eax f0100adb:\tff 75 08 pushl 0x8(%ebp)\t// 取值，该值指向fmt的字符串首地址 f0100ade:\te8 b7 ff ff ff call f0100a9a \u003cvcprintf\u003e va_end(ap); return cnt; }   Let's say that GCC changed its calling convention so that it pushed arguments on the stack in declaration order, so that the last argument is pushed last. How would you have to change cprintf or its interface so that it would still be possible to pass it a variable number of arguments?\n  The Stack 下面是栈的大概分布情况：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  +------------+ | | arg 2 | \\ +------------+ \u003e- previous function's stack frame | arg 1 | / +------------+ | | ret %eip | / +============+ | saved %ebp | \\ %ebp-\u003e +------------+ | | | | | local | \\ | variables, | \u003e- current function's stack frame | etc. | / | | | | | | %esp-\u003e +------------+ /   根据发生错误的ip输出backtrace：通过链接脚本，将程序的符号表加载到elf文件中，具体可以参看stabs。\n参考链接   OS boot 的时候为什么要 enable A20？ - 坂本鱼子酱的回答 - 知乎 https://www.zhihu.com/question/29375534/answer/44137152 ↩︎\n IO端口地址分布, http://web.archive.org/web/20040304063834/http://members.iweb.net.au/~pstorr/pcbook/book2/ioassign.htm ↩︎\n   ","description":"","tags":["mit6828 os实验"],"title":"Lab1 - 实验笔记","uri":"/posts/course/mit6828/lab1/"},{"categories":["文件系统"],"content":"嵌入式采用的存储设备一般是flash，由于flash同块设备有很较大的差异，因此并不能在flash设备上采用通用文件系统，比如Fat、Ext等等。现在对flash存储器的文件系统越来越多了，主要可以分成两大类：一类是基于转译层和块的设备文件系统，另一类是基于日志结构的文件系统。\n基于转译层的文件系统之所以称为基于转译层，是因为在文件系统和flash设备之间有一个中间层叫转译层。这个转译层可以将 Flash 设备模拟成块设备，以供文件系统控制和使用。而基于日志结构的文件系统又可以分为两大类：一类是通用的flash文件系统，一类是专用的flash文件系统。通用的文件系统可以在nor flash上运行，也可以在nand flash上运行。以 jffs为代表的文件系统就是这样的通用 flash文件系统。另一类专业的flash文件系统指的是专门针对flash的文件系统，以yaffs系列的文件系统为代表1。 具体可以参看下图2：\n本文首先介绍了flash的特性，最后介绍了flash上常用的文件系统，以及对应的数据压缩技术、坏块管理技术、磨损均衡技术和垃圾回收技术。\nflash介绍34 flash存储器，也叫快擦写存储器，和磁盘设备相比，flash存储器在体积、扩展性、耗电等方面都有着显著的优势，因此成为了嵌入式系统首选的存储设备。\nflash属于非易失性半导体存储器，其类型目前主要两种：nor flash和nand flash。 nand flash广泛应用在各种存储卡、U盘、SSD、eMMC等等大容量设备中。nor flash 能快速随机读取，允许系统直接从存储器中读取代码执行即芯片内执行技术（eXecute In Place），而无需先将代码下载至RAM中再执行，可以像普通ROM一样执行程序。\nflash存储器的擦除次数是有限的，一般是$10^6$次。当某块执行过度的擦除操作后，这一块的存储空间将会变为“只读”状态，不能再写入数据。flash存储器也存在着两个主要缺陷：\n 一是在重写之前必须进行擦除，因为flash存储器划分成很多擦除块（sector-erase），对任何一位数据进行修改必须先擦除整个块（sector）； 二是擦除块的擦除次数有限，当一个块提前达到擦除次数上限时，将导致整个存储器无法使用。  nor flash Intel于1988年首先开发出nor flash存储器技术，这也是最早出现的flash存储器技术，目前主要由供应商Intel与AMD支持，主要应用于擦除和编程操作较少而直接执行代码的场合，尤其是纯代码存储应用的理想选择，如PC的BIOS固件、移 动电话、嵌入式系统中装载启动代码等。其特点如下：\n 又名线性flash存储器； 能快速随机读取，允许系统直接从存储器中读取代码执行即芯片内执行技术（eXecute In Place），而无需先将代码下载至RAM中再执行； 可以单字节或单字编程，但不能单字节擦除，必须以块为单位或对整片进行擦除操作，重新编程之前需要先擦后写。因执行一个编程擦除操作时间较慢（大约5秒），而块尺寸又较大（64KB-128KB），于是在纯数据存储应用中花费的时间较长。  nand flash 自1989年东芝公司开发出nand flash以来，如今主要由三星、富士通及东芝等日韩厂商大力支持，这种结构的存储器适合于数据和文件存储，主要作为SM卡、CF卡、PCMCIA卡、固态盘的存储介质，并正成存储器技术的核心。其特点如下：\n 又名非线性flash存储器； 以页（256B-512B）为单位进行读和编程操作，以块(4KB-32KB)为单位进行擦除操作,编程擦除时间短一般每块时间为4ms； 实现串行读取，随机读取速度慢且不能按字节随机编程； 芯片尺寸小，引脚少，是位成本最低的固态存储器； 芯片包含有失效块，其数目最大可达到3-35块（取决于存储器密度），失效块虽不影响有效块的性能，但设计者仍需要在地址映射表中屏蔽它。  对比表格    类型 nor flash nand flash     存储容量 小，一般1MB-32MB 大，一般8MB-512MB   擦除块大小 64KB-256KB 6KB-128KB，每块被分为若干个512Bytes的页面，每页面含有16Bs附加存储区   读写方式 线形时间按字节随机读写 以页面为单位读写，类似磁盘   读写速度 读取快，写入慢 读取慢，写入快   擦除时间 慢，2s-5s/块 快，2ms-5ms/块   坏块比例 低，出厂一般没有坏道 高，出场可能有坏道   价格 低存储密度，尺寸大，价格高 高存储密度，尺寸小，价格低   接口形式 接并行总线，内存接口 接串行总线，I/O接口   寿命 十万次级 百万次级   可靠性 高，位交换几率低 低，位交换几率高，必须采用错误探测/错误更正（EDC/ECC）算法   XIP 可以 不支持   易用性 容易 复杂，一般需要MTD驱动支持   设计目的 作为ROM的替代产品 磁盘   编程方式 允许逐字节编程 页面编程，且每个页面的编程次数有限制i    基于flash的文件系统 flash存储器的擦除次数是有限的，一般是$10^6$次。当某块执行过度的擦除操作后，这一块的存储空间将会变为“只读”状态，不能再写入数据。根据以上特点，为了避免某些块的过度操作，而导致存储卡使用寿命降低，设计专门针对flash存储器的文件系统是必要的。嵌入式文件系统要考虑的特性4：\n 数据压缩：通过压缩算法不仅可以降低数据的容量，而且间接的加速了I/O； 坏块管理：坏块通常使用带外区域中的特定标记来标识； 磨损均衡：由于flash存储芯片的擦除次数是有限制的，文件系统对存储器使用时必须充分考虑这个特性，因此最好能均匀使用存储芯片的每个扇区，以延长存储器的使用寿命。 垃圾回收：  jffs25 jffs文件系统最早是由瑞典Axis Communications公司基于Linux2.0的内核为嵌入式系统开发的文件系统。jffs2是一个可读写的、压缩的、日志型文件系统，并提供了崩溃掉电安全保护，克服了jffs的一些缺点：使用了基于哈希表的日志节点结构,大大加快了对节点的操作速度；支持数据压缩；提供了“写平衡”支持；支持多种节点类型；提高了对flash存储器的利用率，降低了内存的消耗。它的缺点就是当文件系统己满或接近满时，运行会变慢，这主要是因为垃圾回收的问题。\n数据压缩 jffs2提供了jffs所没有的数据压缩技术。数据在存放到flash之前会先被压缩，这个过程是由jffs2自动完成。因 此，在使用jffs2时无需自己对数据进行压缩。各种类型的压缩算法皆可内嵌到系统中去，目前jffs2使用的是zlib算 法。该算法比较适合ASCII文件，若压缩对象是二进制文件，压缩效果并不明显。\n坏块管理 暂无\n磨损均衡 jffs并没有采用磨损均衡技术，到了jffs2才使用此技术。磨损均衡技术的引入提高了flash的使用寿命期限。jffs2改变了jfss没有块队列的状态，将flash各个块（block）分成三个队列：脏块队列（dirty_list）、干净块队列（clean_list）以及空闲块队列。脏块是指该块上至少有一个节点被标记为废弃（obsoleted）；干净块是指块上 的数据皆有效。工作流程如下：\n 当系统的空闲块数小于6时，GC进程会被唤醒； 判断jiffies计数器的值，如果jiffies% 100非零，选中脏块队列。否则，将选中干净块队列。（很显然,如果存在脏块的话，选中后者的概率仅为1%。） 如果选中的是脏块队列，则直接进行擦写操作，最后把它挂接到空闲块队列； 如果选中的是干净块队列，则被选中的干净块中的所有数据要被全部移至其它空闲块中，接着对该块进行擦写操作，最后把它挂接到空闲块队列。  垃圾回收 jffs2的垃圾收集技术的原理：当需要增添新内容时，就在节点链表的末端添加新的节点存储新的内容；若要修改文 件的某部分，jffs2将该部分标记为废弃，并在节点链表末端添加修改后的内容。jffs2如此不断地在flash上添加新的内容，当flash上的存储空间用完时,系统就回收标记为废弃的空间，该过程就称为垃圾收集。垃圾收集进程(简称GC进程)专门负责该项工作。下面是jffs2 GC进程的工作流程：\n 当系统的空闲块数小于6时，GC进程会被唤醒； GC进程每次只回收一个空闲块； 如果此时flash上的空闲块数仍小于6，那么GC进程必须再回收一个空闲块，直到总的空闲块数达到6为止。  断电保护 断电保护技术的实现依赖于jffs2的日志式存储结构。当系统遭受不正常断电后重新启动时，jffs2自动将系统恢复到断电前最后一个稳定状态。\nyaffs26 数据压缩 暂无\n坏块管理 暂无\n磨损均衡 yaffs2 并未实现真正的磨损均衡算法，而是通过顺序写策略和串行块分配策略将擦除次数尽可能均匀地分布在闪存空间上，测试结果表明其均衡效果较好，但忽略了静态数据和“冷”数据的影响，因此是一种局部的磨损均衡机制，有待进一步的改进。\n顺序写策略：\n串行块分配策略：\n垃圾回收 yaffs2垃圾回收操作一般在向文件写数据（函数 yaffs_wr_data_obj）、更新对象头（函数 yaffs_update_oh）和文件截短操作（函数 yaffs_resize_file）等过程中被调用，由函数yaffs_check_gc视当前闪存空间使用情况决定是否触发垃圾回收操作。YAFFS2 的垃圾回收操作分三步进行：\n 查找到满足回收条件的目标回收块（由函数yaffs_find_gc_block 函数完成），若未找到，则退出； 遍历块中所有页，若页中存放有效数据，则将数据复制到其它块中的空闲页中，然后删除该页，并修改内存中记录闪存空间使用信息的数据结构（如块信息结构 yaffs_block_info，页位图数组 chunk_bits 等）以体现 闪存空间使用情况的变化（由函数 yaffs_gc_process_chunk 完成）； 当块中所有 chunk 都被删除后，则擦除该块（由函数 yaffs_block_became_dirty 完成），使其可以被再次写入数据。  ubifs2 ubifs文件系统主要包含ubifs和ubi模块，ubifs模块建立在ubi模块的基础上，ubi模块是建立在mtd模块基础上的。三个模块整体关系如下图2：\nubifs的设计主要是为了解决jffs2文件系统存在的启动时间长、系统扩展性差、内存消耗量大、损耗均衡处理能力差等问题。jffs2的挂载时间是与闪存大小成线性比的（扫描所有的块），ubifs克服了这一弱点（ubi子模块挂载时间还是与闪存大小成线性比的）。\nubifs同jffs2的三个主要区别：\n jffs2将索引（inode）节点存储在内存上，ubifs将索引节点存储在闪存上 ； jffs2运行在MTD设备层之上，ubifs运行在ubi层之上； jffs2不支持回写（write-back），而ubifs支持回写技术。  坏块管理 ubi在两种情况下认为peb（physical erase block）已损耗：\n 写peb失败，此时，ubi把这个peb的数据搬移到其他的peb，然后诊断该peb是否出现为坏块，如果是，标记为坏块； 擦除操作出现I/O错误，在这种情况下，直接把peb标记为坏块。  磨损均衡 暂无\n垃圾回收 暂无\n  周林. 通用嵌入式文件系统YAFFS的移植[D]. ↩︎\n 冯子陵. 闪存文件系统UBIFS的分析与优化[D]. 2013. ↩︎\n NAND vs. NOR Flash MemoryTechnology Overview ↩︎\n 孙健. 基于Flash存储器的嵌入式文件系统的研究与实现[D]. 西安电子科技大学. ↩︎\n 李桂良, 刘发贵. JFFS2文件系统的关键技术及其在嵌入式系统的应用[J]. 计算机应用, 2003(07):137-139. ↩︎\n [优质文章]李恒恒. 基于YAFFS2文件系统的NAND Flash存储管理关键技术研究[D]. ↩︎\n   ","description":"","tags":["文件系统"],"title":"嵌入式文件系统","uri":"/posts/filesystem/%E5%B5%8C%E5%85%A5%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/"},{"categories":["linux内核"],"content":"进程地址空间布局 下图是虚拟地址空间分布1。\n我们的运行文件是elf格式的，在我们运行程序时，linux通过load_elf_binary函数建立上图的地址空间。\n内存描述符mm_struct 内核使用内存描述符结构体表示进程的地址空间，该结构包含了和进程地址空间有关的全部信息。内存描述符由struct mm_struct结构体表示。下面给出内存描述符的结构和各个域的描述：\n1 2 3 4 5 6 7  struct mm_struct { struct vm_area_struct * mmap; /* 虚拟内存区域列表 */ struct rb_root mm_rb; struct vm_area_struct * mmap_cache; /* 上一次find_vma的结果 */ ... // 地址空间的各种段：栈段、mmap、堆... }   mmap成员使用单独链表连接所有的内存区域对象，所有的区域按地址增长的方向排序，mmap域指向链表中第一个内存区域。mm_rb域使用红一黑树连接所有的内存区域对象。mm域指向红一黑树的根节点。\n虚拟内存区域vm_area_struct struct vm_area_struct结构体描述了指定地址空间内连续区间上的一个独立内存范围。内核将每个内存区域作为一个单独的内存对象管理，每个内存区域都拥有一致的属性，比如访问权限等，另外，相应的操作也都一致。按照这样的方式，每一个VMA就可以代表不同类型的内存区域~~（比如内存映射文件或者进程用户空间栈）~~，下面给出该结构定义和各个域的描述；\n查找VMA  find_vma函数：该函数在指定的地址空间中搜索第一个vm_end大于addr的内存区域。换句话说，该函数寻找第一个包含addr或首地址大于addr的内存区域，如果没有发现这样的区域，该函数返回NULL。否则返回指向匹配的内存区域的vm_area_struct结构体指针。注意，由于返回的VMA首地址可能大于addr，所以指定的地址并不一定就包含在返回的VMA中。因为很有可能在对某个VMA执行操作后，还有其他更多的操作会对该VMA接着进行操作，所以find_vma()函数返回的结果被缓存在内存描述符的mmap_cache域中。 find_vma_prev函数：返回第一个首地址小于addr的VMA。 find_vma_intersection函数：返回第一个和指定地址区间相交的VMA。  创建VMA 内核使用do_mmap()函数创建一个新的线性地址区间。但是说该函数创建了一个新VMA并不非常准确，因为如果创建的地址区间和一个已经存在的地址区间相邻，并且它们具有相同的访问权限的话，两个区间将合并为一个。如果不能合并，就确实需要创建一个新的VMA了。\n删除VMA undone\n堆的管理 参考链接   How “linux process address space” is stored? ↩︎\n   ","description":"","tags":["linux内核"],"title":"进程虚拟地址空间","uri":"/posts/linux-kernel/%E8%BF%9B%E7%A8%8B%E8%99%9A%E6%8B%9F%E5%9C%B0%E5%9D%80%E7%A9%BA%E9%97%B4/"},{"categories":["机器人"],"content":"人工势场法，即Artificial Potential Field，最早于20世纪80年代中期，由Khatib和Krogh提出的一种虚拟力法，其基本思想是将机器人在环境中的运动视为一种虚拟的人工受力场中的运动，其中障碍物对机器人产生斥力，而目标点对机器人产生引力，引力和斥力的合力作为机器人的加速力来控制机器人的运动方向和计算机器人的位置。\n人工势场法具有结构简单、易于理解、便于实现实时控制的优点。其缺点也非常明显，比较容易陷入局部极小值问题，即机器人受到的合力为零，机器人停止运动，但却没有到达目标位置，陷入局部极小值。对于编队机器人数量比较大或者编队处于复杂环境中时，极小值问题显得尤为突出。此外，应用人工势场法的主要工作在于势函数的设计，对于未知环境中无法得知障碍物的尺寸、形状，这使得很难选取合理的势场点，进而很难设计出良好的势函数。基于这个原因，目前人工势场法主要用于全局环境已知情况下的多机器人编队。\n下图中包括多机器人，障碍物和目标物，目标位置势场最低，即其虚拟重力势能是最小的,机器人的最初处于较高的位置因此具有较大的重力势能，环境中的障碍物可以看成为重力场中的凸点,因此机器人由于重力势能从初始位置运动到目标位置的过程可以看成从高处向低处运动的物体。环境中虚拟的势场用等势线的形式表现出来，即图中的密集分布的曲线上的势场都是相等的。因此容易得到如果机器人的运动模型是完整约束的，则机器人的理想运动曲线应该是垂直于所有的等势线的。然而因为机器人模型和障碍物形状的不规则，使得势场法在实际应用中存在一定的缺陷1。\n机器人群集的基本特征：避障避碰、速度匹配和群集中心。相对的，在多机器人群集势场包含三个基本特征，因此可设$U_{obstacle}(q)$为障碍物产生的势场,$U_{neighbor}(q)$为群集中周围邻域的机器人产生的势场，$U_{centre}(q)$为群集中心产生的势场，另外群集运动目标产生的势场为$U_{goal}(q)$，其中q表示机器人的当前位置。进行势场叠加后可得：\n $$ U_{\\Sigma}(q)=\\underbrace{U_{\\text {goal }}(q)+U_{\\text {centre }}(q)}_{\\text {引力势场 }}+\\underbrace{\\sum U_{\\text {obstacles }}(q)}_{\\text {斥力势场 }}+\\underbrace{\\sum U_{\\text {neighbors }}(q)}_{\\text {引力/斥力同时存在 }}\\\\ F(q) = -\\nabla U_{\\Sigma}(q) $$  $U_{\\Sigma}(q)$表示机器人在环境中$q$位置时的综合势场，$F(q)$则表示势场中的势场力，方向沿势场的负梯度方向。 在通常情况下，对于机器人而言环境地图一般都是未知的，常用的策略是传感器对环境进行实时感知并获得局部的位置信息，以进行决策，因此基于这种反应式的势场反馈导航原理如下图所示：\n目标势场和障碍物势场 目标势场体现一种全局的引力效应，即势场存在于环境中一切位置，表现出来的特性为机器人离目标点越远，引力越大，按照这种规律可以建立目标势场函数：\n $$ \\left\\{\\begin{array}{ll} U_{goal}(x) = \\frac{1}{2}k(x-x_{goal})^2\\\\ F_{goal}(x) = -k\\|x-x_{goal}\\| \\end{array}\\right. $$  其中$x_{goal}$为目标点位置，$x$为机器人的实时位置。因此目标点的引力势场通常是基于全局地图的，即机器人和目标点在全局地图中的位置己知，或者机器人 与目标点的相互关系己知为前提。 相对的，障碍物势场为局部排斥力效应，即当且仅当机器人与障碍物之间的距离小于某定值时，才会有斥力势场的存在，而通常情况下要建立具有障碍物的环境地图，尤其是当环境中的障碍物变化的时候，全局地图的建立比较困难，通常采用基于外部传感器的反应式避障。设$f(x)$为当机器人处于$x$位置时激光探测距离，常数$d$为斥力场范围,因此当$f(x)\u003cd$时机器人将受到斥力场的作用。\n $$ \\left\\{\\begin{array}{ll} U_{\\text {obs }}(x)=\\left\\{\\begin{array}{ll} \\frac{\\eta}{2}\\left(\\frac{1}{f(x)}-\\frac{1}{d}\\right)^{2} \u0026 f(x) \\leq d \\\\ 0 \u0026 f(x)d \\end{array}\\right. \\\\ F_{\\text {obs }}(x)=\\left\\{\\begin{array}{ll} \\|\\left(\\frac{1}{f(x)}-\\frac{1}{d}\\right) \\frac{\\eta \\cdot \\partial f}{f^{2} \\cdot \\partial x} \\| \u0026 f(x) \\leq d \\\\ 0 \u0026 f(x)d \\end{array}\\right. \\end{array}\\right. $$  邻域机器人势场 借用物理学中对微小粒子（如分子，原子）的引力和斥力的关系，来描述邻域机器人的势场规律。假设机器人$i$与$j$相邻，则$i$相对$j$的势场函数和势场力函数分别为：\n $$ \\left\\{\\begin{array}{l} U_{n e i g}\\left(x_{i} | x_{j}\\right)=\\frac{\\mu}{2} \\ln ^{2} \\frac{\\left\\|x_{i}-x_{j}\\right\\|}{\\delta}=\\frac{\\mu}{2} \\ln ^{2} \\frac{\\left\\|r_{i j}\\right\\|}{\\delta} \\\\ F_{\\text {neig }}\\left(x_{i} | x_{j}\\right)=-\\frac{\\mu \\cdot \\delta}{\\left(x_{i}-x_{j}\\right)} \\cdot \\ln \\frac{|| x_{i}-x_{j} \\|}{\\delta}=-\\frac{\\mu \\cdot \\delta}{r_{i j}} \\cdot \\ln \\frac{\\left\\|r_{i j}\\right\\|}{\\delta} \\end{array}\\right. $$  可以得到该势场函数和势场力函数对应的曲线： 群集中心势场 在群集中除了目标点、障碍物以及邻域机器人这些可见的势场源外，还存在一种无形的吸引势场使得机器人朝某群集中心聚拢，该势场就是群集中心产生的 势场。首先群集中心本来就是一个非实体的概念，一般将群体的几何中心作为机 器人的群集中心，这样可以缩短机器人聚拢的距离，因此如果设群集中机器人的 个数为$M$个，则群集的几何中心为：\n $$ \\bar{x}=\\sum_{1}^{M} x_{j} / M $$  由于同为引力场，所以类似目标点的势场函数规律可得群集中心的势场和势场力满足：  $$ \\left\\{\\begin{array}{ll} U_{centre}(x) = \\frac{1}{2}c\\|x-\\bar{x})^2\\| \\\\ F_{centre}(x) = -c(x-\\bar{x}) \\end{array}\\right. $$   参考文献   刘磊. 多移动机器人编队及协调控制研究[D]. 华中科技大学, 2009. ↩︎\n   ","description":"","tags":["机器人","多车编队"],"title":"多车编队 人工势场法","uri":"/posts/robot/%E5%A4%9A%E8%BD%A6%E7%BC%96%E9%98%9F-%E4%BA%BA%E5%B7%A5%E5%8A%BF%E5%9C%BA%E6%B3%95/"},{"categories":["机器人"],"content":"基于图论的方法研究多机器人编队，需要将队形信息转换成各种图，依靠图论知识及李雅普诺夫方法分析编队的稳定性，得出队形的控制策略。该方法中机器人编队队形是依据图论理论中图的节点与边的关系来描述。节点即表示机器人的运动学特性，即是对运动方式的描述，而边则表示机器人之间的关系，是对运动的约束，利巧相应的图论知识与控制理论知识研究编队队形控制输出具有一致性。基于图论法的优点在于图的形状可任意变化，队形描述也相对简单，编队中改变队形较容易，并且图论的相关理论研究比较成熟，但不足之处在于物理实现比较复杂，通常只能适用于仿真环境的理论研究1。\n用图论的方法描述车辆编队，这种表示方法能够唯一的（无二义性的）定义多车辆编队队形，准确反映出编队中各车辆间的相互作用关系。车辆队表示为图$G=(V,E)$，其中，V 表示图的结点集，代表车辆集$V={1,2,...,N_a}$ } ； E 表示图的边集，其中的边元素是相关车辆 $i,j\\in V$ 间的一个有序数对$(i,j)\\in E$，表示车辆间的连接关系，对于有$N_a$个车辆的系统，至少用$M=N_a-1$个向量就可以对车辆编队进行唯一描述。编队图被认为是无方向的，所以$(i,j)\\in E,(j,i)\\in E$，如$(i,j)\\in E$，那么车辆 i 和 j就被称为相邻车辆，车辆$i$的相邻车辆集表示为$N_i\\in V$。另外，为了保证车辆编队沿给定的路径运行，所以将编队中与寻径相关的车辆定义为核心车辆。尽管在编队中任意两个核心车辆之间可能没有相关向量，但我们认为核心车辆间均为相邻车辆关系，因为它们之间通过寻径目标耦合2。\n两相邻车辆$i $和$j$间的期望相关向量表示为$d_{ij}\\in R^n$，它表示处于期望编队路径位置时，两相邻车辆间的距离。\n给定了编队图$G=(V,E)$和编队路径$q_{ref}$后，就可以定义出编队向量$F=(f_1,...,f_{M+1})\\in R^{n(M+1)}$它的分量$f_1\\in R^n$定义如下：\n $$ f_l = q_i-q_j+d_{ij},\\forall l=1,...,M\\\\ f_{M+1}=q_{\\Sigma}-q_d $$  其中$i$表示构成边$e_l$的尾结点，$j$表示构成边$e_l$的头结点。$q_{\\Sigma}$表示当前车辆的几何中心，$q_d$表示期望的车辆几何中心。很明显，当$F\\equiv 0$时车辆组成编队，得到从$q$到$F$的映射关系：  $$ F=Gq+\\hat{d}, G^T=[C_{(n)}\\quad V]\\\\ \\hat{d}=(...,d_{ij},...,-q_d)\\\\ V^T=[V_1...V_{N_a}]\\in R^{n\\times n\\times N_a} $$  $V_i\\in R^{n\\times n}$定义如下，  $$ Vi=\\left\\{\\begin{array}{ll} \\frac{1}{N_a}I_{(n)} \\quad if \\quad i=1,2,...,N_a\\\\ 0\\quad otherwise \\end{array}\\right. $$  矩阵$C_{n}\\in R^{n(N_a\\times M)}$与关联矩阵$C\\in R^{N_a\\times M}$有关，其中$C=[c_{ij}]$的元素根据边集$E$的元素定义如下，  $$ c_{ij} = \\left\\{\\begin{array}{ll} +1, i为尾结点\\\\ -1,i为头节点\\\\ 0,其它 \\end{array}\\right. $$  根据以上的定义，当$G_{q^c}=-\\hat{q}$ 时就能够计算出每个车辆的期望运行轨线。 参考文献   岑斌斌. 多机器人编队的分布式协同控制方法研究[D]. 南京大学, 2015. ↩︎\n 贺晨龙, 黄丽湘, 张继业. 多车辆编队协作控制[C]// 全国非线性振动、全国非线性动力学和运动稳定性学术会议. 2007. ↩︎\n   ","description":"","tags":["机器人","多车编队"],"title":"多车编队 基于图论的方法","uri":"/posts/robot/%E5%A4%9A%E8%BD%A6%E7%BC%96%E9%98%9F-%E5%9F%BA%E4%BA%8E%E5%9B%BE%E8%AE%BA%E7%9A%84%E6%96%B9%E6%B3%95/"},{"categories":["机器人"],"content":"基于行为的方法的主要思想是将编队任务转换为一系列具有特定功能的基本运动行为，这些基本运动行为是机器人运动的最小运动单位，例如，向左转、前进、奔向目标、姿态调整、蔽障、沿墙运动、目标导航等等。基于行为的方法通常由两部分组成，分别是基本行为设计部分和行为选择设计部分。对编队中的每个机器人而言，机器人的输入可以是自身传感器的探测信息，也可以是通过通信获取编队中其它某个或者多个机器人的行为输出，行为选择设计部分根据机器人输入信息使得机器人选择对应的基本行为（以一定角速度和线速度的方式）进行输出，从而使得机器人处于编队中理想的位置1，具体如下图所示：\n基于行为方法的编队控制包括以下三种控制方法2：\n 行为抑制法：首先对每个行为进行优先级排序，这样当无人车需要同时执行多个行为时，无人车就会按照优先级顺序执行优先级最高的行为。 加权平均法：首先对每个行为规定一个对应的权值，根据对应行为的重要程度设置对应的权值。这样当无人车需要同时执行多个行为时，无人车会计算所有行为的加权求和，得到的结果经过正则化后作为无人车的行为控制； 模糊逻辑法：当无人车需要同时执行多个行为时，通过查看模糊规则表，执行相对的行为控制。  基于行为的方法是一种分布式的控制方法，该方法具有柔性好的特点，能够在编队中动态的添加或者减少机器人的数量，同时该方法使得机器人表现出一定的智能性，类似于人或者其它高等动物的感官反应或者决策思维，更容易被人们所理解和认识。其缺点是难以设计出合适的运动行为以及难以对系统进行数据建模和系统稳定性分析，由于缺乏相关数学理论，对于设计的运动行为会对编队的稳定性造成什么影响目前仍然无法得知，只能通过实践证明，同时，在复杂环境下的机器人的输出行为是难以预见的，这使得该方法的灵活性不高。目前该方法主要运用于较大规模的编队队形、跟踪、空中多飞行器编队、包围入侵者、大面积区域搜索等。\n基于位姿误差的机器人编队基本运动行为设计3 机器人运动学模型 二维空间机器人位置和姿态表示为：\n $$ q_0=[x_0\\quad y_0\\quad \\theta_0]^T $$  假设机器人当前的线速度$v_0$和角速度$\\omega_0$，可得移动机器人的位姿$\\dot{q}$：  $$ \\dot{\\boldsymbol{q}}_{0}=\\left[\\begin{array}{c} \\dot{x}_{0} \\\\ \\dot{y}_{0} \\\\ \\dot{\\theta}_{0} \\end{array}\\right]=\\left[\\begin{array}{cc} \\cos \\theta_{0} \u0026 0 \\\\ \\sin \\theta_{0} \u0026 0 \\\\ 0 \u0026 1 \\end{array}\\right]\\left[\\begin{array}{l} v_{0} \\\\ \\omega_{0} \\end{array}\\right] $$  #### 位姿误差描述 虚拟结构仅被用以描述目标编队的形状和编队中虚拟机器人的固定位置，而非限制该位置上真实存在的机器人。如下图所示：机器人团队由领航者$q_L(x_L,y_L,\\theta_L)$和跟随者$q_{F2}(x_{F2},y_{F2},\\theta_{F2})$、$q_{F3}(x_{F3},y_{F3},\\theta_{F3})$组成。虚线空心图形表示编队中的虚拟机器人，即跟随者$q_{F2}$和$q_{F3}$和应当抵达的位置，表示为$q_{T2}(x_{T2},y_{T2},\\theta_{T2})$、$q_{T3}(x_{T3},y_{T3},\\theta_{T3})$。$q_L$与$q_{T2}$、$q_{T3}$共同组成虚拟的三角形编队。设计的行为应该能够确保跟随者$q_{F2}$和$q_{F3}$到达编队中$q_{T2}$、$q_{T3}$的位置。那么可以通过$q_{F2}$和$q_{T2}$之间的位置关系描述位姿误差。\n最终通过公式推导可以得到：\n $$ \\boldsymbol{e}_{FiTj}=\\left[\\begin{array}{c} e_{FiTj}^{x} \\\\ e_{FiTj}^{y} \\\\ e_{FiTj}^{\\theta} \\end{array}\\right]=\\left[\\begin{array}{ccc} \\cos \\theta_{F i} \u0026 \\sin \\theta_{F i} \u0026 0 \\\\ -\\sin \\theta_{F i} \u0026 \\cos \\theta_{F i} \u0026 0 \\\\ 0 \u0026 0 \u0026 1 \\end{array}\\right]\\left(\\boldsymbol{q}_{T j}-\\boldsymbol{q}_{F i}\\right), i, j \\in[2, n] $$  上式中，$\\bold{q}_{Tj}$为通过传感器或通信获取的虚拟目标机器人的位姿，$\\bold{q}_{Fi}$为机器人自身的位姿信息，主要通过如GPS、编码器等传感器获得。为了使$\\bold{e}_{FiTj}$处于一定的阈值内，设计了两种基本运动行为与作为基本运动行为的选择依据的三种描述性行为。两种基本行为是直线运动与旋转运动。设计的思想是首先机器人采用旋转运动面向目标，然后采用直线运动奔向目标，最后通过旋转和目标完全重合，此时机器人到达编队中虚拟机器人的位置。当所有机器人均完成此任务时，编队形成。上述思想可以归纳为转向目标，奔向目标与姿态调整三种描述性行为。 令阈值为：$\\boldsymbol{\\delta} = [\\delta_p\\quad \\delta_p\\quad \\delta_o]^T$，$\\delta_p$表示位置误差常数，$\\delta_o$表示姿态角度误差常数，$T$为实际需要的有限时间。\n行为选择  转向目标  当机器人与虚拟目标之间的$y$方向误差$|e^y_{FiTj}|\u003e\\delta_p$，时，虚拟机器人$q_{Tj}$在跟随机器人$q_{Fi}$运动的正前方的时候，机器人选择奔向目标的行为。该行为同时考虑了当$q_{Fi}$背离$q_{Tj}$且两者在同一直线上的情况，此时$|e^y_{FiTj}|\u003c\\delta_p,e^x_{FiTj}\u003c0$。该行为的触发条件：\n $$ \\bigcup\\left\\{\\begin{array}{l} \\left|e_{F i T j}^{y}\\right| \\geq \\delta_{p} \\\\ e_{F i T j}^{x}转向目标行为的结束条件为：  $$ \\lim _{t \\rightarrow t_{F i T j}^{T T}}\\left|e_{F i T j}^{y}(t)\\right| 奔向目标  当虚拟机器人$q_{Tj}$在跟随机器人$q_{Fi}$运动的正前方，即$|e^y_{FiTj}|\u003c\\delta_p$时，机器人选择奔向目标的行为。奔向目标行为用于缩短虚拟机器人$q_{Tj}$与跟随机器人$q_{Fi}$之间的直线距离。该行为的触发条件：\n $$ |e^y_{FiTj}|奔向目标行为的结束条件为：  $$ |e^y_{FiTj}| 姿态调整  当虚拟机器人$q_{Tj}$与跟随机器人$q_{Fi}$之间的距离误差分量$|e^x_{FiTj}|\u003c\\delta_p$，$|e^y_{FiTj}|\u003c\\delta_p$，且两者方向误差值$|e^\\theta_{FiTj}|\u003e\\delta_o$的时候，机器人选择姿态调整行为。姿态调整行为是为了让近似处于同一点上的$q_{Tj}$与$q_{Fi}$完成角度的重合。该行为的触发条件：\n $$ |e^x_{FiTj}|姿态调整行为的结束条件为：  $$ |e^x_{FiTj}|参考文献   陈传均. 多机器人领航—跟随型编队控制研究[D]. 杭州电子科技大学. ↩︎\n 王京.多智能体系统编队避障算法研究[D].上海:华东理工大学,2013. ↩︎\n 刘磊. 多移动机器人编队及协调控制研究[D]. 华中科技大学, 2009. ↩︎\n   ","description":"","tags":["机器人","多车编队"],"title":"多车编队 基于行为的方法","uri":"/posts/robot/%E5%A4%9A%E8%BD%A6%E7%BC%96%E9%98%9F-%E5%9F%BA%E4%BA%8E%E8%A1%8C%E4%B8%BA%E7%9A%84%E6%96%B9%E6%B3%95/"},{"categories":["c语言"],"content":"本文主要汇总了c语言的一些运算符，因此我们能对运算符有全局的认识。\n   运算符 名称     算术运算符 +, -, *, /, %   移位运算符 \u003e\u003e, \u003c\u003c   位运算符 \u0026, |, ^   赋值运算符 =   复合赋值运算符 +=, -=, *=, /=, %=, \u003c\u003c=, \u003e\u003e=, \u0026=, ^=   单目运算符 !, ++, -, \u0026, sizeof, ~, --, +, *, 强制类型转换   关系运算符 \u003e, \u003e=, \u003c, \u003c=, !=, ==   逻辑运算符 \u0026\u0026, ||   条件运算符 a?b:c   逗号运算符 ,   下标运算符 []   函数运算符 ()   结构成员运算符 ., -\u003e    算术运算符 算术运算符包括：+, -, *, /, %，它们也都是双目运算符。\n 注意同单目运算符中的+, -, *区分开来，算术运算符时双目运算符，所以使用它们时并不会造成语义上的歧义。\n  `, `    = \u0026= ^= |=--` 单目运算符 单目运算符包括：!, ++, -, \u0026, sizeof, ~, --, +, *, 强制类型转换\n  经过!的数值只有两种结果： $$ !a=\\left{\\begin{array}{cc} 0,a\\neq0 \\\n1,a=0 \\end{array}\\right. $$\n  ~会对所有的位取反，包括符号位：\n1 2 3 4 5 6  #include \u003cbits/stdc++.h\u003eusing namespace std; int main(){ int a = 0; cout \u003c\u003c ~a \u003c\u003cendl; //-1 }     sizeof的参数可以是数组、指针、类型、对象、函数等，其作用是计算静态对象的内存大小；\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  // sizeof的宏定义原型：#define sizeof(L_Value) ((char*)(\u0026L_Value + 1) - (char*)(\u0026L_Value))  #include \u003cbits/stdc++.h\u003eusing namespace std; int main(){ char a[100]; char *a_ptr = a; cout \u003c\u003c (void *)a \u003c\u003c \" \" \u003c\u003c (void *)a_ptr \u003c\u003cendl; cout \u003c\u003c sizeof(a) \u003c\u003c \" \" \u003c\u003c sizeof(a_ptr) \u003c\u003cendl; //这里可以看出数组和指针并非完全相等，编译器对其处理的方式还是不同的。  //0x62fdb0 0x62fdb0  //100 8 }       = `           -- ","description":"","tags":["c语言","c语言运算符"],"title":"c语言运算符","uri":"/posts/c/%E8%BF%90%E7%AE%97%E7%AC%A6/"},{"categories":["linux内核"],"content":"伙伴算法是一种动态存储分配算法，用于实现操作系统内核空间和用户空间 (如 C 语言库) 的分配和回收操作.。Knowlton和 Knuth最早系统地描述了用于内存管理中的二分伙伴算法。之后，Hirschberg和Shen先后提出斐波那契伙伴算法和加权伙伴算法，作为伙伴算法的两种变体。为了适应不同的内存请求概率分布，Peterson又进一步提出泛化伙伴算法，针对不同请求概率分布采取不同的分配策略。为了追求时间效率，Linux 内核选择实现了二分伙伴算法, 该算法的优点在于伙伴地址的计算更加简便、高效1。\nLinux内核伙伴系统算法把所有的空闲页框分组为11个块链表，每个块链表分别包含大小为1、2、4、8、16、32、64、128、256、512和1024个连续页框的页框块。最大可以申请1024个连续页框，也即4MB大小的连续空间。假设要申请一个256个页框的块，先从256个页框的链表中查找空闲块，如果没有，就去512个页框的链表中找，找到了即将页框分为两个256个页框的块，一个分配给应用，另外一个移到256个页框的链表中。如果512个页框的链表中仍没有空闲块，继续向1024个页框的链表查找，如果仍然没有，则返回错误2。\n下图是伙伴系统只有0阶和2阶内存块时的分配情况。\n本文首先介绍了linux内存区域的概念，然后详细描述了伙伴系统算法中分配算法和回收算法，最后简要的描述了伙伴系统算法在分配时区域的选择。\n内存区域 UMA（Uniform-Memory-Access）模型：物理存储器被所有处理机均匀共享。所有处理机对所有存储字具有相同的存取时间，这就是为什么称它为均匀存储器存取的原因。每台处理机可以有私用高速缓存,外围设备也以一定形式共享。\nNUMA（Non-Uniform-Memory-Access）模型：NUMA模式下，处理器被划分成多个\"节点\"（node）， 每个节点被分配有的本地存储器空间。 所有节点中的处理器都可以访问全部的系统物理存储器，但是访问本节点内的存储器所需要的时间，比访问某些远程节点内的存储器所花的时间要少得多。\nUMA可以看成NUMA特例。下图是内存区域管理结构，pg_data_t代表着一个节点，zones存储着每个内存区域的物理页的数据结构struct page。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  typedef struct pglist_data { struct zone node_zones[MAX_NR_ZONES]; //内存分区，ZONE_DMA, ZONE_NORMAL, ZONE_HIGHMEM  struct zonelist node_zonelists[MAX_ZONELISTS]; int nr_zones; struct page *node_mem_map; //该节点物理内存中每一页的页框描述符  struct bootmem_data *bdata; unsigned long node_start_pfn; unsigned long node_present_pages; unsigned long node_spanned_pages; int node_id; struct pglist_data *pgdat_next; wait_queue_head_t kswapd_wait; struct task_struct *kswapd; int kswapd_max_order; } pg_data_t;   在内存中，每个node又被分成的区，它们各自描述在内存中的范围。一个管理区(zone)由struct zone结构体来描述，下面是可能分区的情况：\n ZONE_DMA标记适合DMA的内存域。该区域的长度依赖于处理器类型。在IA-32计算机上，一般的限制是16 MiB，这是由古老的ISA设备强加的边界，因此现代的计算机也可能受这一限制的影响。 ZONE_DMA32标记了使用32位地址字可寻址、适合DMA的内存域。显然，只有在64位系统上，两种DMA内存域才有差别。在32位计算机上，本内存域是空的，即长度为0 MiB。在Alpha和AMD64系统上，该内存域的长度可能从0到4 GiB。 ZONE_NORMAL标记了可直接映射到内核段的普通内存域。这是在所有体系结构上保证都会存在的唯一内存域，但无法保证该地址范围对应了实际的物理内存。例如，如果AMD64系统有2 GiB内存，那么所有内存都属于ZONE_DMA32范围，而ZONE_NORMAL则为空。 ZONE_HIGHMEM标记了超出内核段的物理内存，对应着IA-32计算机上896MB以上的物理内存。  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  struct zone { unsigned long pages_min, pages_low, pages_high; unsigned long lowmem_reserve[MAX_NR_ZONES]; struct per_cpu_pageset pageset[NR_CPUS]; spinlock_t lock; struct free_area free_area[MAX_ORDER];\t//伙伴系统算法  ZONE_PADDING(_pad1_) /* 通常由页面收回扫描程序访问的字段 */ spinlock_t lru_lock; struct list_head active_list; struct list_head inactive_list; unsigned long nr_scan_active; unsigned long nr_scan_inactive; unsigned long pages_scanned; /* 上一次回收以来扫描过的页 */ unsigned long flags; /* 内存域标志，见下文 */ /* 内存域统计量 */ atomic_long_t vm_stat[NR_VM_ZONE_STAT_ITEMS]; int prev_priority; ZONE_PADDING(_pad2_) /* 很少使用或大多数情况下只读的字段 */ wait_queue_head_t * wait_table; unsigned long wait_table_hash_nr_entries; unsigned long wait_table_bits; /* 支持不连续内存模型的字段。 */ struct pglist_data *zone_pgdat; unsigned long zone_start_pfn; unsigned long spanned_pages; /* 总长度，包含空洞 */ unsigned long present_pages; /* 内存数量（除去空洞） */ /* * 很少使用的字段： */ char *name; } ____cacheline_maxaligned_in_smp; struct free_area { struct list_head free_list[MIGRATE_TYPES]; //反内存碎片机制 \tunsigned long nr_free; };   分配算法 当内核请求分配内存时，伙伴系统执行分配算法以满足其需求。分配算法的基本思想是寻找能够满足内核需求的最小空闲内存块，如果该内存块的阶大于内核请求的阶，则将其逐步划分为一系列低阶内存块，直到划分出恰好满足需求的一个内存块，并分配给内核使用。其余低阶内存块按其所属的阶被依次插入相应的空闲链表，用于满足今后的内存请求。\n假设当前内存的分布情况如下：\n1 2 3  free_area[1].nr_free = 0 free_area[2].nr_free = 0 free_area[3].nr_free = 1   内核请求分配一个1 阶空闲内存块，对于 free_area 数组的查找将从 1 阶开始。由于 1 阶的空闲链表为空，因此继续查找高阶空闲链表，直到发现 3 阶空闲链表非空，查找结果为 3，这说明存在一个 3 阶的空闲内存块可供分配。然后，从 3 阶空闲链表移除第一个内存块的首页描述符，并用 page 指针指向它。同时计算 free_area[3].nr_free--。\n由于找到的空闲内存块的尺寸 (3 阶) 超过了内核请求的尺寸 (1 阶)，因此需要划分.。首先将其等分为两个（二分伙伴算法的名字来源） 2 阶内存块, 其中高地址端一块的首页描述符指针为 page + 4，将其插入2阶空闲链表，并计算free_area[2].nr_free++。然后，进一步划分低地址端的另一个2阶内存块，将其等分为两个1阶内存块。高地址端一块的首页描述符指针为page+2，将其插入1阶空闲链表，并计算free_area[1].nr_free++。剩下低地址端的一块1阶内存块恰好满足内核的需求，因此停止划分。\n回收算法 伙伴系统回收算法的基本思想是首先确定待回收内存块的伙伴，如果伙伴是空闲的，则将二者合并为一个高阶空闲内存块。重复上述合并过程, 直至伙伴不再空闲或合并形成的空闲内存块达到最高阶。在上述过程中，每次合并前都要将伙伴从其所在的空闲链表中移除。合并完成后，最终形成的高阶空闲内存块被插入相应空闲链表的表头。不难看出，回收算法的一项重要的工作是确定待回收内存块的伙伴：\n两个内存块互为伙伴 (Buddy) 当且仅当满足以下三个条件：\n 二者在内存中相邻且不重叠； 二者具有相同的阶； 假设二者的阶都为k，则合并后形成一个k+1阶空闲内存块。  我们知道linux采用数组的方式维护每一个物理页的数据结构struct page，即mem_map。所以通过mem_map[page_addr\u003e\u003e12]可以获得某物理页的struct page。该结构中包含了该页是否为空闲的状态。所以可以通过mem_map[(page_addr\u003e\u003e12)-2^k]确定左伙伴是否为空闲；通过mem_map[(page_addr\u003e\u003e12)+2^k]确定右伙伴是否为空闲。\n分配区域的选择 我们知道Linux内存分为结点，在结点内部分为区域，每一个区域由伙伴系统算法管理。伙伴系统算法在分配时首先会考虑当前结点是否有充足的内存，其次才会考虑从其它结点借用内存。节点内部分配的优先级从高到低依次是：ZONE_HIGHMEM、ZONE_NORMAL和ZONE_DMA。\n参考索引   薛峰.Linux内核伙伴系统分析[J].计算机系统应用,2018,27(01):174-179. ↩︎\n https://glemontree.github.io/2017/10/23/[Linux]%20Linux%E4%B8%AD%E7%9A%84%E4%BC%99%E4%BC%B4%E7%B3%BB%E7%BB%9F%E5%92%8Cslab%E6%9C%BA%E5%88%B6/ ↩︎\n   ","description":"","tags":["linux内核","伙伴系统"],"title":"伙伴系统","uri":"/posts/linux-kernel/%E4%BC%99%E4%BC%B4%E7%B3%BB%E7%BB%9F/"},{"categories":["机器人"],"content":"跟随者领航者法的基本思想是将编队控制问题转化为跟随者跟踪领航者的位置和方向的问题。在多机器人组成的群体中，某个机器人被指定为领航者，其余作为它的跟随者，跟随者以一定的距离间隔跟踪领航机器人的位置和方向。\n跟随者通过跟踪功能强大的领航者实现预期的目标，当领航者的位置等参量发生变化时，跟随者机器人通过比较这些参量的误差得到控制量并引导行为。对于跟随领航者法有两种控制器形式：$l-l$ 控制器和$l-\\varphi$控制器。$l-\\varphi$控制器的控制目标是使跟随者和领航者之间的距离和相对转角达到设定值。$l-l$控制器考虑的是三个机器人之间的相对位置问题。当跟随者和两个领航者之间的距离达到设定值的话，就可以认为整个队形稳定了1。\n这种方法的优点是比较容易实现编队控制。能够实现硬件资源的合理配置，减少了跟随机器人间的相互干扰，因为编队的所有轨迹均有领航者提供，同时跟随者只需跟随领航者运行，不需配置像领航者那样强大的传感器资源，只需配置满足任务需要的传感器即可。但是，这种方法采用的集中式控制使得编队的抗干扰能力较差，如果领航者受到破坏或者因为某种原因不能正常运行，整个编队的正常运行将不能得到保证，再者，所有跟随者都需要与领航者通信，使得领航者的通信负担重，使得这种方法不适用于跟随者较多的情形[^3]。\n建立模型2 领航跟随编队控制算法可以分为两个部分: 一是确定领航者及队形。二是跟随者跟随领航者。第一部分比较容易实现，可以根据实际情况确定。第二部分比较复杂，是算法的重点。在上面模型中领航跟随法的控制也就是控制领航者和跟随者之间的间距 $l$ 与角度 $\\varphi$。这种编队控制的主要思想是: 控制间距 $l$ 和角度 $\\varphi$，使之与期望的间距 $l_d$和期望的角度 $\\varphi_d$相等。使跟随机器人以角度 $\\varphi$ 和间距 $l$ 跟踪领航机器人，从而形成编队。领航者只需要沿着预定规划的路径行走，跟随者需要知道领航者的位置坐标$( x_1，y_1)$ 、角度 $\\theta_1$、速度 $v_1$、角速度 $\\omega_1$及自身的位置坐标$( x_2，y_2)$ 、角度 $\\theta_2$，通过计算确定自身的前进速度 $v_2$、角速度 $\\omega_2$来完成编队控制。\n使用参考点的坐标来表示机器人描述中的坐标。机器人描述向量 $r_i= ( x_i，y_i，\\theta_i，v_i，\\omega_i) $描述机器人 $i$ 的位置坐标、角度、前进速度和角速度，则领航者机器人的描述向量为 $r_1= ( x_1，y_1，\\theta_1，v_1，\\omega_1)$ ，跟随机器人的描述向量为 $r_2= ( x_2，y_2，\\theta_2，v_2，\\omega_2)$ 。根据上图中的模型可以得出机器人描述向量各个量之间的关系:\n $$ \\left\\{\\begin{array}{1} \\dot{x}=v_{i} \\cos \\theta_i \\\\ \\dot{y}=v_{i} \\sin \\theta_i \\\\ \\dot{\\theta}_{i}=\\omega_{i} \\end{array}\\right. $$   注：单位时间内位姿的变化量。\n 根据上图的控制关系，跟随机器人的运动学方程为:\n $$ \\left\\{\\begin{array}{l} \\dot{l}=v_{2} \\cos \\gamma-v_{1} \\cos \\varphi+d \\omega_{2} \\sin \\gamma \\\\ \\dot{\\varphi}=\\frac{1}{l}\\left(v_{1} \\sin \\varphi-v_{2} \\sin \\gamma+d \\omega_{2} \\cos \\gamma-l \\omega_{1}\\right) \\\\ \\dot{\\theta}_{2}=\\omega_{2} \\tag{2} \\end{array}\\right. $$  其中 $γ = \\varphi + \\theta_1－ \\theta_2$。\n根据算法原理结合结构图的闭环特性可得:\n $$ \\left\\{\\begin{array}{1} \\dot{l}=\\alpha(l_d-l) \\\\ \\dot{\\varphi}=\\beta(\\varphi_d-\\varphi) \\end{array}\\right.\\tag{3} $$   通过控制使得$l$和$\\varphi$逐渐达到期望值$l_d$和$\\varphi_d$，最终$l$和$\\varphi$单位时间内的变化量为0。\n 式中的 $α，β$ 是闭环控制中的比例系数，联合(2) 式和(3) 式可得跟随机器人的速度$( v_2，\\omega_2)$ ，即控制系统的输出：\n $$ \\left\\{\\begin{array}{l} \\omega_{2}=\\frac{\\cos \\gamma}{d}\\left[\\beta l\\left(\\varphi_{d}-\\varphi\\right)-v_{1} \\sin \\varphi+l \\omega_{1}+p \\sin \\gamma\\right] \\\\ v_{2}=p-d \\omega_{2} \\tan \\gamma \\tag{4} \\end{array}\\right. $$  其中$p=\\frac{v_icos\\varphi+\\alpha(l_d-l)}{cos\\gamma}$由( 4) 式得出跟随机器人的控制输入量$( v_2，\\omega_2)$ 。根据上面结果输入跟随机器人的控制量来控制跟随机器人的前进速度与角速度，从而完成领航跟随编队控制算法的队形保持，实现编队控制。 参考文献 [] 陈传均. 多机器人领航—跟随型编队控制研究[D]. 杭州电子科技大学. ↩\n  胡玮韬.多机器人编队及运动控制研究[D].陕西:西安电子科技大学,2010. DOI:10.7666/d.y1668394. ↩︎\n 赵明, 林茂松, 黄玉清. Leader-following Formation Control of Multi-robots Based on Dynamic Value of $\\varphi$%基于动态\\varphi值的领航跟随法多机器人编队控制[J]. 西南科技大学学报, 2013, 028(004):57-61. ↩︎\n   ","description":"","tags":["机器人","多车编队"],"title":"多车编队 领航跟随法","uri":"/posts/robot/%E5%A4%9A%E8%BD%A6%E7%BC%96%E9%98%9F-%E9%A2%86%E8%88%AA%E8%B7%9F%E9%9A%8F%E6%B3%95/"},{"categories":["linux内核"],"content":"在linux内核中伙伴系统用来管理物理内存，其分配的基本单位是页。由于伙伴系统分配的粒度又太大，因此linux采用slab分配器提供动态内存的管理功能，而且可以作为经常分配并释放的对象的高速缓存。slab分配器的优点：\n  可以提供小块内存的分配支持，通用高速缓存可分配的大小从$2^5B$到$2^{25}B$，专用高速缓存没有大小限制；\n  不必每次申请释放都和伙伴系统打交道，提供了分配释放效率；\n  如果在slab缓存的话，其在CPU高速缓存的概率也会较高；\n  伙伴系统对系统的数据和指令高速缓存有影响，slab分配器采用着色降低了这种副作用。\n  伙伴系统分配的内存大小是页的倍数，不利于CPU的高速缓存：如果每次都将数据存放到从伙伴系统分配的页开始的位置会使得高速缓存的有的行被过度使用，而有的行几乎从不被使用[cpu的L1 cache一般大小为32KB，采用伙伴系统分配$order\\geq5$时就会出现上述的问题]。slab分配器通过着色使得slab对象能够均匀的使用高速缓存，提高高速缓存的利用率。\n基本数据结构 struct kmem_cache、struct array_cache、struct kmem_cache_node\n程序经常需要创建一些数据结构，比如进程描述符task_struct，内存描述符mm_struct等。slab分配器把这些需要分配的小块内存区作为对象，类似面向对象的思想。每一类对象分配一个cache，cache有一个或多个slab组成，slab由一个或多个物理页面组成。需要分配对象的时候从slab中空闲的对象取，用完了再放回slab中，而不是释放给物理页分配器，这样下次用的时候就不用重新初始化了，实现了对象的复用。\n鸡和蛋问题 由于slab分配器对象（struct kmem_cache、struct array_cache、struct kmem_cache_node）也需要用slab分配器提供的高速缓存机制，那么问题是：先有高速缓存还是先有slab分配器对象。\n 我们知道slab分配器对象用来管理高速缓存：假设先有高速缓存的话，那么谁来管理高速缓存呢？ 我们知道高速缓存用来存放slab分配器对象：假设先有slab分配器对象，那么它存放在哪里呢？  linux的做法是：静态分配slab分配器对象内存空间，然后利用伙伴算法分配存储对象的页帧。\nslab着色原理 一般情况下，slab的大小为4KB。我们令cpu的L1 cache为32KB（每一行64B、512行），那么要填充满cpu的L1 cache需要8个slab。那么slab分配器会从伙伴分配系统中分配8个slab，也就是8个页（8个页不一定是物理连续的）。假设获取的8个页对应的的起始地址和终止地址如下：\n1 2 3 4 5 6 7 8  0x10000000 - 0x10001000：对应cache行是0-64 0x10010000 - 0x10011000：对应cache行是0-64 0x10020000 - 0x10021000：对应cache行是0-64 0x10030000 - 0x10031000：对应cache行是0-64 0x10040000 - 0x10041000：对应cache行是0-64 0x10050000 - 0x10051000：对应cache行是0-64 0x10060000 - 0x10061000：对应cache行是0-64 0x10070000 - 0x10071000：对应cache行是0-64   可以发现不同的slab起始地址映射到cpu cache的同一行。slab着色原理是利用浪费的空间，假设浪费的空间所占用的cache行是$[1,10)$。那么slab中对象实际占用的cache行是$0\\cup[11,64)$。那么可以通过偏移来让部分的对象移动到$[1,10)$行内。尽管slab着色可以使得对象尽可能占满cache，但是slab的着色仍然有很大的限制：\n 无法占满剩余的行数，上述实例中$[64,512)$仍然未被使用； 当slab的个数超过颜色数，效果甚微； slab的着色是用空间换时间，浪费一些空间来提高cache的命中率。  ","description":"","tags":["linux内核、slab分配器"],"title":"Slab分配器","uri":"/posts/linux-kernel/slab/"},{"categories":["c语言"],"content":"在开发中经常会使用到static和extern等关键字，它们会涉及到c语言中的作用域、链接和存储期等概念。作用域和链接描述了标识符的可见性，存储期描述了通过这些标识符访问的对象的生存期。\n作用域 作用域描述了标识符可见范围，包括：文件作用域、代码块作用域、函数作用域、函数原型作用域。\n文件作用域 在所有代码块之外的声明的标识符具有文件作用域；\n1 2 3 4  int a; int main(){ a = 0; }    先声明再使用；\n 1 2 3 4  int main(){ a = 0; //error: 'a' was not declared in this scope } int a;   代码块作用域 花括号之间声明的标识符具有块作用域；\n 函数定义的形式参数也具有块作用域。\n 1 2 3 4 5 6  int main(){ { int a; } a = 0; //error: 'a' was not declared in this scope }   函数原型作用域 函数原型中声明的参数名具有函数原型作用域；范围是从形参定义处到函数原型声明的结束。\n 唯一要注意的是在函数原型中的参数名字不能重复；\n 1  void func(int a,int a); //redefinition of 'int a'   函数作用域 一个标签首次出现在函数的内层块中，它的作用域也延伸至整个函数；\n 只适用于语句标签\n 1 2 3 4 5 6 7  void func(int a,int b){ ret: return 0; } int main(){ goto ret;\t//error: label 'ret' used but not defined }   1 2 3 4 5 6 7 8 9 10  int main(){ { ret: return 0; } { ret: //error: duplicate label 'ret'  return 0; } }   链接属性 C 变量有 3 种链接属性：外部链接、内部链接或无链接。具有块作用域、函数作用域或函数原型作用域的变量都是无链接变量。这意味着这些变量属于定义它们的块、函数或原型私有。具有文件作用域的变量可以是外部链接或内部链接。外部链接变量可以在多文件程序中使用，内部链接变量只能在一个翻译单元中使用。\nstatic关键字可以使一个外部链接变成内部链接（注意：1. 无法将无链接的变成内部链接，比如在函数内声明static变量，其链接属性仍然是无连接。2. static无法改变标识符的作用域），即每一个翻译单元都单独享有static声明的标识符。本例有两个源文件：file1.c和file2.c，源文件都包含同一个头文件file.h。具体代码如下：\n1 2 3 4 5 6  //file.h #ifndef TEST_FILE_H #define TEST_FILE_H static int a; void file1_modify_static(); #endif   1 2 3 4 5 6 7 8  //file1.c #include \u003cbits/stdc++.h\u003e#include \"file.h\"using namespace std; void file1_modify_static(){ a = 1; cout\u003c\u003c__func__\u003c\u003c\": a is \"\u003c\u003ca\u003c\u003cendl; }   1 2 3 4 5 6 7 8 9 10  //file2.c #include \u003cbits/stdc++.h\u003e#include \"file.h\"using namespace std; int main(){ a = 0; cout\u003c\u003c__func__\u003c\u003c\": a is \"\u003c\u003ca\u003c\u003cendl; file1_modify_static(); cout\u003c\u003c__func__\u003c\u003c\": a is \"\u003c\u003ca\u003c\u003cendl; }   最终的输出是：\n1 2 3  main: a is 0 file1_modify_static: a is 1 main: a is 0   可以看到文件1和文件2修改的a不是同一个a，说明static将变量a变成了内部链接。\n存储期 C对象有4种存储期：静态存储期、线程存储期、自动存储期、动态分配存储期。这里主要需要了解静态存储期和自动存储期：\n  如果对象具有静态存储期，那么它在程序的执行期间一直存在。\n  块作用域的变量通常都具有自动存储期。当程序进入定义这些变量的块时，为这些变量分配内存；当退出这个块时，释放刚才为变量分配的内存。\n  总结  static关键字的作用\n   static用于修改标识符的链接属性，将其链接属性从外部链接转换成内部链接，使得改标识符只能由该源文件访问，但标识符的存储期和作用域不受影响；\n 外部链接和内部链接的存储期和作用域都一样，属于：文件作用域，静态存储期\n   static用于块作用域的标识符声明时，将其从自动变量转换成静态变量，但变量的链接属性和作用域不受影响。\n  ","description":"","tags":["c语言、c语言基础概念"],"title":"作用域、链接、存储类型","uri":"/posts/c/%E4%BD%9C%E7%94%A8%E5%9F%9F%E9%93%BE%E6%8E%A5%E5%AD%98%E5%82%A8%E7%B1%BB%E5%9E%8B/"},{"categories":null,"content":"个人技能  armv8的虚拟化/hypervisor linux内核  ","description":"","tags":null,"title":"简介","uri":"/about/"},{"categories":null,"content":"NAND controller\n° 8/16-bit I/O width with one chip select signal\n° ONFI specification 1.0\n° 16-word read and 16-word write data FIFOs\n° 8-word command FIFO\n° Programmable I/O cycle timing\n° ECC assist\n° Asynchronous memory operating mode\n达尔文第二代已经结束\nosek操作系统-》达尔文操作系统\n达尔文第三代\n先做第二代\n应用：语音和图像\nAXI总线 ARM和FPGA通信\n文件系统\nflash文件系统又包括cramfs和squashfs和jffs/jffs2和yaffs/yaffs2和ubifs。\ncramfs是只读压缩的文件系统，可以将文件系统进行压缩，提高存储效率。\nsquashfs是只读压缩的文件系统，相比于cramfs可以支持更大的单个文件大小。\njffs/jffs2是可以读写，压缩的日志闪存文件系统，主要是应用于nor flash。\nyaffs/yaffs2是另一种日志闪存文件系统，主要是为nand型flash设计的文件系统，为了应对flash容量的快速增长。\nubifs是作为jffs2的后继文件系统，满足大容量的需求。\n https://baijiahao.baidu.com/s?id=1610041455262486965\u0026wfr=spider\u0026for=pc\n 2. 达尔文硬件平台介绍 2.3 达尔文硬件平台的架构及约束\n5个端口：东南西北四个方向和一个本地\n每个端口包含四个虚拟通道,virtual channel\n缓存\n路由器有路由计算模块用来决定数据的传输路线\nx,y各占6bit\n头微片\n64x64，但是只实现了28x28\n16x16：32KB\n10: 16KB\n2:8KB\n3. 脉冲神经网络的标准化和约束抽象  首次适应算法：首次适应算法会找到第一个还有容量放下这个物体的箱子（箱子的顺序是根据箱子开始使用的先后顺序来排定的），然后将当前的物体放进去，如果没有放得下当前物体的箱子，那么会使用一个新箱子来放这个物体。 最佳适应： 遗传算法  单个芯片的片上网络是一个28×28的网格，在每个网格节点上有一个神经元处理单元（NPU）。它由一个物理神经元通过时分复用的方式达到256个逻辑神经元的效果。并且每个神经元处理单元（NPU）都有属于自己的一块片上存储，分别是8KB、16KB和32KB的几种不同规格。所以在进行网络配置的时候每个网格节点上的硬件约束条件就是配置的神经元总数不超过256，并且当前配置的节点上面配置的神经元需要的存储权重的空间不超过相应的片上存储，对于不同的节点这个值就可能是8KB、16KB或者32KB中的一个。\n神经元数量和网络连接数量\n5. 开发平台软件系统   需要一个满足本文描述规范的脉冲神经网络作为输入，根据硬件约束条件判断其是否符合硬件要求；\n  在检查是否符合硬件约束条件的时候，会为整个网络分配好硬件资源，即按第三章所述的方案来执行\n 主要是看神经元数量和连接数\n   将分配好的硬件资源映射到芯片上，并对其进行最优化求解，即采用第四章所述的算法。\n 找到功耗最小的映射方案\n   具体部署到芯片上了，这时候先会生成配置文件，然后通过网口将配置信息传递到用于芯片和PC间通信转发的FPGA，再由FPGA将配置信息发送到芯片上\n  配置完成之后就是将数据输入芯片，以及对这个脉冲神经网络的软件仿真，输入数据采用的是已经编码成脉冲的输入，最后读出芯片运行的结果并与软件仿真结果进行对比，如果结果不一致就可以开始对芯片进行调试。\n  脉冲神经网络\n基于脉冲神经网络的仿脑计算芯片 这种芯片直接用硬件实现了脉冲神经元模型以及神经元之间的连接，一般通过片上网络来实现神经元之间的通信，并且它有大量的核，而神经元就分布在不同的核即片上网络节点上。\n对于基于脉冲神经网络的仿脑计算芯片的编程，关键在于神经元在仿脑芯片上的分配，将脉冲神经网络配置到芯片上时如何配置才可以满足芯片实时运行的同时功耗尽可能小，是学术界和工业界都在探索的研究课题。\n本文的应用开发平台，使用分区域的方式把脉冲神经网络分配到芯片的不同节点，从而将脉冲神经网络转换成为一个规模较小的并且每个节点对应芯片上一个节点的网络，然后在映射时分别对功耗和负载进行优化使网络满足实时性和低功耗的需求。本文采用了一种通用化的脉冲神经网络描述规范，开发环境以该规范描述的网络为输入，实现了应用的自动部署、运行以及调试仿真，让应用开发者几乎不需要考虑达尔文芯片特定的一些硬件约束。\n现在有很多神经网络芯片，但是很大一部分都是为传统神经网络进行加速的，这类芯片虽然用于特定的算法的时候有比较好的效果，但是其生物相似性是不高的，要更好的模拟大脑，更好的办法是用脉冲神经网络。和经典的神经网络不同的是，其神经元之间的通信是利用的脉冲来实现的，而不是传递一个具体的数值，这和在生物神经网络里面的情况一样，并且在每个神经元的内部也不是进行一个数值计算，而是模拟了生物神经元的行为。\n真北” 4096个处理核，在每个核上还有256个神经元，总共拥有100万个神经元和2.56亿个突触连接 每秒进行266G次的定点运算\n真北的每个核有256个轴突和一个256×256的突触交叉开关\n“天机”\n个芯片包含了6个核，每个核可以支持256个神经元的运算 在核之间通过一个2×3的片上网络（Network on Chip）相连，每秒可以进行的定点运算次数最多可以达到153.6G 153.6G\n“达尔文”芯片 2048个神经元和4194304个突触连接，并且支持15种不同的突触延时 28×28的片上网络来连接处理核 每个核上最多可以支持256个神经元\n在多核系统中用片上网络来代替总线和交叉开关是一个很好的选择。、\n","description":"","tags":null,"title":"程书意","uri":"/diaoyan/"},{"categories":null,"content":"Two key network-core functions\n​\trouting\n​\tforwarding\nAlternative core: circuit switching\nfour sources of packet delay\nsecurty\n​\tpacket sniffing:broadcast media\n​\tip spoofing: send packet with false source address\nApplication architectures\n​\tclient-server ​\tpeer-to-peer (P2P)\n​\tTCP service: reliable transport between sending and receiving process flow control: sender won’t overwhelm receiver congestion control: throttle sender when network overloaded does not provide: timing, minimum throughput guarantee, security connection-oriented: setup required between client and server processes\nUDP service: unreliable data transfer between sending and receiving process does not provide: reliability, flow control, congestion control, timing, throughput guarantee, security, or connection setup,\nQ: why bother? Why is there a UDP?\nsecuring tcp\nweb and http\n​\tnon-persistent HTTP\n​\tpersistent HTTP\n 国立清华大学 domain name(domain name server) -\u003e ip address : about 6 message to find ip address\n3 message for connection establishment of tcp\n4\n4\npoint-to-point multiple access\npeer-to-peer\nspanning tree：任何两个结点之间只有一条路径\n封包最大值 每个网路不一样\nstatistical multiplexing:fifo rr priorities\nbandwidth\nlatency\n","description":"","tags":null,"title":"程书意","uri":"/posts/computer-network/untitled/"},{"categories":null,"content":"01 Coping with complexity\nModularity Abstraction Layering Hierarchy Start with a small group of modules Assemble them into a stable, self-contained subsystem with well defined interface Assemble a group of subsystems to a larger subsystem\n02 L1: Block Layer Mapping: block number -\u003e block data Super Block the size of block which block is free Kernel reads superblock when mount the FS L2: File Layer inode (index node) A container for metadata about the file block, indrect block, double indirect block, triple indirect block\nL3: inode Number Layer Mapping: inode number -\u003e inode\n![](https://gitee.com/chengshuyi/scripts/raw/master/img/20200427113436.png) inode number is enough to operate a file  L4: File Name Layer L5: Path Name Layer link unlink rename L6: Absolute Path Name Layer L7: Symbolic Link\u000bLayer mount Record the device and the root inode number of the file system in memory Record in the in-memory version of the inode for \"/dev/fd1\" its parent’s inode link\nFAT (File Allocation Table) File System File is collection of disk blocks In FAT: file attributes are kept in directory\n03 Directly Dump a Directory Two Types of Links Hard link Soft link Soft link can create cycle by SYMLINK(\"a\", \"a\") --\n","description":"","tags":null,"title":"程书意","uri":"/posts/cse/"},{"categories":null,"content":"notes1\nA relation is an unordered set that contains the relationship of attributes that represent entities. Since the relationships are unordered, the DBMS can store them in any way it wants, allowing for optimization.\nA tuple is a set of attribute values (also known as its domain) in the relation. Originally, values had to be atomic or scalar, but now values can also be lists or nested data structures. Every attribute can be a special value, NULL, which means for a given tuple the attribute is undefined.\nA relation with n attributes is called an n-ary relation.\nData Manipulation Languages (DMLs) Relational Algebra\nselect type,primary_title,runtime_minutes from titles where runtime_minutes in (select runtime_minutes from titles order by runtime_minutes desc limit 1) order by type asc, primary_title asc;\nselect type, count() FROM titles GROUP BY type ORDER BY count() ASC\nselect cast(premiered/1010 as varchar)||'s',count() from titles where premiered NOT NULL group by premiered/10;\nselect cast(premiered/1010 as varchar)||'s',round(count()100.0/(select count() from titles),4) from titles where premiered NOT NULL group by premiered/10;\nselect primary_title,b.num from titles as a, (select title_id,count() as num from akas group by title_id order by count() desc limit 10) as b where a.title_id = b.title_id;\n","description":"","tags":null,"title":"程书意","uri":"/posts/leet/"},{"categories":null,"content":"SYSCALL_DEFINE3(read, unsigned int, fd, char __user *, buf, size_t, count)\ncall ksys_read\n​\tcall fdget_pos // convert long fd to struct fd\n​\tcall vfs_read\n​\tcall rw_verify_area\n​\tcall __vfs_read\n​\tcall file-\u003ef_op-\u003eread(file, buf, count, pos);\n假设我们使用的ext4文件系统\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  const struct file_operations ext4_file_operations = { .llseek\t= ext4_llseek, .read_iter\t= ext4_file_read_iter, .write_iter\t= ext4_file_write_iter, .unlocked_ioctl = ext4_ioctl, #ifdef CONFIG_COMPAT \t.compat_ioctl\t= ext4_compat_ioctl, #endif \t.mmap\t= ext4_file_mmap, .mmap_supported_flags = MAP_SYNC, .open\t= ext4_file_open, .release\t= ext4_release_file, .fsync\t= ext4_sync_file, .get_unmapped_area = thp_get_unmapped_area, .splice_read\t= generic_file_splice_read, .splice_write\t= iter_file_splice_write, .fallocate\t= ext4_fallocate, };   call ext4_file_read_iter\n1 2 3 4  if (iocb-\u003eki_flags \u0026 IOCB_DIRECT) return ext4_dio_read_iter(iocb, to); return generic_file_read_iter(iocb, to);   call generic_file_read_iter\ncall generic_file_buffered_read\ncheck page cache\ncall error = mapping-\u003ea_ops-\u003ereadpage(filp, page);\n(1) 页缓存（page cache）针对以页为单位的所有操作，并考虑了特定体系结构上的页长度。一个\n主要的例子是许多章讨论过的内存映射技术。因为其他类型的文件访问也是基于内核中的这一技术实\n现的，所以页缓存实际上负责了块设备的大部分缓存工作。\n(2) 块缓存（buffer cache）以块为操作单位。在进行I/O操作时，存取的单位是设备的各个块，而\n不是整个内存页。尽管页长度对所有文件系统都是相同的，但块长度取决于特定的文件系统或其设置。\n因而，块缓存必须能够处理不同长度的块。\n","description":"","tags":null,"title":"程书意","uri":"/posts/linux-kernel/untitled/"},{"categories":null,"content":"中断下半部-tasklet\nsoftirqhttps://zhuanlan.zhihu.com/p/80371745\nlinux原子操作\n","description":"","tags":null,"title":"程书意","uri":"/%E5%BE%85%E8%A7%A3%E5%86%B3/"},{"categories":null,"content":"Exceptions and interrupts are both \"protected control transfers,\" which cause the processor to switch from user to kernel mode (CPL=0) without giving the user-mode code any opportunity to interfere with the functioning of the kernel or other environments. In Intel's terminology, an interrupt is a protected control transfer that is caused by an asynchronous event usually external to the processor, such as notification of external device I/O activity. An exception, in contrast, is a protected control transfer caused synchronously by the currently running code, for example due to a divide by zero or an invalid memory access.\nThere are two sources for external interrupts and two sources for exceptions:\n Interrupts  Maskable interrupts, which are signalled via the INTR pin. Nonmaskable interrupts, which are signalled via the NMI (Non-Maskable Interrupt) pin.   Exceptions  Processor detected. These are further classified as faults, traps, and aborts. Programmed. The instructions INTO, INT 3, INT n, and BOUND can trigger exceptions. These instructions are often called \"software interrupts\", but the processor handles them as exceptions.    [Linux-孤儿进程与僵尸进程总结]\n 操作系统一个栈一般多大\n 按M单位，10M\n 进程调度算法\n  02. 先来先服务调度算法 03. 时间片轮转调度法 04. 短作业(SJF)优先调度算法 05. 最短剩余时间优先 06. 高响应比优先调度算法 07. 优先级调度算法 08. 多级反馈队列调度算法   线程和进程是怎么调度的\n https://www.cnblogs.com/gmpy/p/10265284.html\n线程是最小的调度单位，进程是最小的资源分配单位\n linux内存管理\n kmalloc内存分配最终总是调用__get_free_pages 来进行实际的分配，故前缀都是GFP_开头。 kmalloc分最多只能分配32个page大小的内存，每个page=4k，也就是128K大小，其中16个字节用来记录页描述结构。kmalloc分配的是常驻内存，不会被交换到文件中。最小分配单位是32或64字节。\n   分配函数 区域 连续性 大小 释放函数 优势     kmalloc 内核空间 物理地址连续 最大值128K-16 kfree 性能更佳   vmalloc 内核空间 虚拟地址连续 更大 vfree 更易分配大内存   malloc 用户空间 虚拟地址连续 更大 free     cat /proc/buddyinfo\ncat /proc/pagetypeinfo\ncat /proc/slabinfo\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  \u003cmm_types.h\u003e struct mm_struct { ... unsigned long (*get_unmapped_area) (struct file *filp, unsigned long addr, unsigned long len, unsigned long pgoff, unsigned long flags); ... //0x4000000，  unsigned long mmap_base; /* mmap区域的基地址 */ //3g长度  unsigned long task_size; /* 进程虚拟内存空间的长度 */ ... unsigned long start_code, end_code, start_data, end_data; unsigned long start_brk, brk, start_stack; unsigned long arg_start, arg_end, env_start, env_end; ... }   1 2 3 4 5 6  struct mm_struct { struct vm_area_struct * mmap; /* 虚拟内存区域列表 */ struct rb_root mm_rb; struct vm_area_struct * mmap_cache; /* 上一次find_vma的结果 */ ... }    pgdir_walk() boot_map_region() page_lookup() page_remove() page_insert() // Given 'pgdir', a pointer to a page directory, pgdir_walk returns // a pointer to the page table entry (PTE) for linear address 'va'. // This requires walking the two-level page table structure.\n// Return the page mapped at virtual address 'va'. // If pte_store is not zero, then we store in it the address // of the pte for this page. This is used by page_remove and // can be used to verify page permissions for syscall arguments, // but should not be used by most callers.\n","description":"","tags":null,"title":"程书意","uri":"/%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98/"}]
